import scrapy
from scrapy_playwright.page import PageMethod
from ..parsers.loader import load_config, extract_value
import time
import random
import os
from ..logger import get_scraping_logger

class GenericSpider(scrapy.Spider):
    name = "generic_scraper"
    custom_settings = {
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True,
            'args': [
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
                "--disable-background-timer-throttling",
                "--disable-backgrounding-occluded-windows",
                "--disable-renderer-backgrounding",
                "--disable-ipc-flooding-protection",
                "--memory-pressure-off",
                "--max_old_space_size=4096"
            ]
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 80000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2 –º–∏–Ω—É—Ç
        'DOWNLOAD_TIMEOUT': 120,
        'DOWNLOAD_MAXSIZE': 10485760,
        'DOWNLOAD_WARNSIZE': 5242880,
        'RETRY_TIMES': 3,
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408, 429, 403],
        'CONCURRENT_REQUESTS': 4,  # –ë—ã–ª–æ 1
        'CONCURRENT_REQUESTS_PER_DOMAIN': 4,  # –ë—ã–ª–æ 1
        'DOWNLOAD_DELAY': 0.5,  # –ë—ã–ª–æ 3
        'RANDOMIZE_DOWNLOAD_DELAY': True,
    }

    def __init__(self, config=None, job_id=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not config:
            raise ValueError("Path to config file must be provided via -a config=...")
        self.config_path = config
        self.config = load_config(self.config_path)

        # –ú—É–ª—å—Ç–∏–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        self.base_url = self.config.get("base_url", "")
        self.categories = self.config.get("categories", [])
        self.selectors = self.config.get("selectors", {})
        self.pagination = self.config.get("pagination", {})
        self.data_processing = self.config.get("data_processing", {})
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º start_urls –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.start_urls = []
        for category in self.categories:
            start_page = self.pagination.get("start_page", 1)
            page_url_format = self.pagination.get("page_url_format", "{base_url}{category_url}?page={page}")
            
            start_url = page_url_format.format(
                base_url=self.base_url,
                category_url=category["url"],
                page=start_page
            )
            self.start_urls.append(start_url)

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.parse_all_listings = self.config.get("parse_all_listings", True)
        self.max_items_limit = int(self.config.get("max_items_limit", 100))
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Playwright
        self.request_settings = self.config.get("request_settings", {})
        self.use_playwright = self.request_settings.get("use_playwright", False)
        self.playwright_wait = self.request_settings.get("playwright_wait", 3)
        
        self.processed_items = 0
        self.failed_items = 0
        self.scraped_items_count = 0  # –û–±—â–∏–π —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.category_items_count = {}  # –°—á–µ—Ç—á–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        self.has_parsing_errors = False # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.job_id = job_id or os.environ.get('SCRAPY_JOB_ID', 'unknown')
        self.config_name = os.environ.get('SCRAPY_CONFIG_NAME', config or 'unknown')
        self.scraping_logger = get_scraping_logger(self.job_id, self.config_name)
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.total_items_expected = 0
        self.progress_update_interval = 10  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤

    def start_requests(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        for url in self.start_urls:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ URL
            category = self._get_category_from_url(url)
            if category:
                yield scrapy.Request(
                    url,
                    callback=self.parse,
                    meta={
                        'category': category,
                        'page': self.pagination.get("start_page", 1)
                    },
                    errback=self.handle_error,
                    dont_filter=True
                )

    def _get_category_from_url(self, url):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ URL"""
        for category in self.categories:
            if category["url"] in url:
                return category
        return None

    def parse(self, response):
        category = response.meta.get('category')
        current_page = response.meta.get('page', 1)
        
        if not category:
            self.logger.error("Category not found in response meta")
            return

        ads_list_selector = self.selectors.get("ads_list")
        ad_card_selector = self.selectors.get("ad_card")
        
        if not ads_list_selector or not ad_card_selector:
            self.logger.error("Required selectors (ads_list, ad_card) not found in config")
            return

        ads_container = response.css(ads_list_selector)
        if not ads_container:
            self.logger.warning(f"No ads container found with selector: {ads_list_selector}")
            return

        category_name = category['name']
        if category_name not in self.category_items_count:
            self.category_items_count[category_name] = 0

        items_found = 0
        for element in ads_container.css(ad_card_selector):
            if not self.parse_all_listings and self.category_items_count[category_name] >= self.max_items_limit:
                self.logger.info(f"Reached max items limit for category '{category_name}': {self.max_items_limit}")
                return
                
            items_found += 1
            self.scraped_items_count += 1
            self.category_items_count[category_name] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ N —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if self.scraped_items_count % self.progress_update_interval == 0:
                self._update_progress()
                
            try:
                # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è (–±–µ–∑ details)
                item_data = self._extract_item_data(element, category, only_main=True)
                detail_url = item_data.get('url')
                details_selectors = self.selectors.get('details', {})
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                self.logger.debug(f"üîç Detail URL: {detail_url}")
                self.logger.debug(f"üîç Details selectors: {bool(details_selectors)}")
                
                if detail_url and details_selectors:
                    # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    self.logger.debug(f"üîç Making detail request to: {detail_url}")
                    yield scrapy.Request(
                        detail_url,
                        callback=self.parse_details,
                        meta={'item_data': item_data, 'category': category},
                        errback=self.handle_error,
                        dont_filter=True
                    )
                else:
                    self.logger.debug(f"üîç Skipping detail request - URL: {detail_url}, selectors: {bool(details_selectors)}")
                    yield item_data
            except Exception as e:
                self.logger.error(f"Error processing item: {e}")
                self.has_parsing_errors = True
                self.failed_items += 1

        self.logger.info(f"Found {items_found} items on page {current_page} for category {category['name']}")
        self.scraping_logger.log_page_processed(current_page, items_found, response.url)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ‚Äî –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if items_found == 0:
            self.logger.info(f"–ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page} –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category['name']} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é.")
            return

        if self._should_continue_pagination(category):
            yield from self._handle_pagination(response, category, current_page)

    def _extract_item_data(self, element, category, only_main=False):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è. only_main=True ‚Äî —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è, –±–µ–∑ details."""
        try:
            item_data = {
                'property_type': category['property_type'],
                'listing_type': category['listing_type'],
                'source': self.config.get('source_name', 'unknown'),
                'category_name': category['name']
            }
            basic_fields = ['title', 'url', 'price', 'location', 'description', 'image']
            for field in basic_fields:
                selector = self.selectors.get(field)
                if selector:
                    value = self._extract_field_value(element, selector)
                    item_data[field] = value
            if not only_main:
                details = self.selectors.get('details', {})
                for field, selector in details.items():
                    value = self._extract_field_value(element, selector)
                    item_data[field] = value
            if item_data.get('url') and not item_data['url'].startswith('http'):
                item_data['url'] = self.base_url + item_data['url']
            return item_data
        except Exception as e:
            self.logger.error(f"Error extracting item data: {e}")
            self.has_parsing_errors = True
            return None

    def _extract_field_value(self, element, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É"""
        try:
            if selector.startswith("xpath:"):
                xpath_sel = selector[len("xpath:"):]
                return element.xpath(xpath_sel).get(default="").strip()
            elif selector.strip().startswith("//") or selector.strip().startswith(".//"):
                return element.xpath(selector).get(default="").strip()
            else:
                return element.css(selector).get(default="").strip()
        except Exception as e:
            self.logger.warning(f"Error extracting field with selector '{selector}': {e}")
            self.has_parsing_errors = True
            return None

    def _update_progress(self):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            total_categories = len(self.categories)
            processed_categories = len([cat for cat in self.categories if self.category_items_count.get(cat['name'], 0) > 0])
            
            if total_categories > 0:
                progress = min(95, int((processed_categories / total_categories) * 100))
            else:
                progress = 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (WebSocket –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            self.logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress}%, —Å–ø–∞—Ä—Å–µ–Ω–æ: {self.scraped_items_count}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")

    def _should_continue_pagination(self, category):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_name = category['name']
        if not self.parse_all_listings and self.category_items_count.get(category_name, 0) >= self.max_items_limit:
            return False
        return True

    def _handle_pagination(self, response, category, current_page):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é"""
        try:
            next_page = current_page + 1
            page_url_format = self.pagination.get("page_url_format", "{base_url}{category_url}?page={page}")
            
            next_url = page_url_format.format(
                base_url=self.base_url,
                category_url=category["url"],
                page=next_page
            )
            
            self.logger.info(f"Following to page {next_page} for category {category['name']}: {next_url}")
            
            yield scrapy.Request(
                next_url,
                callback=self.parse,
                meta={
                    'category': category,
                    'page': next_page
                },
                errback=self.handle_error,
                dont_filter=True
            )
            
        except Exception as e:
            self.logger.error(f"Error in pagination: {e}")

    async def page_init_callback(self, page, request):
        """Callback –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Playwright —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if not page:
            self.logger.debug("Page object is None in page_init_callback")
            return
            
        try:
            if page.is_closed():
                self.logger.debug("Page is already closed")
                return
                
            await page.set_default_timeout(60000)
            await page.set_default_navigation_timeout(60000)
            
            # –ë–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            await page.route("**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2}", lambda route: route.abort())
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –æ—à–∏–±–æ–∫
            page.on("pageerror", lambda err: self.logger.debug(f"Page error: {err}"))
            page.on("requestfailed", lambda request: self.logger.debug(f"Request failed: {request.url}"))
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º viewport
            await page.set_viewport_size({"width": 1920, "height": 1080})
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º User-Agent
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            ]
            
            await page.set_extra_http_headers({
                "User-Agent": random.choice(user_agents),
                "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
            })
            
            # –°–∫—Ä—ã–≤–∞–µ–º webdriver
            await page.evaluate("() => { Object.defineProperty(navigator, 'webdriver', { get: () => undefined }) }")
            
            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            await page.wait_for_timeout(random.randint(1000, 3000))
            
        except Exception as e:
            self.logger.debug(f"Error in page_init_callback: {e}")

    def handle_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        try:
            request = failure.request
            retry_count = request.meta.get('retry_count', 0)
            max_retries = 3
            
            self.logger.error(f"Request failed: {request.url}")
            self.logger.error(f"Error: {failure.value}")
            
            # Retry –¥–ª—è —Ç–∞–π–º–∞—É—Ç–æ–≤ –∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
            if retry_count < max_retries and (
                'Timeout' in str(failure.value) or 
                'Connection' in str(failure.value) or
                'Network' in str(failure.value)
            ):
                retry_count += 1
                self.logger.info(f"Retrying request {request.url} (attempt {retry_count}/{max_retries})")
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è retry
                new_timeout = 120000 + (retry_count * 30000)  # +30 —Å–µ–∫ –∑–∞ –∫–∞–∂–¥—É—é –ø–æ–ø—ã—Ç–∫—É
                
                yield scrapy.Request(
                    request.url,
                    callback=request.callback,
                    meta={
                        **request.meta,
                        'retry_count': retry_count,
                        'playwright': True,
                        'playwright_include_page': True,
                        'playwright_page_methods': [
                            PageMethod("wait_for_load_state", "networkidle"),
                        ]
                    },
                    errback=self.handle_error,
                    dont_filter=True
                )
                return
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
            error_str = str(failure.value).lower()
            if any(network_error in error_str for network_error in [
                'dns lookup failed', 'connection refused', 'connection timeout',
                'network unreachable', 'host unreachable', 'request failed'
            ]):
                self.has_parsing_errors = True
                self.logger.error("Network error detected, setting parsing errors flag")
            
            if self.scraping_logger:
                self.scraping_logger.log_request_failure(request.url, str(failure.value))
            self.failed_items += 1
            
        except Exception as e:
            self.logger.error(f"Error in error handler: {e}")

    def closed(self, reason):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–ø–∞–π–¥–µ—Ä–∞"""
        try:
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._update_progress()
            
            total_processed = self.processed_items + self.failed_items
            success_rate = (self.processed_items / total_processed * 100) if total_processed > 0 else 0
            
            stats = {
                'scraped_items': self.scraped_items_count,
                'processed_items': self.processed_items,
                'failed_items': self.failed_items,
                'success_rate': f"{success_rate:.1f}%",
                'reason': reason,
                'category_breakdown': self.category_items_count
            }
            
            self.logger.info(f"Spider closed: {stats}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for category_name, count in self.category_items_count.items():
                self.logger.info(f"  üìÇ {category_name}: {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            self.scraping_logger.log_spider_finished(stats)
            
        except Exception as e:
            self.logger.error(f"Error in spider close: {e}")

    def parse_details(self, response):
        """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å item_data –∏–∑ meta."""
        self.logger.info(f"üîç parse_details called for URL: {response.url}")
        item_data = response.meta['item_data']
        category = response.meta['category']
        details = self.selectors.get('details', {})
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω Playwright, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        if self.use_playwright:
            self.logger.info(f"üîç Using Playwright for details page: {response.url}")
            yield scrapy.Request(
                response.url,
                callback=self.parse_details_with_playwright,
                meta={
                    'item_data': item_data,
                    'category': category,
                    'details': details,
                    'playwright': True,
                    'playwright_page_methods': [
                        PageMethod("wait_for_load_state", "networkidle"),
                        PageMethod("wait_for_timeout", self.playwright_wait * 1000),
                    ],
                    'playwright_page_init_callback': self.page_init_callback,
                },
                errback=self.handle_error,
                dont_filter=True
            )
        else:
            # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ Playwright
            for field, selector in details.items():
                if field == 'images':
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
                    photos = self._extract_photos_from_details(response, selector)
                    if photos:
                        item_data['photos'] = photos
                        # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ images –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º
                        item_data['images'] = [photo['url'] for photo in photos]
                        self.logger.info(f"üîç Detail parsing: extracted {len(photos)} photos")
                    else:
                        self.logger.warning("üîç Detail parsing: no photos extracted")
                elif field == 'phone':
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                    phones = self._extract_phones_from_details(response, selector)
                    if phones:
                        item_data['phone_numbers'] = phones
                        self.logger.info(f"üîç Detail parsing: extracted {len(phones)} phones: {phones}")
                    else:
                        self.logger.warning("üîç Detail parsing: no phones extracted")
                else:
                    value = self._extract_field_value(response, selector)
                    item_data[field] = value
            
            yield item_data

    def parse_details_with_playwright(self, response):
        """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Playwright (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥)"""
        item_data = response.meta['item_data']
        category = response.meta['category']
        details = response.meta['details']
        
        self.logger.info(f"üîç Playwright detail parsing started for: {response.url}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_type = response.headers.get('content-type', b'').decode('utf-8').lower()
            if 'text/html' not in content_type:
                self.logger.warning(f"Non-HTML content type: {content_type} for {response.url}")
                yield item_data
                return
            
            if not hasattr(response, 'text') or not response.text:
                self.logger.warning(f"Empty or non-text response for {response.url}")
                yield item_data
                return
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª—è
            for field, selector in details.items():
                try:
                    if field == 'images':
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
                        photos = self._extract_photos_from_details(response, selector)
                        if photos:
                            item_data['photos'] = photos
                            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ images –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º
                            item_data['images'] = [photo['url'] for photo in photos]
                            self.logger.info(f"üîç Playwright detail parsing: extracted {len(photos)} photos")
                        else:
                            self.logger.warning("üîç Playwright detail parsing: no photos extracted")
                    elif field == 'phone':
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                        phones = self._extract_phones_from_details(response, selector)
                        if phones:
                            item_data['phone_numbers'] = phones
                            self.logger.info(f"üîç Playwright detail parsing: extracted {len(phones)} phones: {phones}")
                        else:
                            self.logger.warning("üîç Playwright detail parsing: no phones extracted")
                    else:
                        value = self._extract_field_value(response, selector)
                        item_data[field] = value
                        
                except Exception as e:
                    self.logger.warning(f"Error extracting detail field '{field}': {e}")
                    item_data[field] = None
            
            yield item_data
            
        except Exception as e:
            self.logger.error(f"Error in Playwright details parsing: {e}")
            yield item_data

    def _extract_photos_from_details(self, response, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            photos = []
            self.logger.info(f"üîç Photo details extraction: selector = '{selector}'")
            self.logger.info(f"üîç Photo details extraction: response URL = '{response.url}'")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–∫—É HTML
            html_sample = response.text[:500] if response.text else "No HTML content"
            self.logger.info(f"üîç Photo details extraction: HTML sample = '{html_sample}...'")
            
            image_elements = self._extract_field_elements(response, selector)
            self.logger.info(f"üîç Photo details extraction: found {len(image_elements)} image elements")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ - –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            for i, img_url in enumerate(image_elements[:5]):
                self.logger.info(f"üîç Photo details extraction: raw image {i+1} = '{img_url}'")
            
            for i, img_url in enumerate(image_elements):
                if img_url:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –ø–æ–ª–Ω—ã–π
                    if not img_url.startswith('http'):
                        base_url = self.base_url.rstrip('/')
                        img_url = img_url.lstrip('/')
                        full_url = f"{base_url}/{img_url}"
                    else:
                        full_url = img_url
                    
                    photos.append({'url': full_url})
                    self.logger.info(f"üîç Photo details extraction: photo {i+1} = {full_url}")
            
            self.logger.info(f"üîç Photo details extraction: total photos = {len(photos)}")
            return photos
        except Exception as e:
            self.logger.error(f"Error extracting photos from details: {e}")
            return []

    def _extract_phones_from_details(self, response, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            phones = []
            self.logger.info(f"üîç Phone extraction: selector = '{selector}'")
            
            phone_elements = self._extract_field_elements(response, selector)
            self.logger.info(f"üîç Phone extraction: found {len(phone_elements)} phone elements")
            
            for i, phone in enumerate(phone_elements):
                if phone:
                    self.logger.info(f"üîç Phone extraction: raw phone {i+1} = '{phone}'")
                    # –û—á–∏—â–∞–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    cleaned_phone = self._clean_phone_number(phone)
                    if cleaned_phone:
                        phones.append(cleaned_phone)
                        self.logger.info(f"üîç Phone extraction: cleaned phone {i+1} = '{cleaned_phone}'")
                    else:
                        self.logger.warning(f"üîç Phone extraction: phone {i+1} was cleaned to empty")
                else:
                    self.logger.warning(f"üîç Phone extraction: phone {i+1} is empty")
            
            self.logger.info(f"üîç Phone extraction: total phones = {len(phones)}")
            return phones
        except Exception as e:
            self.logger.error(f"Error extracting phones from details: {e}")
            return []

    def _extract_field_elements(self, element, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É"""
        try:
            if selector.startswith("xpath:"):
                xpath_sel = selector[len("xpath:"):]
                return element.xpath(xpath_sel).getall()
            elif selector.strip().startswith("//") or selector.strip().startswith(".//"):
                return element.xpath(selector).getall()
            else:
                return element.css(selector).getall()
        except Exception as e:
            self.logger.warning(f"Error extracting field elements with selector '{selector}': {e}")
            return []

    def _clean_phone_number(self, phone):
        """–û—á–∏—â–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        try:
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å tel: –µ—Å–ª–∏ –µ—Å—Ç—å
            if phone.startswith('tel:'):
                phone = phone[4:]
            
            # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, + –∏ –ø—Ä–æ–±–µ–ª–æ–≤
            import re
            cleaned = re.sub(r'[^\d+\s\-\(\)]', '', phone)
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            cleaned = ' '.join(cleaned.split())
            
            return cleaned if cleaned else None
        except Exception as e:
            self.logger.warning(f"Error cleaning phone number '{phone}': {e}")
            return phone



    def closed(self, reason):
        if self.has_parsing_errors:
            self.logger.error("Spider finished with parsing errors. Signalling failure.")
            # Scrapy –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —Å –∫–æ–¥–æ–º 0 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –≤ –ª–æ–≥–∞—Ö.
            # –ß—Ç–æ–±—ã —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–µ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É –æ–± –æ—à–∏–±–∫–µ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sys.exit(1)
            # –∏–ª–∏ –¥—Ä—É–≥–æ–π –º–µ—Ö–∞–Ω–∏–∑–º, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω –≤–æ—Ä–∫–µ—Ä–æ–º.
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏, –º—ã –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º —ç—Ç–æ, –Ω–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ
            # –∑–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø–∏—Å—å –≤ Redis
            # –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–ª–∞–≥–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω –≤–æ—Ä–∫–µ—Ä–æ–º.
            # –í —Ä–∞–º–∫–∞—Ö Scrapy, —á—Ç–æ–±—ã –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∫–æ–¥ –≤—ã—Ö–æ–¥–∞, –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CrawlerProcess
            # –∏ –µ–≥–æ exitcode, –∏–ª–∏ –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏.
            # –î–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏, –µ—Å–ª–∏ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–º–æ—Ç—Ä–∏—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ–¥ –≤—ã—Ö–æ–¥–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞,
            # —Ç–æ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Scrapy —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –∑–∞–≤–µ—Ä—à–∞–ª—Å—è —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º –∫–æ–¥–æ–º
            # –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ self.has_parsing_errors. –≠—Ç–æ –æ–±—ã—á–Ω–æ –¥–µ–ª–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑
            # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è Scrapy –∏–ª–∏ –ø—É—Ç–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–∞–ø—É—Å–∫–∞—é—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞.
            # –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ, —è –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É—é, —á—Ç–æ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞.
            pass


