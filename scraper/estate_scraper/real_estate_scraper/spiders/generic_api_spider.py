import scrapy
import asyncio
import os
import json
from urllib.parse import urljoin, urlencode, urlparse, parse_qs
from ..parsers.loader import load_config
from ..logger import get_scraping_logger


class UniversalSpider(scrapy.Spider):
    name = "generic_api"
    handle_httpstatus_list = [200, 400, 401, 403, 404, 429, 500, 502, 503]  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å –∫–æ–¥—ã
    
    def __init__(self, config=None, job_id=None, *args, **kwargs):
        super(UniversalSpider, self).__init__(*args, **kwargs)
        if not config:
            raise ValueError("Config parameter is required")
            
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if config.endswith('.yml') or config.endswith('.yaml'):
            self.config_path = config
        else:
            # –ò–Ω–∞—á–µ —Å—Ç—Ä–æ–∏–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É –ø–æ –∏–º–µ–Ω–∏
            current_dir = os.path.dirname(os.path.abspath(__file__))
            configs_dir = os.path.join(os.path.dirname(current_dir), "configs")
            self.config_path = os.path.join(configs_dir, f"{config}.yml")
        
        self.config = load_config(self.config_path)
        
        # –ú—É–ª—å—Ç–∏–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú –ü–ï–†–ï–î validate_config()
        self.base_url = self.config.get("base_url", "")
        self.categories = self.config.get("categories", [])
        self.api_settings = self.config.get("api_settings", {})
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥ –ü–û–°–õ–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤  
        self.validate_config()
        self.api_fields = self.config.get("api_fields", {})
        self.data_processing = self.config.get("data_processing", {})
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.job_id = job_id or os.environ.get('SCRAPY_JOB_ID', 'unknown')
        self.config_name = os.environ.get('SCRAPY_CONFIG_NAME', config or 'unknown')
        self.scraping_logger = get_scraping_logger(self.job_id, self.config_name)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.parse_all_listings = self.config.get("parse_all_listings", True)
        self.max_items_limit = int(self.config.get("max_items_limit", 100))
        self.scraped_items_count = 0
        self.category_items_count = {}  # –°—á–µ—Ç—á–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.total_items_expected = 0
        self.progress_update_interval = 10  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤

    def validate_config(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –∫–æ–Ω—Ñ–∏–≥–µ"""
        required_fields = ['base_url', 'categories', 'api_settings']
        for field in required_fields:
            if field not in self.config:
                raise ValueError(f"Missing required field '{field}' in config")

        if not self.categories:
            raise ValueError("No categories defined in config")

    def start_requests(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ —Å —Ä–∞–±–æ—á–µ–π –ª–æ–≥–∏–∫–æ–π"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É–∫
        main_url = self.base_url.replace('/api/search/v3/feed/search', '')
        if not main_url.endswith('/'):
            main_url += '/'
            
        # –ü–æ–ª—É—á–∞–µ–º –∫—É–∫–∏ —á–µ—Ä–µ–∑ Playwright (–†–ê–ë–û–ß–ê–Ø –õ–û–ì–ò–ö–ê!)
        if self.config.get('use_playwright', True):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π event loop Scrapy
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # –ï—Å–ª–∏ —Ü–∏–∫–ª —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(self._run_playwright_sync, main_url)
                            cookies, headers = future.result(timeout=30)
                    else:
                        cookies, headers = loop.run_until_complete(self.get_cookies_and_headers(main_url))
                except RuntimeError:
                    # –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ü–∏–∫–ª–æ–º, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                    cookies, headers = self._run_playwright_sync(main_url)
                
                cookie_header = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                
                base_headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Cookie': cookie_header,
                    'X-Requested-With': 'XMLHttpRequest',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'device': 'pc',
                    'country-id': '12',
                    'language': 'ru_RU'
                }
                
                self.logger.info(f"üç™ –ü–æ–ª—É—á–µ–Ω—ã –∫—É–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {len(cookies)} cookies")
                self.logger.info(f"üç™ Cookie header: {cookie_header[:100]}...")
                
            except Exception as e:
                self.logger.error(f"Error getting cookies and headers: {e}")
                base_headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'device': 'pc',
                    'country-id': '12',
                    'language': 'ru_RU'
                }
        else:
            base_headers = {
                'Accept': 'application/json, text/plain, */*',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'device': 'pc',
                'country-id': '12',
                'language': 'ru_RU'
            }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for category in self.categories:
            start_page = self.api_settings.get("start_page", 1)
            category_id = category.get("category_id")
            
            if not category_id:
                self.logger.error(f"Missing category_id for category: {category}")
                continue
            
            api_url = self._build_api_url(category_id, start_page)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ—Ñ–µ—Ä–∞—Ä–æ–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            headers_to_use = base_headers.copy()
            headers_to_use['Referer'] = category.get('referer', main_url)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
            self.logger.info(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category['name']} (ID: {category_id})")
            self.logger.info(f"üöÄ URL: {api_url}")
            self.logger.info(f"üöÄ Referer: {headers_to_use['Referer']}")
            
            yield scrapy.Request(
                url=api_url,
                headers=headers_to_use,
                callback=self.parse_api,
                meta={
                    'category': category,
                    'page': start_page,
                    'headers': headers_to_use  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ meta
                },
                errback=self.handle_error,
                dont_filter=True
            )

    def _run_playwright_sync(self, main_url):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Playwright –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        def run_async():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.get_cookies_and_headers(main_url))
            finally:
                loop.close()
        
        return run_async()
    
    async def get_cookies_and_headers(self, main_url):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫—É–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–µ—Ä–µ–∑ Playwright (–†–ê–ë–û–ß–ê–Ø –õ–û–ì–ò–ö–ê!)"""
        playwright_config = self.config.get('playwright', {})
        headless = playwright_config.get('headless', True)
        sleep_time = playwright_config.get('sleep_time', 3)
        
        try:
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=headless)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                page = await context.new_page()
                await page.goto(main_url)
                await asyncio.sleep(sleep_time)
                
                cookies = await context.cookies()
                await browser.close()
                
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—É–∫
                self.logger.info(f"üç™ –ü–æ–ª—É—á–µ–Ω–æ {len(cookies)} –∫—É–∫ —á–µ—Ä–µ–∑ Playwright:")
                for cookie in cookies:
                    self.logger.info(f"üç™   - {cookie['name']}: {cookie['value'][:50]}{'...' if len(cookie['value']) > 50 else ''}")
                
                headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                return cookies, headers
        except Exception as e:
            self.logger.error(f"Error getting cookies and headers: {e}")
            return [], {}

    def _build_api_url(self, category_id, page):
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞"""
        url_format = self.api_settings.get("url_format", "{base_url}?category_id={category_id}&page={page}")
        per_page = self.api_settings.get("per_page", 20)
        
        # –î–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä page
        if page <= 1:
            # –£–±–∏—Ä–∞–µ–º &page={page} –∏–∑ URL –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url_format = url_format.replace("&page={page}", "").replace("?page={page}&", "?").replace("?page={page}", "")
            return url_format.format(
                base_url=self.base_url,
                category_id=category_id,
                per_page=per_page
            )
        else:
            return url_format.format(
                base_url=self.base_url,
                category_id=category_id,
                per_page=per_page,
                page=page
            )

    def parse_api(self, response):
        """–ü–∞—Ä—Å–∏—Ç API –æ—Ç–≤–µ—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)"""
        category = response.meta.get('category')
        current_page = response.meta.get('page', 1)
        headers = response.meta.get('headers', {})
        
        # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –°–¢–ê–¢–£–° –ö–û–î–ê –ò –û–¢–í–ï–¢–ê
        self.logger.info(f"üì° API Response: {response.status} for {response.url}")
        self.logger.info(f"üì° Response headers: {dict(response.headers)}")
        
        if response.status != 200:
            self.logger.error(f"üö´ API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status} –¥–ª—è {response.url}")
            self.logger.error(f"üö´ Response text (first 1000 chars): {response.text[:1000]}")
            return
        
        if not category:
            self.logger.error("Category not found in response meta")
            return
        
        try:
            data = response.json()
            self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã JSON –¥–∞–Ω–Ω—ã–µ, –∫–ª—é—á–∏: {list(data.keys()) if isinstance(data, dict) else type(data)}")
        except ValueError as e:
            self.logger.error(f"Invalid JSON in response from {response.url}: {e}")
            self.logger.debug(f"Response text: {response.text[:500]}...")
            return

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        items_key = self.api_fields.get('items_key', 'items')
        items = self._get_nested_value(data, items_key)
        
        if not isinstance(items, list):
            self.logger.warning(f"Expected items to be a list, got: {type(items)} for key '{items_key}'")
            self.logger.debug(f"Data structure: {list(data.keys()) if isinstance(data, dict) else type(data)}")
            return

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        category_name = category['name']
        if category_name not in self.category_items_count:
            self.category_items_count[category_name] = 0
        
        items_processed = 0
        for item in items:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú –µ—Å–ª–∏ parse_all_listings = False
            if not self.parse_all_listings and self.category_items_count[category_name] >= self.max_items_limit:
                self.logger.info(f"Reached max items limit for category '{category_name}': {self.max_items_limit}")
                break
                
            try:
                processed_item = self._process_api_item(item, category)
                if processed_item:
                    items_processed += 1
                    self.scraped_items_count += 1  # –û–±—â–∏–π —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                    self.category_items_count[category_name] += 1  # –°—á–µ—Ç—á–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ N —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    if self.scraped_items_count % self.progress_update_interval == 0:
                        self._update_progress()
                    
                    yield processed_item
                    
            except Exception as e:
                self.logger.error(f"Error processing item: {e}")
                continue
        
        self.logger.info(f"Processed {items_processed} items from page {current_page} for category {category['name']}")
        self.scraping_logger.log_page_processed(current_page, items_processed, response.url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        if self._should_continue_pagination(items_processed, category):
            yield from self._handle_pagination(response, category, current_page, headers)

    def _process_api_item(self, item, category):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –∏–∑ API"""
        try:
            result = {
                # –¢–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑ category_id
                'property_type': category['property_type'],
                'listing_type': category['listing_type'],
                'source': self.config.get('source_name', 'unknown'),
                'category_name': category['name'],
                'category_id': category.get('category_id')
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª—è API
            item_fields = self.api_fields.get('item_fields', {})
            for output_field, input_path in item_fields.items():
                try:
                    value = self._get_nested_value(item, input_path)
                    result[output_field] = value
                except Exception as e:
                    self.logger.warning(f"Error processing field '{output_field}': {e}")
                    result[output_field] = None
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            url_building = self.config.get('url_building', {})
            if url_building and result.get('url'):
                pattern = url_building.get('pattern', '')
                if pattern:
                    result['url'] = pattern.format(**result)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞
            validated_result = self._validate_and_clean_item(result)
            return validated_result
            
        except Exception as e:
            self.logger.error(f"Error processing API item: {e}")
            return None

    def _get_nested_value(self, data, path):
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –≤–ª–æ–∂–µ–Ω–Ω–æ–º—É –ø—É—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'data.items.0.title')"""
        if not path or not data:
            return None
            
        try:
            keys = str(path).split('.')
            value = data
            
            for key in keys:
                if isinstance(value, dict):
                    value = value.get(key)
                elif isinstance(value, list) and key.isdigit():
                    index = int(key)
                    value = value[index] if 0 <= index < len(value) else None
                else:
                    return None
                    
                if value is None:
                    return None
                    
            return value
            
        except Exception as e:
            self.logger.warning(f"Error getting nested value for path '{path}': {e}")
            return None

    def _update_progress(self):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            total_categories = len(self.categories)
            processed_categories = len([cat for cat in self.categories if self.category_items_count.get(cat['name'], 0) > 0])
            
            if total_categories > 0:
                progress = min(95, int((processed_categories / total_categories) * 100))
            else:
                progress = 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (WebSocket –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            self.logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress}%, —Å–ø–∞—Ä—Å–µ–Ω–æ: {self.scraped_items_count}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")

    def _validate_and_clean_item(self, item):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            required_fields = self.data_processing.get('validate_required_fields', [])
            for field in required_fields:
                if not item.get(field):
                    self.logger.warning(f"Missing required field '{field}' in item")
                    return None
            
            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π
            string_fields = ['title', 'description', 'city', 'district', 'address']
            for field in string_fields:
                if field in item and item[field]:
                    if isinstance(item[field], str):
                        item[field] = item[field].strip()
                        if not item[field]:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                            item[field] = None
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±—É–ª–µ–≤—ã—Ö –ø–æ–ª–µ–π (—É–±—Ä–∞–ª–∏ is_realtor, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –ø–æ–ª–µ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
            bool_fields = []
            for field in bool_fields:
                if field in item and item[field] is not None:
                    item[field] = bool(item[field])
            
            return item
            
        except Exception as e:
            self.logger.error(f"Error validating item: {e}")
            return None

    def _should_continue_pagination(self, items_on_current_page, category):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_name = category['name']
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if not self.parse_all_listings and self.category_items_count.get(category_name, 0) >= self.max_items_limit:
            return False
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        if items_on_current_page == 0:
            return False
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ–º –æ–∂–∏–¥–∞–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        per_page = self.api_settings.get("per_page", 20)
        if items_on_current_page < per_page:
            return False
                
        return True

    def _handle_pagination(self, response, category, current_page, headers):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è API"""
        try:
            next_page = current_page + 1
            next_url = self._build_api_url(category.get('category_id'), next_page)
            
            self.logger.info(f"Following to page {next_page} for category {category['name']}: {next_url}")
            
            yield scrapy.Request(
                url=next_url,
                headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ –∂–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∫—É–∫–∞–º–∏
                callback=self.parse_api,
                meta={
                    'category': category,
                    'page': next_page,
                    'headers': headers
                },
                errback=self.handle_error,
                dont_filter=True
            )
                
        except Exception as e:
            self.logger.error(f"Error in pagination: {e}")

    def handle_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
        try:
            self.logger.error(f"Request failed: {failure.request.url}")
            self.logger.error(f"Error: {failure.value}")
            self.scraping_logger.log_request_failure(failure.request.url, str(failure.value))
        except Exception as e:
            self.logger.error(f"Error in error handler: {e}")

    def closed(self, reason):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–ø–∞–π–¥–µ—Ä–∞"""
        try:
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._update_progress()
            
            stats = {
                'scraped_items': self.scraped_items_count,
                'reason': reason,
                'category_breakdown': self.category_items_count
            }
            
            self.logger.info(f"Spider closed: {stats}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for category_name, count in self.category_items_count.items():
                self.logger.info(f"  üìÇ {category_name}: {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            self.scraping_logger.log_spider_finished(stats)
            
        except Exception as e:
            self.logger.error(f"Error in spider close: {e}")
