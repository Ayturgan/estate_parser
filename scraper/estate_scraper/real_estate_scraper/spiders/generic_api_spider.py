import scrapy
import asyncio
import os
import json
from urllib.parse import urljoin, urlencode, urlparse, parse_qs
from ..parsers.loader import load_config
from ..logger import get_scraping_logger


class UniversalSpider(scrapy.Spider):
    name = "generic_api"
    handle_httpstatus_list = [200, 400, 401, 403, 404, 429, 500, 502, 503]  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å –∫–æ–¥—ã
    
    def __init__(self, config=None, job_id=None, *args, **kwargs):
        super(UniversalSpider, self).__init__(*args, **kwargs)
        if not config:
            raise ValueError("Config parameter is required")
            
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if config.endswith('.yml') or config.endswith('.yaml'):
            self.config_path = config
        else:
            # –ò–Ω–∞—á–µ —Å—Ç—Ä–æ–∏–º –ø—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É –ø–æ –∏–º–µ–Ω–∏
            current_dir = os.path.dirname(os.path.abspath(__file__))
            configs_dir = os.path.join(os.path.dirname(current_dir), "configs")
            self.config_path = os.path.join(configs_dir, f"{config}.yml")
        
        self.config = load_config(self.config_path)
        
        # –ú—É–ª—å—Ç–∏–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú –ü–ï–†–ï–î validate_config()
        self.base_url = self.config.get("base_url", "")
        self.categories = self.config.get("categories", [])
        self.api_settings = self.config.get("api_settings", {})
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥ –ü–û–°–õ–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤  
        self.validate_config()
        self.api_fields = self.config.get("api_fields", {})
        self.data_processing = self.config.get("data_processing", {})
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.job_id = job_id or os.environ.get('SCRAPY_JOB_ID', 'unknown')
        self.config_name = os.environ.get('SCRAPY_CONFIG_NAME', config or 'unknown')
        self.scraping_logger = get_scraping_logger(self.job_id, self.config_name)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.parse_all_listings = self.config.get("parse_all_listings", True)
        self.max_items_limit = int(self.config.get("max_items_limit", 100))
        self.scraped_items_count = 0
        self.category_items_count = {}  # –°—á–µ—Ç—á–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        self.has_parsing_errors = False # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API
        self.detail_api = self.config.get("detail_api", {})
        self.detail_api_enabled = self.detail_api.get("enabled", False)
        self.common_params_mapping = self.detail_api.get("common_params_mapping", {})
        self.property_type_params_mapping = self.detail_api.get("property_type_params_mapping", {})
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.total_items_expected = 0
        self.progress_update_interval = 10  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤

    def validate_config(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –≤ –∫–æ–Ω—Ñ–∏–≥–µ"""
        required_fields = ['base_url', 'categories', 'api_settings']
        for field in required_fields:
            if field not in self.config:
                raise ValueError(f"Missing required field '{field}' in config")

        if not self.categories:
            raise ValueError("No categories defined in config")

    def start_requests(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ —Å —Ä–∞–±–æ—á–µ–π –ª–æ–≥–∏–∫–æ–π"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É–∫
        main_url = self.base_url.replace('/api/search/v3/feed/search', '')
        if not main_url.endswith('/'):
            main_url += '/'
            
        # –ü–æ–ª—É—á–∞–µ–º –∫—É–∫–∏ —á–µ—Ä–µ–∑ Playwright (–†–ê–ë–û–ß–ê–Ø –õ–û–ì–ò–ö–ê!)
        if self.config.get('use_playwright', True):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π event loop Scrapy
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # –ï—Å–ª–∏ —Ü–∏–∫–ª —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(self._run_playwright_sync, main_url)
                            cookies, headers = future.result(timeout=30)
                    else:
                        cookies, headers = loop.run_until_complete(self.get_cookies_and_headers(main_url))
                except RuntimeError:
                    # –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ü–∏–∫–ª–æ–º, –∑–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
                    cookies, headers = self._run_playwright_sync(main_url)
                
                cookie_header = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
                
                base_headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Cookie': cookie_header,
                    'X-Requested-With': 'XMLHttpRequest',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'device': 'pc',
                    'country-id': '12',
                    'language': 'ru_RU'
                }
                
                self.logger.info(f"üç™ –ü–æ–ª—É—á–µ–Ω—ã –∫—É–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {len(cookies)} cookies")
                self.logger.info(f"üç™ Cookie header: {cookie_header[:100]}...")
                
            except Exception as e:
                self.logger.error(f"Error getting cookies and headers: {e}")
                base_headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'device': 'pc',
                    'country-id': '12',
                    'language': 'ru_RU'
                }
        else:
            base_headers = {
                'Accept': 'application/json, text/plain, */*',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'device': 'pc',
                'country-id': '12',
                'language': 'ru_RU'
            }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        for category in self.categories:
            start_page = self.api_settings.get("start_page", 1)
            category_id = category.get("category_id")
            
            if not category_id:
                self.logger.error(f"Missing category_id for category: {category}")
                continue
            
            api_url = self._build_api_url(category_id, start_page)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ—Ñ–µ—Ä–∞—Ä–æ–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            headers_to_use = base_headers.copy()
            headers_to_use['Referer'] = category.get('referer', main_url)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
            self.logger.info(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category['name']} (ID: {category_id})")
            self.logger.info(f"üöÄ URL: {api_url}")
            self.logger.info(f"üöÄ Referer: {headers_to_use['Referer']}")
            
            yield scrapy.Request(
                url=api_url,
                headers=headers_to_use,
                callback=self.parse_api,
                meta={
                    'category': category,
                    'page': start_page,
                    'headers': headers_to_use  # –ü–µ—Ä–µ–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤ meta
                },
                errback=self.handle_error,
                dont_filter=True
            )

    def _run_playwright_sync(self, main_url):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Playwright –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        def run_async():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.get_cookies_and_headers(main_url))
            finally:
                loop.close()
        
        return run_async()
    
    async def get_cookies_and_headers(self, main_url):
        """–ü–æ–ª—É—á–∞–µ—Ç –∫—É–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–µ—Ä–µ–∑ Playwright (–†–ê–ë–û–ß–ê–Ø –õ–û–ì–ò–ö–ê!)"""
        playwright_config = self.config.get('playwright', {})
        headless = playwright_config.get('headless', True)
        sleep_time = playwright_config.get('sleep_time', 3)
        
        try:
            from playwright.async_api import async_playwright
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=headless)
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                )
                
                page = await context.new_page()
                await page.goto(main_url)
                await asyncio.sleep(sleep_time)
                
                cookies = await context.cookies()
                await browser.close()
                
                # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—É–∫
                self.logger.info(f"üç™ –ü–æ–ª—É—á–µ–Ω–æ {len(cookies)} –∫—É–∫ —á–µ—Ä–µ–∑ Playwright:")
                for cookie in cookies:
                    self.logger.info(f"üç™   - {cookie['name']}: {cookie['value'][:50]}{'...' if len(cookie['value']) > 50 else ''}")
                
                headers = {
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                return cookies, headers
        except Exception as e:
            self.logger.error(f"Error getting cookies and headers: {e}")
            return [], {}

    def _build_api_url(self, category_id, page):
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è API –∑–∞–ø—Ä–æ—Å–∞"""
        url_format = self.api_settings.get("url_format", "{base_url}?category_id={category_id}&page={page}")
        per_page = self.api_settings.get("per_page", 20)
        
        # –î–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä page
        if page <= 1:
            # –£–±–∏—Ä–∞–µ–º &page={page} –∏–∑ URL –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url_format = url_format.replace("&page={page}", "").replace("?page={page}&", "?").replace("?page={page}", "")
            return url_format.format(
                base_url=self.base_url,
                category_id=category_id,
                per_page=per_page
            )
        else:
            return url_format.format(
                base_url=self.base_url,
                category_id=category_id,
                per_page=per_page,
                page=page
            )

    def parse_api(self, response):
        """–ü–∞—Ä—Å–∏—Ç API –æ—Ç–≤–µ—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ê–î–ê–ü–¢–ò–†–û–í–ê–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê)"""
        category = response.meta.get('category')
        current_page = response.meta.get('page', 1)
        headers = response.meta.get('headers', {})
        
        # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –°–¢–ê–¢–£–° –ö–û–î–ê –ò –û–¢–í–ï–¢–ê
        self.logger.info(f"üì° API Response: {response.status} for {response.url}")
        self.logger.info(f"üì° Response headers: {dict(response.headers)}")
        
        if response.status != 200:
            self.logger.error(f"üö´ API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status} –¥–ª—è {response.url}")
            self.logger.error(f"üö´ Response text (first 1000 chars): {response.text[:1000]}")
            return
        
        if not category:
            self.logger.error("Category not found in response meta")
            return
        
        try:
            data = response.json()
            self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã JSON –¥–∞–Ω–Ω—ã–µ, –∫–ª—é—á–∏: {list(data.keys()) if isinstance(data, dict) else type(data)}")
        except ValueError as e:
            self.logger.error(f"Invalid JSON in response from {response.url}: {e}")
            self.logger.debug(f"Response text: {response.text[:500]}...")
            self.has_parsing_errors = True
            return

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        items_key = self.api_fields.get('items_key', 'items')
        items = self._get_nested_value(data, items_key)
        
        if not isinstance(items, list):
            self.logger.warning(f"Expected items to be a list, got: {type(items)} for key '{items_key}'")
            self.logger.debug(f"Data structure: {list(data.keys()) if isinstance(data, dict) else type(data)}")
            return

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        category_name = category['name']
        if category_name not in self.category_items_count:
            self.category_items_count[category_name] = 0
        
        items_processed = 0
        for item in items:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú –µ—Å–ª–∏ parse_all_listings = False
            if not self.parse_all_listings and self.category_items_count[category_name] >= self.max_items_limit:
                self.logger.info(f"Reached max items limit for category '{category_name}': {self.max_items_limit}")
                break
                
            try:
                processed_item = self._process_api_item(item, category)
                if processed_item:
                    # –ï—Å–ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π API –≤–∫–ª—é—á–µ–Ω, processed_item –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –∑–∞–ø—Ä–æ—Å–æ–≤
                    if self.detail_api_enabled and hasattr(processed_item, '__iter__'):
                        # –≠—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É API
                        yield from processed_item
                    else:
                        # –û–±—ã—á–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API
                        items_processed += 1
                        self.scraped_items_count += 1  # –û–±—â–∏–π —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                        self.category_items_count[category_name] += 1  # –°—á–µ—Ç—á–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ N —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                        if self.scraped_items_count % self.progress_update_interval == 0:
                            self._update_progress()
                        
                        yield processed_item
                    
            except Exception as e:
                self.logger.error(f"Error processing item: {e}")
                continue
        
        self.logger.info(f"Processed {items_processed} items from page {current_page} for category {category['name']}")
        self.scraping_logger.log_page_processed(current_page, items_processed, response.url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        if self._should_continue_pagination(items_processed, category):
            yield from self._handle_pagination(response, category, current_page, headers)

    def _process_api_item(self, item, category):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –∏–∑ API"""
        try:
            result = {
                # –¢–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑ category_id
                'property_type': category['property_type'],
                'listing_type': category['listing_type'],
                'source': self.config.get('source_name', 'unknown'),
                'category_name': category['name'],
                'category_id': category.get('category_id')
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª—è API
            item_fields = self.api_fields.get('item_fields', {})
            for output_field, input_path in item_fields.items():
                try:
                    value = self._get_nested_value(item, input_path)
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º source_id –≤ —Å—Ç—Ä–æ–∫—É
                    if output_field == 'source_id' and value is not None:
                        value = str(value)
                    
                    result[output_field] = value
                except Exception as e:
                    self.logger.warning(f"Error processing field '{output_field}': {e}")
                    self.has_parsing_errors = True
                    result[output_field] = None
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            url_building = self.config.get('url_building', {})
            if url_building and result.get('url'):
                pattern = url_building.get('pattern', '')
                if pattern:
                    result['url'] = pattern.format(**result)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞
            validated_result = self._validate_and_clean_item(result)
            
            # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π API –∏ –µ—Å—Ç—å source_id, –¥–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É API
            if self.detail_api_enabled and validated_result and validated_result.get('source_id'):
                return self._request_detail_api(validated_result, category)
            else:
                return validated_result
            
        except Exception as e:
            self.logger.error(f"Error processing API item: {e}")
            return None

    def _request_detail_api(self, item, category):
        """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        try:
            source_id = item.get('source_id')
            if not source_id:
                self.logger.warning(f"No source_id found for item: {item.get('title', 'Unknown')}")
                return item
            
            # –°—Ç—Ä–æ–∏–º URL –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API
            detail_url_format = self.detail_api.get('url_format', '')
            if not detail_url_format:
                self.logger.warning("No detail API URL format configured")
                return item
            
            detail_url = detail_url_format.format(source_id=source_id)
            
            self.logger.info(f"üîç –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è {source_id}: {detail_url}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á—Ç–æ –∏ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ API
            headers = {
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'X-Requested-With': 'XMLHttpRequest',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'device': 'pc',
                'country-id': '12',
                'language': 'ru_RU'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ—Ñ–µ—Ä–∞—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
            if category.get('referer'):
                headers['Referer'] = category['referer']
            
            yield scrapy.Request(
                url=detail_url,
                headers=headers,
                callback=self._parse_detail_api,
                meta={
                    'original_item': item,
                    'category': category,
                    'source_id': source_id
                },
                errback=self._handle_detail_error,
                dont_filter=True
            )
            
        except Exception as e:
            self.logger.error(f"Error requesting detail API: {e}")
            return item

    def _parse_detail_api(self, response):
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API"""
        try:
            original_item = response.meta.get('original_item', {})
            category = response.meta.get('category', {})
            source_id = response.meta.get('source_id')
            
            self.logger.info(f"üì° Detail API Response: {response.status} for {response.url}")
            
            if response.status != 200:
                self.logger.error(f"üö´ Detail API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status} –¥–ª—è {response.url}")
                return original_item
            
            try:
                data = response.json()
                self.logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –¥–µ—Ç–∞–ª—å–Ω—ã–µ JSON –¥–∞–Ω–Ω—ã–µ")
            except ValueError as e:
                self.logger.error(f"Invalid JSON in detail response: {e}")
                return original_item
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º params –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API
            params = data.get('params', [])
            if not isinstance(params, list):
                self.logger.warning(f"Expected params to be a list, got: {type(params)}")
                return original_item
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API
            enriched_item = original_item.copy()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
            for param in params:
                param_id = param.get('id')
                param_name = param.get('name', '')
                param_value = param.get('value', '')
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤
                property_type = enriched_item.get('property_type', '')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–µ –º–∞–ø–ø–∏–Ω–≥–∏
                if param_id and param_id in self.common_params_mapping:
                    field_name = self.common_params_mapping[param_id]
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª—è
                    if field_name == 'district':
                        # –†–∞–π–æ–Ω –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –ë–î
                        enriched_item['district'] = param_value
                        self.logger.debug(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ä–∞–π–æ–Ω: {param_value}")
                    
                    else:
                        # –û–±—ã—á–Ω—ã–µ –ø–æ–ª—è –ë–î (–≤–∫–ª—é—á–∞—è condition –∏ building_type)
                        enriched_item[field_name] = param_value
                        self.logger.debug(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø–æ–ª–µ –ë–î {field_name}: {param_value}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
                elif property_type and property_type in self.property_type_params_mapping:
                    type_mapping = self.property_type_params_mapping[property_type]
                    if param_id and param_id in type_mapping:
                        field_name = type_mapping[param_id]
                        
                        # –í—Å–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–ª—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ attributes
                        if 'attributes' not in enriched_item:
                            enriched_item['attributes'] = {}
                        enriched_item['attributes'][field_name] = param_value
                        self.logger.debug(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç {field_name}: {param_value}")
                
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ attributes
                    if 'attributes' not in enriched_item:
                        enriched_item['attributes'] = {}
                    enriched_item['attributes'][f"param_{param_id}"] = {
                        'name': param_name,
                        'value': param_value
                    }
                    self.logger.debug(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä {param_id}: {param_name} = {param_value}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            validated_result = self._validate_and_clean_item(enriched_item)
            
            if validated_result:
                self.scraped_items_count += 1
                self.category_items_count[category['name']] = self.category_items_count.get(category['name'], 0) + 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if self.scraped_items_count % self.progress_update_interval == 0:
                    self._update_progress()
                
                self.logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {source_id}: {validated_result.get('title', 'Unknown')}")
                yield validated_result
            else:
                self.logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ {source_id}")
            
        except Exception as e:
            self.logger.error(f"Error parsing detail API: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            if response.meta.get('original_item'):
                self.scraped_items_count += 1
                yield response.meta.get('original_item', {})

    def _handle_detail_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ API"""
        try:
            original_item = failure.request.meta.get('original_item', {})
            source_id = failure.request.meta.get('source_id', 'unknown')
            
            self.logger.error(f"Detail API request failed for {source_id}: {failure.request.url}")
            self.logger.error(f"Error: {failure.value}")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
            error_str = str(failure.value).lower()
            if any(network_error in error_str for network_error in [
                'dns lookup failed', 'connection refused', 'connection timeout',
                'network unreachable', 'host unreachable', 'request failed'
            ]):
                self.has_parsing_errors = True
                self.logger.error("Detail API network error detected, setting parsing errors flag")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–µ–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            if original_item:
                self.scraped_items_count += 1
                yield original_item
            
        except Exception as e:
            self.logger.error(f"Error in detail error handler: {e}")

    def _get_nested_value(self, data, path):
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –≤–ª–æ–∂–µ–Ω–Ω–æ–º—É –ø—É—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'data.items.0.title')"""
        if not path or not data:
            return None
            
        try:
            keys = str(path).split('.')
            value = data
            
            for key in keys:
                if isinstance(value, dict):
                    value = value.get(key)
                elif isinstance(value, list) and key.isdigit():
                    index = int(key)
                    value = value[index] if 0 <= index < len(value) else None
                else:
                    return None
                    
                if value is None:
                    return None
                    
            return value
            
        except Exception as e:
            self.logger.warning(f"Error getting nested value for path '{path}': {e}")
            return None

    def _update_progress(self):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            total_categories = len(self.categories)
            processed_categories = len([cat for cat in self.categories if self.category_items_count.get(cat['name'], 0) > 0])
            
            if total_categories > 0:
                progress = min(95, int((processed_categories / total_categories) * 100))
            else:
                progress = 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (WebSocket –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            self.logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress}%, —Å–ø–∞—Ä—Å–µ–Ω–æ: {self.scraped_items_count}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")

    def _validate_and_clean_item(self, item):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –±–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            required_fields = self.data_processing.get('validate_required_fields', [])
            for field in required_fields:
                if not item.get(field):
                    self.logger.warning(f"Missing required field '{field}' in item")
                    return None
            
            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –ø–æ–ª–µ–π
            string_fields = ['title', 'description', 'city', 'district', 'address', 
                           'furniture', 'heating', 'condition', 'building_type', 'utilities']
            for field in string_fields:
                if field in item and item[field]:
                    if isinstance(item[field], str):
                        item[field] = item[field].strip()
                        if not item[field]:  # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                            item[field] = None
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π
            numeric_fields = ['rooms', 'area_sqm', 'land_area_sotka', 'floor', 'total_floors', 'ceiling_height']
            for field in numeric_fields:
                if field in item and item[field] is not None:
                    try:
                        if isinstance(item[field], str):
                            # –£–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "5 –∫–æ–º–Ω–∞—Ç" -> 5)
                            import re
                            number_match = re.search(r'(\d+(?:\.\d+)?)', str(item[field]))
                            if number_match:
                                item[field] = float(number_match.group(1))
                            else:
                                item[field] = None
                        else:
                            item[field] = float(item[field])
                    except (ValueError, TypeError):
                        item[field] = None
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±—É–ª–µ–≤—ã—Ö –ø–æ–ª–µ–π
            bool_fields = []
            for field in bool_fields:
                if field in item and item[field] is not None:
                    item[field] = bool(item[field])
            
            return item
            
        except Exception as e:
            self.logger.error(f"Error validating item: {e}")
            return None

    def _should_continue_pagination(self, items_on_current_page, category):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        category_name = category['name']
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if not self.parse_all_listings and self.category_items_count.get(category_name, 0) >= self.max_items_limit:
            return False
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        if items_on_current_page == 0:
            return False
        
        # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ–º –æ–∂–∏–¥–∞–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        per_page = self.api_settings.get("per_page", 20)
        if items_on_current_page < per_page:
            return False
                
        return True

    def _handle_pagination(self, response, category, current_page, headers):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è API"""
        try:
            next_page = current_page + 1
            next_url = self._build_api_url(category.get('category_id'), next_page)
            
            self.logger.info(f"Following to page {next_page} for category {category['name']}: {next_url}")
            
            yield scrapy.Request(
                url=next_url,
                headers=headers,  # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ –∂–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∫—É–∫–∞–º–∏
                callback=self.parse_api,
                meta={
                    'category': category,
                    'page': next_page,
                    'headers': headers
                },
                errback=self.handle_error,
                dont_filter=True
            )
                
        except Exception as e:
            self.logger.error(f"Error in pagination: {e}")

    def handle_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫"""
        try:
            self.logger.error(f"Request failed: {failure.request.url}")
            self.logger.error(f"Error: {failure.value}")
            self.scraping_logger.log_request_failure(failure.request.url, str(failure.value))
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
            error_str = str(failure.value).lower()
            if any(network_error in error_str for network_error in [
                'dns lookup failed', 'connection refused', 'connection timeout',
                'network unreachable', 'host unreachable', 'request failed'
            ]):
                self.has_parsing_errors = True
                self.logger.error("Network error detected, setting parsing errors flag")
                
        except Exception as e:
            self.logger.error(f"Error in error handler: {e}")

    def closed(self, reason):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–ø–∞–π–¥–µ—Ä–∞"""
        try:
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._update_progress()
            
            stats = {
                'scraped_items': self.scraped_items_count,
                'reason': reason,
                'category_breakdown': self.category_items_count
            }
            
            self.logger.info(f"Spider closed: {stats}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for category_name, count in self.category_items_count.items():
                self.logger.info(f"  üìÇ {category_name}: {count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            
            self.scraping_logger.log_spider_finished(stats)
            
        except Exception as e:
            self.logger.error(f"Error in spider close: {e}")


    def closed(self, reason):
        if self.has_parsing_errors:
            self.logger.error("Spider finished with parsing errors. Signalling failure.")
            pass


