import scrapy
import random
from scrapy_playwright.page import PageMethod
from ..parsers.loader import load_config
from ..logger import get_scraping_logger
import logging
import os


class GenericShowMoreSimpleSpider(scrapy.Spider):
    name = "generic_show_more_simple"
    custom_settings = {
        'DOWNLOAD_HANDLERS': {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            "headless": True,
            "args": [
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
                "--disable-background-timer-throttling",
                "--disable-backgrounding-occluded-windows",
                "--disable-renderer-backgrounding",
                "--disable-ipc-flooding-protection",
                "--memory-pressure-off",
                "--max_old_space_size=4096"
            ]
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 80000,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2 –º–∏–Ω—É—Ç
        'PLAYWRIGHT_PAGE_METHODS': [
            PageMethod("wait_for_load_state", "networkidle"),
        ],
        'DOWNLOAD_DELAY': 1,  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 4,  # –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑
        'CONCURRENT_REQUESTS_PER_DOMAIN': 4
    }

    def __init__(self, config=None, job_id=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if not config:
            raise ValueError("Path to config file must be provided via -a config=...")
        self.config_path = config
        self.config = load_config(self.config_path)
        
        self.base_url = self.config.get('base_url', '')
        self.start_url = self.config.get('start_url', '/')
        self.selectors = self.config.get('selectors', {})
        self.show_more_settings = self.config.get('show_more_settings', {})
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.parse_all_listings = self.config.get('parse_all_listings', False)
        self.max_items_limit = self.config.get('max_items_limit', 100)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.scraped_items_count = 0
        self.processed_items = 0
        self.failed_items = 0
        self.progress_update_interval = 10
        
        # –õ–æ–≥–≥–µ—Ä
        self.job_id = job_id or os.environ.get('SCRAPY_JOB_ID', 'unknown')
        self.config_name = os.environ.get('SCRAPY_CONFIG_NAME', config or 'unknown')
        self.scraping_logger = get_scraping_logger(self.job_id, self.config_name)
        self.has_parsing_errors = False # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
    def start_requests(self):
        """–ù–∞—á–∏–Ω–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        url = self.base_url + self.start_url
        self.logger.info(f"Starting scraping from: {url}")
        
        yield scrapy.Request(
            url,
            callback=self.parse,
            meta={
                'playwright': True,
                'playwright_include_page': True,
                'playwright_page_methods': [
                    PageMethod("wait_for_load_state", "networkidle"),
                ]
            },
            errback=self.handle_error,
            dont_filter=True
        )

    def parse(self, response):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        page = response.meta.get('playwright_page')
        
        if not page:
            self.logger.error("Playwright page not found")
            return

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–Ω–æ–ø–∫–∏ "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ"
        show_more_enabled = self.show_more_settings.get('enabled', False)
        button_selector = self.show_more_settings.get('button_selector', '')
        max_clicks = self.show_more_settings.get('max_clicks', 5)
        wait_time = self.show_more_settings.get('wait_time', 3)
        scroll_before_click = self.show_more_settings.get('scroll_before_click', True)
        
        # –ö–ª–∏–∫–∞–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ" –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if show_more_enabled and button_selector:
            self._handle_show_more(page, button_selector, max_clicks, wait_time, scroll_before_click)
        
        # –ü–∞—Ä—Å–∏–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        yield from self._parse_current_page(response)

    def _handle_show_more(self, page, button_selector, max_clicks, wait_time, scroll_before_click):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–ª–∏–∫–∏ –ø–æ –∫–Ω–æ–ø–∫–µ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ'"""
        try:
            clicks_count = 0
            
            while clicks_count < max_clicks:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–Ω–æ–ø–∫–∏
                button = page.locator(button_selector)
                if not button.count():
                    self.logger.info(f"–ö–Ω–æ–ø–∫–∞ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ—Å–ª–µ {clicks_count} –∫–ª–∏–∫–æ–≤")
                    break
                
                # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –∫ –∫–Ω–æ–ø–∫–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if scroll_before_click:
                    button.scroll_into_view_if_needed()
                
                # –ö–ª–∏–∫–∞–µ–º –ø–æ –∫–Ω–æ–ø–∫–µ
                button.click()
                clicks_count += 1
                
                self.logger.info(f"–ö–ª–∏–∫ #{clicks_count} –ø–æ –∫–Ω–æ–ø–∫–µ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ'")
                
                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                page.wait_for_timeout(wait_time * 1000)
                
                # –ñ–¥–µ–º –ø–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–æ–∫–æ–∏—Ç—Å—è
                page.wait_for_load_state("networkidle")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–Ω–æ–ø–∫–∏ '–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ': {e}")


    def _parse_current_page(self, response):
        """–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        ads_list_selector = self.selectors.get("ads_list")
        ad_card_selector = self.selectors.get("ad_card")
        
        if not ads_list_selector or not ad_card_selector:
            self.logger.error("Required selectors (ads_list, ad_card) not found in config")
            return

        ads_container = response.css(ads_list_selector)
        if not ads_container:
            self.logger.warning(f"No ads container found with selector: {ads_list_selector}")
            return

        items_found = 0
        for element in ads_container.css(ad_card_selector):
            if not self.parse_all_listings and self.scraped_items_count >= self.max_items_limit:
                self.logger.info(f"Reached max items limit: {self.max_items_limit}")
                return
                
            items_found += 1
            self.scraped_items_count += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ N —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if self.scraped_items_count % self.progress_update_interval == 0:
                self._update_progress()
                
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                item_data = self._extract_item_data(element)
                if item_data:
                    detail_url = item_data.get('url')
                    details_selectors = self.selectors.get('details', {})
                    
                    if detail_url and details_selectors:
                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        yield scrapy.Request(
                            detail_url,
                            callback=self.parse_details,
                            meta={'item_data': item_data},
                            errback=self.handle_error,
                            dont_filter=True
                        )
                    else:
                        yield item_data
            except Exception as e:
                self.logger.error(f"Error processing item: {e}")
                self.failed_items += 1

        self.logger.info(f"Found {items_found} items on page")
        if self.scraping_logger:
            self.scraping_logger.log_page_processed(1, items_found, response.url)

    def _extract_item_data(self, element):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            item_data = {
                'source': self.config.get('source_name', 'unknown'),
            }
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
            basic_fields = ['title', 'url', 'price', 'location', 'description']
            for field in basic_fields:
                selector = self.selectors.get(field)
                if selector:
                    value = self._extract_field_value(element, selector)
                    item_data[field] = value
                    # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    if field in ['title', 'url']:
                        self.logger.info(f"Field '{field}': selector='{selector}', value='{value}'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø—ã –∏–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤
            property_type_selector = self.selectors.get('property_type')
            if property_type_selector:
                property_type = self._extract_field_value(element, property_type_selector)
                if property_type:
                    item_data['property_type'] = property_type
                    self.logger.info(f"Property type from selector: '{property_type}'")
            
            listing_type_selector = self.selectors.get('listing_type')
            if listing_type_selector:
                listing_type = self._extract_field_value(element, listing_type_selector)
                if listing_type:
                    item_data['listing_type'] = listing_type
                    self.logger.info(f"Listing type from selector: '{listing_type}'")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL
            if item_data.get('url') and not item_data['url'].startswith('http'):
                # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω–æ–π —Å–ª–µ—à
                base_url = self.base_url.rstrip('/')
                url = item_data['url'].lstrip('/')
                item_data['url'] = f"{base_url}/{url}"
            
            # –î–æ–±–∞–≤–ª—è–µ–º source_url –¥–ª—è API
            if item_data.get('url'):
                item_data['source_url'] = item_data['url']
            
            # –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –±—É–¥—É—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å—Å—è –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            
            self.logger.info(f"Extracted item data: {item_data}")
            return item_data
        except Exception as e:
            self.logger.error(f"Error extracting item data: {e}")
            self.has_parsing_errors = True
            return None

    def _extract_photos(self, element):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            photos = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            images_selector = self.selectors.get('images')
            self.logger.info(f"üîç Photo extraction: images_selector from main selectors = '{images_selector}'")
            
            if not images_selector:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—â–µ–º –≤ –¥–µ—Ç–∞–ª—è—Ö
                details = self.selectors.get('details', {})
                images_selector = details.get('images')
                self.logger.info(f"üîç Photo extraction: images_selector from details = '{images_selector}'")
            
            if images_selector:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_elements = self._extract_field_elements(element, images_selector)
                self.logger.info(f"üîç Photo extraction: found {len(image_elements)} image elements")
                
                for i, img_url in enumerate(image_elements):
                    if img_url:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –ø–æ–ª–Ω—ã–π
                        if not img_url.startswith('http'):
                            base_url = self.base_url.rstrip('/')
                            img_url = img_url.lstrip('/')
                            full_url = f"{base_url}/{img_url}"
                        else:
                            full_url = img_url
                        
                        photos.append({'url': full_url})
                        self.logger.info(f"üîç Photo {i+1}: {full_url}")
            else:
                self.logger.warning("üîç Photo extraction: no images selector found")
            
            self.logger.info(f"üîç Photo extraction: total photos extracted = {len(photos)}")
            return photos
        except Exception as e:
            self.logger.error(f"Error extracting photos: {e}")
            return []

    def _extract_field_elements(self, element, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É"""
        try:
            if selector.startswith("xpath:"):
                xpath_sel = selector[len("xpath:"):]
                return element.xpath(xpath_sel).getall()
            elif selector.strip().startswith("//") or selector.strip().startswith(".//"):
                return element.xpath(selector).getall()
            else:
                return element.css(selector).getall()
        except Exception as e:
            self.logger.warning(f"Error extracting field elements with selector '{selector}': {e}")
            return []

    def _extract_field_value(self, element, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É"""
        try:
            if selector.startswith("xpath:"):
                xpath_sel = selector[len("xpath:"):]
                return element.xpath(xpath_sel).get(default="").strip()
            elif selector.strip().startswith("//") or selector.strip().startswith(".//"):
                return element.xpath(selector).get(default="").strip()
            else:
                return element.css(selector).get(default="").strip()
        except Exception as e:
            self.logger.warning(f"Error extracting field with selector '{selector}': {e}")
            self.has_parsing_errors = True
            return None

    def _update_progress(self):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            if self.max_items_limit > 0:
                progress = min(95, int((self.scraped_items_count / self.max_items_limit) * 100))
            else:
                progress = 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            self.logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress}%, —Å–ø–∞—Ä—Å–µ–Ω–æ: {self.scraped_items_count}")
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")

    def parse_details(self, response):
        """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        item_data = response.meta['item_data']
        details = self.selectors.get('details', {})
        
        self.logger.info(f"üîç Detail parsing: processing URL {response.url}")
        self.logger.info(f"üîç Detail parsing: original item_data location = '{item_data.get('location')}'")
        
        for field, selector in details.items():
            if field == 'images':
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ
                photos = self._extract_photos_from_details(response, selector)
                if photos:
                    item_data['photos'] = photos
                    # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ images –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø–∞–π–ø–ª–∞–π–Ω–æ–º
                    item_data['images'] = [photo['url'] for photo in photos]
                    self.logger.info(f"üîç Detail parsing: extracted {len(photos)} photos")
                else:
                    self.logger.warning("üîç Detail parsing: no photos extracted")
            elif field == 'phone':
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                self.logger.info(f"üîç Phone extraction: selector = '{selector}'")
                phones = self._extract_phones_from_details(response, selector)
                if phones:
                    item_data['phone_numbers'] = phones
                    self.logger.info(f"üîç Detail parsing: extracted {len(phones)} phones: {phones}")
                else:
                    self.logger.warning("üîç Detail parsing: no phones extracted")
                    # –ü–æ–ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è an.kg
                    if 'an.kg' in response.url:
                        alt_selector = ".info_item .phone::text"
                        self.logger.info(f"üîç Phone extraction: trying alternative selector = '{alt_selector}'")
                        alt_phones = self._extract_phones_from_details(response, alt_selector)
                        if alt_phones:
                            item_data['phone_numbers'] = alt_phones
                            self.logger.info(f"üîç Detail parsing: extracted {len(alt_phones)} phones with alt selector: {alt_phones}")
            else:
                value = self._extract_field_value(response, selector)
                item_data[field] = value
                if field in ['rooms', 'area', 'floor']:
                    self.logger.info(f"üîç Detail parsing: field '{field}' = '{value}'")
        
        self.logger.info(f"üîç Detail parsing: final item_data location = '{item_data.get('location')}'")
        self.logger.info(f"üîç Detail parsing: final item_data photos = {len(item_data.get('photos', []))} photos")
        yield item_data

    def _extract_phones_from_details(self, response, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            phones = []
            self.logger.info(f"üîç Phone extraction: selector = '{selector}'")
            
            phone_elements = self._extract_field_elements(response, selector)
            self.logger.info(f"üîç Phone extraction: found {len(phone_elements)} phone elements")
            
            for i, phone in enumerate(phone_elements):
                if phone:
                    self.logger.info(f"üîç Phone extraction: raw phone {i+1} = '{phone}'")
                    # –û—á–∏—â–∞–µ–º –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    cleaned_phone = self._clean_phone_number(phone)
                    if cleaned_phone:
                        phones.append(cleaned_phone)
                        self.logger.info(f"üîç Phone extraction: cleaned phone {i+1} = '{cleaned_phone}'")
                    else:
                        self.logger.warning(f"üîç Phone extraction: phone {i+1} was cleaned to empty")
                else:
                    self.logger.warning(f"üîç Phone extraction: phone {i+1} is empty")
            
            self.logger.info(f"üîç Phone extraction: total phones = {len(phones)}")
            return phones
        except Exception as e:
            self.logger.error(f"Error extracting phones from details: {e}")
            return []

    def _clean_phone_number(self, phone):
        """–û—á–∏—â–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        try:
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å tel: –µ—Å–ª–∏ –µ—Å—Ç—å
            if phone.startswith('tel:'):
                phone = phone[4:]
            
            # –£–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, + –∏ –ø—Ä–æ–±–µ–ª–æ–≤
            import re
            cleaned = re.sub(r'[^\d+\s\-\(\)]', '', phone)
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            cleaned = ' '.join(cleaned.split())
            
            return cleaned if cleaned else None
        except Exception as e:
            self.logger.warning(f"Error cleaning phone number '{phone}': {e}")
            return phone

    def _extract_photos_from_details(self, response, selector):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –∏–∑ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            photos = []
            self.logger.info(f"üîç Photo details extraction: selector = '{selector}'")
            self.logger.info(f"üîç Photo details extraction: response URL = '{response.url}'")
            
            image_elements = self._extract_field_elements(response, selector)
            self.logger.info(f"üîç Photo details extraction: found {len(image_elements)} image elements")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ - –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            for i, img_url in enumerate(image_elements[:5]):
                self.logger.info(f"üîç Photo details extraction: raw image {i+1} = '{img_url}'")
            
            for i, img_url in enumerate(image_elements):
                if img_url:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –ø–æ–ª–Ω—ã–π
                    if not img_url.startswith('http'):
                        base_url = self.base_url.rstrip('/')
                        img_url = img_url.lstrip('/')
                        full_url = f"{base_url}/{img_url}"
                    else:
                        full_url = img_url
                    
                    photos.append({'url': full_url})
                    self.logger.info(f"üîç Photo details extraction: photo {i+1} = {full_url}")
            
            self.logger.info(f"üîç Photo details extraction: total photos = {len(photos)}")
            return photos
        except Exception as e:
            self.logger.error(f"Error extracting photos from details: {e}")
            return []

    def handle_error(self, failure):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º"""
        try:
            request = failure.request
            retry_count = request.meta.get('retry_count', 0)
            max_retries = 3
            
            self.logger.error(f"Request failed: {request.url}")
            self.logger.error(f"Error: {failure.value}")
            
            # Retry –¥–ª—è —Ç–∞–π–º–∞—É—Ç–æ–≤ –∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫
            if retry_count < max_retries and (
                'Timeout' in str(failure.value) or 
                'Connection' in str(failure.value) or
                'Network' in str(failure.value)
            ):
                retry_count += 1
                self.logger.info(f"Retrying request {request.url} (attempt {retry_count}/{max_retries})")
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è retry
                new_timeout = 120000 + (retry_count * 30000)  # +30 —Å–µ–∫ –∑–∞ –∫–∞–∂–¥—É—é –ø–æ–ø—ã—Ç–∫—É
                
                yield scrapy.Request(
                    request.url,
                    callback=request.callback,
                    meta={
                        **request.meta,
                        'retry_count': retry_count,
                        'playwright': True,
                        'playwright_include_page': True,
                        'playwright_page_methods': [
                            PageMethod("wait_for_load_state", "networkidle"),
                        ]
                    },
                    errback=self.handle_error,
                    dont_filter=True
                )
                return
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö
            error_str = str(failure.value).lower()
            if any(network_error in error_str for network_error in [
                'dns lookup failed', 'connection refused', 'connection timeout',
                'network unreachable', 'host unreachable', 'request failed'
            ]):
                self.has_parsing_errors = True
                self.logger.error("Network error detected, setting parsing errors flag")
            
            if self.scraping_logger:
                self.scraping_logger.log_request_failure(request.url, str(failure.value))
            self.failed_items += 1
            
        except Exception as e:
            self.logger.error(f"Error in error handler: {e}")

    def closed(self, reason):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–ø–∞–π–¥–µ—Ä–∞"""
        try:
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._update_progress()
            
            total_processed = self.processed_items + self.failed_items
            success_rate = (self.processed_items / total_processed * 100) if total_processed > 0 else 0
            
            stats = {
                'scraped_items': self.scraped_items_count,
                'processed_items': self.processed_items,
                'failed_items': self.failed_items,
                'success_rate': f"{success_rate:.1f}%",
                'reason': reason
            }
            
            self.logger.info(f"Spider closed: {stats}")
            
            if self.scraping_logger:
                self.scraping_logger.log_spider_finished(stats)
            
        except Exception as e:
            self.logger.error(f"Error in spider close: {e}") 

    def closed(self, reason):
        if self.has_parsing_errors:
            self.logger.error("Spider finished with parsing errors. Signalling failure.")
            pass


