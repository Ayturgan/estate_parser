import redis
import subprocess
import json
import uuid
from datetime import datetime
import os
import select
import signal
import time
from logger import get_scraping_logger, remove_scraping_logger

REDIS_HOST = os.environ.get('REDIS_HOST', 'redis')
REDIS_PORT = int(os.environ.get('REDIS_PORT', 6379))

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

JOBS_KEY = "scrapy_jobs"
LOG_PREFIX = "scrapy_log:"


def update_status(job_id, **kwargs):
    job = r.hget(JOBS_KEY, job_id)
    job = json.loads(job) if job else {}
    job.update(kwargs)
    r.hset(JOBS_KEY, job_id, json.dumps(job))

def append_log(job_id, line):
    r.rpush(f"{LOG_PREFIX}{job_id}", line)

def check_job_status(job_id):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –≤ Redis"""
    try:
        job = r.hget(JOBS_KEY, job_id)
        if job:
            job_data = json.loads(job)
            return job_data.get('status')
    except Exception as e:
        print(f"[Worker] –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {job_id}: {e}")
    return None

def monitor_process_with_stop_check(proc, job_id):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏"""
    import fcntl
    
    # –î–µ–ª–∞–µ–º stdout –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–º
    fd = proc.stdout.fileno()
    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
    
    last_check_time = time.time()
    check_interval = 5  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
    
    while proc.poll() is None:  # –ü–æ–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç–∞–µ—Ç
        current_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫—É –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
        if current_time - last_check_time >= check_interval:
            status = check_job_status(job_id)
            if status == "–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ":
                print(f"[Worker] üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ {job_id}")
                append_log(job_id, "[Worker] üõë –ó–∞–¥–∞—á–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
                
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
                try:
                    proc.terminate()  # –ú—è–≥–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
                    time.sleep(3)
                    if proc.poll() is None:
                        proc.kill()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
                        print(f"[Worker] ‚ö° –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ {job_id}")
                except Exception as e:
                    print(f"[Worker] –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ {job_id}: {e}")
                
                return True  # –ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
            
            last_check_time = current_time
        
        # –ß–∏—Ç–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ stdout
        try:
            ready, _, _ = select.select([proc.stdout], [], [], 0.1)
            if ready:
                line = proc.stdout.readline()
                if line:
                    append_log(job_id, line.rstrip())
        except Exception as e:
            print(f"[Worker] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è stdout: {e}")
            time.sleep(0.1)
    
    # –ß–∏—Ç–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
    try:
        for line in proc.stdout:
            append_log(job_id, line.rstrip())
    except Exception:
        pass
    
    return False  # –ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ

print("[Scrapy Worker] –°—Ç–∞—Ä—Ç –≤–æ—Ä–∫–µ—Ä–∞. –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á...")

try:
    subprocess.run(["playwright", "install", "--with-deps"], check=True)
except Exception as e:
    print(f"[Scrapy Worker] –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä—ã Playwright: {e}")

while True:
    task = r.brpop("scrapy_tasks")[1]
    task = json.loads(task)
    job_id = task["job_id"]
    config = task["config"]
    spider = task["spider"]
    
    # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏
    scraping_logger = get_scraping_logger(job_id, config)
    scraping_logger.log_job_start()
    
    update_status(job_id, status="–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è", started_at=datetime.utcnow().isoformat())
    os.chdir("/app/estate_scraper/real_estate_scraper")
    
    # –ü–µ—Ä–µ–¥–∞–µ–º job_id –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—É–∫–æ–≤
    env = os.environ.copy()
    env['SCRAPY_JOB_ID'] = job_id
    env['SCRAPY_CONFIG_NAME'] = config
    
    cmd = ["scrapy", "crawl", spider, "-a", f"config={config}", "-a", f"job_id={job_id}"]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)
    
    print(f"[Worker] üöÄ –ó–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ {config} (job_id: {job_id}, pid: {proc.pid})")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    was_stopped = monitor_process_with_stop_check(proc, job_id)
    
    proc.wait()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    if was_stopped:
        status = "–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"
        scraping_logger.log_job_end("stopped", "–ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print(f"[Worker] üõë –ó–∞–¥–∞—á–∞ {job_id} –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    elif proc.returncode == 0:
        status = "–∑–∞–≤–µ—Ä—à–µ–Ω–æ"
        scraping_logger.log_job_end("completed")
        print(f"[Worker] ‚úÖ –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    else:
        status = "–æ—à–∏–±–∫–∞"
        scraping_logger.log_job_end("failed", f"Process returned code {proc.returncode}")
        print(f"[Worker] ‚ùå –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π")
    
    update_status(job_id, status=status, finished_at=datetime.utcnow().isoformat(), returncode=proc.returncode)
    
    # –û—á–∏—â–∞–µ–º –ª–æ–≥–≥–µ—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
    remove_scraping_logger(job_id, config) 