import redis
import subprocess
import json
import uuid
from datetime import datetime
import os
import select
import signal
import time
import re
from logger import get_scraping_logger, remove_scraping_logger

REDIS_HOST = os.environ.get('REDIS_HOST', 'redis')
REDIS_PORT = int(os.environ.get('REDIS_PORT', 6379))

r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)

JOBS_KEY = "scrapy_jobs"
LOG_PREFIX = "scrapy_log:"

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
PARSING_ERROR_PATTERNS = [
    r"Spider finished with parsing errors",
    r"has_parsing_errors.*True",
    r"Error extracting field",
    r"Error extracting item data",
    r"Error extracting photos",
    r"Error extracting phones",
    r"Invalid JSON in response",
    r"Required selectors.*not found",
    r"No ads container found",
    r"Error processing item",
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏
    r"DNS lookup failed",
    r"Connection refused",
    r"Network unreachable",
    r"Host unreachable",
    r"Request failed",
    r"HTTP –∑–∞–ø—Ä–æ—Å –Ω–µ—É—Å–ø–µ—à–µ–Ω",
    r"Gave up retrying",
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ - –æ–Ω–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã
    # r"Downloader/exception_count"  # –£–±–∏—Ä–∞–µ–º —ç—Ç–æ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
SUCCESS_PATTERNS = [
    r"Spider closed.*success",
    r"Items scraped.*\d+",
    r"‚úÖ.*successfully",
    r"Successfully extracted",
    r"item_scraped_count.*\d+",  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    r"finish_reason.*finished"    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø—Ä–∏—á–∏–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
]


def update_status(job_id, **kwargs):
    job = r.hget(JOBS_KEY, job_id)
    job = json.loads(job) if job else {}
    job.update(kwargs)
    r.hset(JOBS_KEY, job_id, json.dumps(job))

def append_log(job_id, line):
    r.rpush(f"{LOG_PREFIX}{job_id}", line)

def check_job_status(job_id):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –≤ Redis"""
    try:
        job = r.hget(JOBS_KEY, job_id)
        if job:
            job_data = json.loads(job)
            return job_data.get('status')
    except Exception as e:
        print(f"[Worker] –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {job_id}: {e}")
    return None

def detect_parsing_errors(log_line):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –ª–æ–≥–µ"""
    for pattern in PARSING_ERROR_PATTERNS:
        if re.search(pattern, log_line, re.IGNORECASE):
            return True
    return False

def detect_success_signals(log_line):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
    for pattern in SUCCESS_PATTERNS:
        if re.search(pattern, log_line, re.IGNORECASE):
            return True
    return False

def monitor_process_with_stop_check(proc, job_id):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    import fcntl
    
    # –î–µ–ª–∞–µ–º stdout –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–º
    fd = proc.stdout.fileno()
    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
    
    last_check_time = time.time()
    check_interval = 5  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
    
    # –§–ª–∞–≥–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
    parsing_errors_detected = False
    success_signals_detected = False
    
    while proc.poll() is None:  # –ü–æ–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–±–æ—Ç–∞–µ—Ç
        current_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞–Ω–æ–≤–∫—É –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
        if current_time - last_check_time >= check_interval:
            status = check_job_status(job_id)
            if status == "–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ":
                print(f"[Worker] üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –∑–∞–¥–∞—á–∏ {job_id}")
                append_log(job_id, "[Worker] üõë –ó–∞–¥–∞—á–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
                
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
                try:
                    proc.terminate()  # –ú—è–≥–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
                    time.sleep(3)
                    if proc.poll() is None:
                        proc.kill()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
                        print(f"[Worker] ‚ö° –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ {job_id}")
                except Exception as e:
                    print(f"[Worker] –û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ {job_id}: {e}")
                
                return (True, False, False)  # –ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
            
            last_check_time = current_time
        
        # –ß–∏—Ç–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ stdout
        try:
            ready, _, _ = select.select([proc.stdout], [], [], 0.1)
            if ready:
                line = proc.stdout.readline()
                if line:
                    line = line.rstrip()
                    append_log(job_id, line)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
                    if not parsing_errors_detected and detect_parsing_errors(line):
                        parsing_errors_detected = True
                        print(f"[Worker] ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–¥–∞—á–µ {job_id}")
                        append_log(job_id, "[Worker] ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ —É—Å–ø–µ—Ö–∞
                    if not success_signals_detected and detect_success_signals(line):
                        success_signals_detected = True
                        print(f"[Worker] ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–¥–∞—á–µ {job_id}")
                        append_log(job_id, "[Worker] ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
                        
        except Exception as e:
            print(f"[Worker] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è stdout: {e}")
            time.sleep(0.1)
    
    # –ß–∏—Ç–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
    try:
        for line in proc.stdout:
            line = line.rstrip()
            append_log(job_id, line)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ª–æ–≥–∏
            if not parsing_errors_detected and detect_parsing_errors(line):
                parsing_errors_detected = True
                print(f"[Worker] ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–¥–∞—á–µ {job_id}")
                append_log(job_id, "[Worker] ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            
            if not success_signals_detected and detect_success_signals(line):
                success_signals_detected = True
                print(f"[Worker] ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–¥–∞—á–µ {job_id}")
                append_log(job_id, "[Worker] ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏–≥–Ω–∞–ª—ã —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞")
                
    except Exception:
        pass
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    return False, parsing_errors_detected, success_signals_detected

print("[Scrapy Worker] –°—Ç–∞—Ä—Ç –≤–æ—Ä–∫–µ—Ä–∞. –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á...")

try:
    subprocess.run(["playwright", "install", "--with-deps"], check=True)
except Exception as e:
    print(f"[Scrapy Worker] –ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±—Ä–∞—É–∑–µ—Ä—ã Playwright: {e}")

while True:
    task = r.brpop("scrapy_tasks")[1]
    task = json.loads(task)
    job_id = task["job_id"]
    config = task["config"]
    spider = task["spider"]
    
    # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏
    scraping_logger = get_scraping_logger(job_id, config)
    scraping_logger.log_job_start()
    
    update_status(job_id, status="–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è", started_at=datetime.utcnow().isoformat())
    os.chdir("/app/estate_scraper/real_estate_scraper")
    
    # –ü–µ—Ä–µ–¥–∞–µ–º job_id –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—É–∫–æ–≤
    env = os.environ.copy()
    env['SCRAPY_JOB_ID'] = job_id
    env['SCRAPY_CONFIG_NAME'] = config
    
    cmd = ["scrapy", "crawl", spider, "-a", f"config={config}", "-a", f"job_id={job_id}"]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, env=env)
    
    print(f"[Worker] üöÄ –ó–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ {config} (job_id: {job_id}, pid: {proc.pid})")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
    was_stopped, parsing_errors_detected, success_signals_detected = monitor_process_with_stop_check(proc, job_id)
    
    proc.wait()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
    if was_stopped:
        status = "–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"
        scraping_logger.log_job_end("stopped", "–ó–∞–¥–∞—á–∞ –±—ã–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print(f"[Worker] üõë –ó–∞–¥–∞—á–∞ {job_id} –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    elif proc.returncode == 0:
        if parsing_errors_detected:
            status = "–∑–∞–≤–µ—Ä—à–µ–Ω–æ —Å –æ—à–∏–±–∫–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"
            scraping_logger.log_job_end("completed_with_parsing_errors", "–ó–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            print(f"[Worker] ‚ö†Ô∏è –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞")
        else:
            status = "–∑–∞–≤–µ—Ä—à–µ–Ω–æ"
            scraping_logger.log_job_end("completed")
            print(f"[Worker] ‚úÖ –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    else:
        if parsing_errors_detected:
            status = "–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"
            scraping_logger.log_job_end("failed_with_parsing_errors", f"Process returned code {proc.returncode} and parsing errors detected")
            print(f"[Worker] ‚ùå –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π –ø–∞—Ä—Å–∏–Ω–≥–∞")
        else:
            status = "–æ—à–∏–±–∫–∞"
            scraping_logger.log_job_end("failed", f"Process returned code {proc.returncode}")
            print(f"[Worker] ‚ùå –ó–∞–¥–∞—á–∞ {job_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–æ–π")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–∞—Ö –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —Å—Ç–∞—Ç—É—Å
    status_data = {
        'status': status, 
        'finished_at': datetime.utcnow().isoformat(), 
        'returncode': proc.returncode,
        'parsing_errors_detected': parsing_errors_detected,
        'success_signals_detected': success_signals_detected
    }
    
    update_status(job_id, **status_data)
    
    # –û—á–∏—â–∞–µ–º –ª–æ–≥–≥–µ—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
    remove_scraping_logger(job_id, config) 