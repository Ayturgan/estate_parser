import re
import os
from datetime import datetime
from itemadapter import ItemAdapter
import requests
import sys
import os
from .logger import get_scraping_logger

# –ò–º–ø–æ—Ä—Ç –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
try:
    from .services.photo_validator_service import PhotoValidatorService
    PHOTO_VALIDATION_ENABLED = True
    print("‚úÖ Photo validator service loaded successfully!")
except ImportError as e:
    print(f"‚ö†Ô∏è Photo validator service not available: {e}")
    PHOTO_VALIDATION_ENABLED = False

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ä–µ–¥—ã
if '/app/' in __file__:
    # Docker —Å—Ä–µ–¥–∞
    project_root = "/app"
else:
    # –õ–æ–∫–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞ - –∏—â–µ–º estate_parser –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    current_dir = os.path.dirname(__file__)
    while current_dir != '/' and os.path.basename(current_dir) != 'estate_parser':
        current_dir = os.path.dirname(current_dir)
    
    if os.path.basename(current_dir) == 'estate_parser':
        project_root = current_dir
    else:
        # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))

# –î–æ–±–∞–≤–ª—è–µ–º backend –≤ –ø—É—Ç—å –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
backend_path = os.path.join(project_root, 'backend')
if os.path.exists(backend_path):
    sys.path.insert(0, backend_path)

sys.path.append(project_root)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º AI —Å–µ—Ä–≤–∏—Å —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –º–æ–¥—É–ª—å–Ω—ã–π AI extractor
    from backend.app.services.ai_data_extractor import AIDataExtractor
    ai_extractor = AIDataExtractor()
    AI_ENABLED = True
    print(f"‚úÖ AI Data Extractor loaded successfully with modular architecture!")
except ImportError as e:
    print(f"‚ö†Ô∏è AI Data Extractor not available: {e}")
    AI_ENABLED = False
    ai_extractor = None
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞
    ai_module_path = os.path.join(project_root, 'backend', 'app', 'services', 'ai_data_extractor.py')
    print(f"üîç Expected AI module path: {ai_module_path}")
    print(f"üîç AI module exists: {os.path.exists(ai_module_path)}")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫
    print(f"üîç Contents of project_root ({project_root}): {os.listdir(project_root) if os.path.exists(project_root) else 'N/A'}")
    backend_path = os.path.join(project_root, 'backend')
    if os.path.exists(backend_path):
        print(f"üîç Contents of backend folder: {os.listdir(backend_path)}")
        app_path = os.path.join(backend_path, 'app')
        if os.path.exists(app_path):
            print(f"üîç Contents of app folder: {os.listdir(app_path)}")
            services_path = os.path.join(app_path, 'services')
            if os.path.exists(services_path):
                print(f"üîç Contents of services folder: {os.listdir(services_path)}")
    
    import traceback
    print(f"üîç Full traceback: {traceback.format_exc()}")


class ParserPipeline:
    def process_item(self, item, spider):
        return item


class DataCleaningPipeline:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π: —Ü–µ–Ω—ã, –ø–ª–æ—â–∞–¥–∏, —ç—Ç–∞–∂–µ–π –∏ —Ç.–¥.
    """
    
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        price_original = adapter.get("price")
        if price_original is not None:
            adapter["price_original"] = str(price_original)
            cleaned_price, currency = self.clean_price_and_currency(str(price_original))
            adapter["price"] = cleaned_price
            adapter["currency"] = currency
            spider.logger.debug(f"Price cleaned: '{price_original}' -> {cleaned_price} {currency}")
        
        area_original = adapter.get("area") or adapter.get("area_sqm")
        if area_original is not None:
            cleaned_area = self.extract_number(str(area_original))
            adapter["area_sqm"] = cleaned_area
            spider.logger.debug(f"Area cleaned: '{area_original}' -> {cleaned_area}")
        
        floor_original = adapter.get("floor")
        if floor_original is not None:
            cleaned_floor = self.extract_floor_number(str(floor_original))
            adapter["floor"] = cleaned_floor
            spider.logger.debug(f"Floor cleaned: '{floor_original}' -> {cleaned_floor}")
        
        total_floors_original = adapter.get("total_floors")
        if total_floors_original is not None:
            cleaned_total_floors = self.extract_total_floors(str(total_floors_original))
            adapter["total_floors"] = cleaned_total_floors
            spider.logger.debug(f"Total floors cleaned: '{total_floors_original}' -> {cleaned_total_floors}")
        elif floor_original and "–∏–∑" in str(floor_original):
            total_floors = self.extract_total_floors_from_floor_string(str(floor_original))
            if total_floors:
                adapter["total_floors"] = total_floors
        
        rooms_original = adapter.get("rooms")
        if rooms_original is not None:
            cleaned_rooms = self.extract_rooms_number(str(rooms_original))
            adapter["rooms"] = cleaned_rooms
            spider.logger.debug(f"Rooms cleaned: '{rooms_original}' -> {cleaned_rooms}")
        
        ceiling_height_original = adapter.get("ceiling_height")
        if ceiling_height_original is not None:
            cleaned_ceiling_height = self.extract_number(str(ceiling_height_original))
            adapter["ceiling_height"] = cleaned_ceiling_height
            spider.logger.debug(f"Ceiling height cleaned: '{ceiling_height_original}' -> {cleaned_ceiling_height}")

        images_original = adapter.get("images")
        if images_original is not None:
            cleaned_images = self.extract_images_universal(images_original)
            adapter["images"] = cleaned_images
            spider.logger.debug(f"Images cleaned: found {len(cleaned_images)} images")

        return item
    
    def clean_price_and_currency(self, price_str):
        """
        –û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Ü–µ–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–ª—é—Ç—É
        """
        if not price_str:
            return None, "SOM"
        
        price_str = str(price_str).strip()
        
        currency = "SOM"  
        
        if any(keyword in price_str.lower() for keyword in ['—Å–æ–º', 'som', 'kgs', '–∫–≥—Å']):
            currency = "SOM"
        elif any(keyword in price_str.lower() for keyword in ['usd', '$', '–¥–æ–ª–ª', 'dollar']):
            currency = "USD"
        elif any(keyword in price_str.lower() for keyword in ['eur', '‚Ç¨', '–µ–≤—Ä–æ', 'euro']):
            currency = "EUR"
        
        cleaned = re.sub(r'[^\d.,]', '', price_str)
        cleaned = re.sub(r'\s+', '', cleaned)
        
        if '.' in cleaned and ',' in cleaned:
            last_dot = cleaned.rfind('.')
            last_comma = cleaned.rfind(',')
            
            if last_dot > last_comma:
                cleaned = cleaned.replace(',', '')
            else:
                cleaned = cleaned.replace('.', '').replace(',', '.')
        elif ',' in cleaned:
            comma_parts = cleaned.split(',')
            if len(comma_parts) == 2 and len(comma_parts[1]) <= 2:
                cleaned = cleaned.replace(',', '.')
            else:
                cleaned = cleaned.replace(',', '')
        
        try:
            if '.' in cleaned:
                price_value = float(cleaned)
            else:
                price_value = int(cleaned)
            
            if price_value <= 0:
                return None, currency
                
            return price_value, currency
            
        except (ValueError, TypeError):
            return None, currency
        

    def extract_images_universal(self, images_data):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –∫–∞–∫ –ø—Ä—è–º—ã–µ URL, —Ç–∞–∫ –∏ URL –∏–∑ background-image —Å—Ç–∏–ª–µ–π,
        –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã API (–Ω–∞–ø—Ä–∏–º–µ—Ä, Lalafo).
        """
        if not images_data:
            return []
        
        image_urls = []
        
        # –ï—Å–ª–∏ images_data - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ —Å—Ç–∏–ª—è –∏–ª–∏ —Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ –ø—Ä—è–º–æ–π URL
        if isinstance(images_data, str):
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ —Å—Ç–∏–ª—è
            urls_from_style = re.findall(r"url\([\\'\"]?([^\\'\"]+)[\\'\"]?\)", images_data)
            if urls_from_style:
                image_urls.extend(urls_from_style)
            else:
                # –ï—Å–ª–∏ –Ω–µ —Å—Ç–∏–ª—å, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ø—Ä—è–º–æ–π URL
                image_urls.append(images_data)
        # –ï—Å–ª–∏ images_data - —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
        elif isinstance(images_data, list):
            for item in images_data:
                if item:
                    # –ï—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å (API —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä Lalafo)
                    if isinstance(item, dict):
                        # –ò—â–µ–º URL –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—è—Ö
                        url = (item.get('original_url') or 
                               item.get('url') or 
                               item.get('src') or 
                               item.get('image_url'))
                        if url:
                            image_urls.append(str(url))
                    else:
                        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ —Å—Ç–∏–ª—è
                        urls_from_style = re.findall(r"url\([\\'\"]?([^\\'\"]+)[\\'\"]?\)", str(item))
                        if urls_from_style:
                            image_urls.extend(urls_from_style)
                        else:
                            # –ï—Å–ª–∏ –Ω–µ —Å—Ç–∏–ª—å, —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ø—Ä—è–º–æ–π URL
                            image_urls.append(str(item))
        
        return list(filter(None, list(set(image_urls))))
    
    def extract_number(self, text):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ –∏–∑ —Å—Ç—Ä–æ–∫–∏
        """
        if not text:
            return None
        
        match = re.search(r'(\d+(?:[.,]\d+)?)', str(text))
        if match:
            number_str = match.group(1).replace(',', '.')
            try:
                if '.' in number_str:
                    return float(number_str)
                else:
                    return int(number_str)
            except (ValueError, TypeError):
                return None
        return None
    
    def extract_floor_number(self, floor_str):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä —ç—Ç–∞–∂–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "3 —ç—Ç–∞–∂ –∏–∑ 10"
        """
        if not floor_str:
            return None
        
        match = re.search(r'(\d+)\s*(?:—ç—Ç–∞–∂|—ç—Ç\.?)', str(floor_str), re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except (ValueError, TypeError):
                return None
        
        return self.extract_number(floor_str)
    
    def extract_total_floors(self, total_floors_str):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–∂–µ–π
        """
        if not total_floors_str:
            return None
        
        match = re.search(r'–∏–∑\s*(\d+)', str(total_floors_str), re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except (ValueError, TypeError):
                return None
        
        return self.extract_number(total_floors_str)
    
    def extract_total_floors_from_floor_string(self, floor_str):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–∂–µ–π –∏–∑ —Å—Ç—Ä–æ–∫–∏ —ç—Ç–∞–∂–∞ —Ç–∏–ø–∞ "3 —ç—Ç–∞–∂ –∏–∑ 10"
        """
        if not floor_str:
            return None
        
        match = re.search(r'–∏–∑\s*(\d+)', str(floor_str), re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except (ValueError, TypeError):
                return None
        return None
    
    def extract_rooms_number(self, rooms_str):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "1-–∫–æ–º–Ω. –∫–≤."
        """
        if not rooms_str:
            return None
        
        match = re.search(r'(\d+)[-\s]*–∫–æ–º–Ω', str(rooms_str), re.IGNORECASE)
        if match:
            try:
                return int(match.group(1))
            except (ValueError, TypeError):
                return None
        return self.extract_number(rooms_str)


class DatabasePipeline:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ API FastAPI (POST /ads)
    """
    def __init__(self):
        try:
            from backend.app.core.config import SCRAPY_API_URL
            self.API_URL = SCRAPY_API_URL
        except ImportError:
            self.API_URL = os.getenv("SCRAPY_API_URL", "http://api:8000/api/ads")
        
        # –£–±–∏—Ä–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ —Ñ–æ—Ç–æ
        self.photo_validator = None
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Ñ–æ—Ç–æ
        self.photo_filter_patterns = [
            '/banners/',
            # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä:
            # '/ads/', '/promo/', '/watermark/'
        ]

    def filter_photos(self, images):
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ñ–æ—Ç–æ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∏–∑ self.photo_filter_patterns
        –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ñ–æ—Ç–æ
        """
        if not images:
            return []
        filtered = []
        for url in images:
            if not any(pattern in url for pattern in self.photo_filter_patterns):
                filtered.append(url)
        return filtered

    def process_phone_numbers(self, phone_data):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Ö –≤ –Ω—É–∂–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã: +996700121212, 996700121212, 0700121212, 700121212
        """
        if not phone_data:
            return []
        
        processed_phones = []
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
        if isinstance(phone_data, list):
            for phone_item in phone_data:
                processed_phones.extend(self._split_and_normalize_phone(phone_item))
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ—ë
            processed_phones.extend(self._split_and_normalize_phone(phone_data))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        unique_phones = list(set(filter(None, processed_phones)))
        
        return unique_phones

    def _split_and_normalize_phone(self, phone_str):
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏—Ö
        """
        if not phone_str:
            return []
        
        phone_str = str(phone_str).strip()
        normalized_phones = []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –ø—Ä–æ–±–µ–ª, –∑–∞–ø—è—Ç–∞—è, —Ç–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π, –¥–µ—Ñ–∏—Å
        phone_parts = re.split(r'[\s,;\-]+', phone_str)
        
        for part in phone_parts:
            part = part.strip()
            if not part:
                continue
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã (—Å–∫–æ–±–∫–∏, –∫–∞–≤—ã—á–∫–∏ –∏ —Ç.–¥.)
            part = re.sub(r'[()"\']', '', part)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–æ–º–µ—Ä
            normalized = self._normalize_phone_number(part)
            if normalized:
                normalized_phones.append(normalized)
        
        return normalized_phones

    def _normalize_phone_number(self, phone):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ –≤ –æ–¥–∏–Ω –∏–∑ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: +996700121212, 996700121212, 0700121212, 700121212
        """
        if not phone:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ +
        clean_phone = re.sub(r'[^\d+]', '', phone)
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å +996
        if clean_phone.startswith('+996'):
            if len(clean_phone) == 13:  # +996700121212
                return clean_phone
            elif len(clean_phone) == 12:  # +99670121212 (–±–µ–∑ 0 –ø–æ—Å–ª–µ +996)
                return clean_phone
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 996 (–±–µ–∑ +)
        elif clean_phone.startswith('996'):
            if len(clean_phone) == 12:  # 996700121212
                return f"+{clean_phone}"
            elif len(clean_phone) == 11:  # 99670121212 (–±–µ–∑ 0 –ø–æ—Å–ª–µ 996)
                return f"+{clean_phone}"
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0 (–∫—ã—Ä–≥—ã–∑—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç)
        elif clean_phone.startswith('0'):
            if len(clean_phone) == 10:  # 0700121212
                return f"+996{clean_phone[1:]}"
            elif len(clean_phone) == 9:  # 070757554 (9 —Ü–∏—Ñ—Ä —Å 0)
                return f"+996{clean_phone[1:]}"
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 7 (–±–µ–∑ 0)
        elif clean_phone.startswith('7'):
            if len(clean_phone) == 9:  # 700121212
                return f"+996{clean_phone}"
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 5, 6, 7 (–º–æ–±–∏–ª—å–Ω—ã–µ –∫–æ–¥—ã)
        elif clean_phone.startswith(('5', '6', '7')):
            if len(clean_phone) == 9:  # 700121212
                return f"+996{clean_phone}"
        
        return None

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        
        # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏
        job_id = getattr(spider, 'job_id', 'unknown')
        config_name = getattr(spider, 'config_name', 'unknown')
        scraping_logger = get_scraping_logger(job_id, config_name)
        
        title = adapter.get("title", "Unknown")
        item_url = adapter.get("link") or adapter.get("url", "")
        
        scraping_logger.log_item_processing(title, item_url)
        
        source_name = None
        if hasattr(spider, "config"):
            source_name = spider.config.get("source_name", None)
        if not source_name:
            url = adapter.get('link') or adapter.get('url', '')
            if 'house.kg' in url:
                source_name = "house.kg"
            elif 'lalafo.kg' in url:
                source_name = "lalafo.kg"
            elif 'stroka.kg' in url:
                source_name = "stroka.kg"
            elif 'an.kg' in url:
                source_name = "an.kg"
            elif 'agency.kg' in url:
                source_name = "agency.kg"
            else:
                source_name = "unknown"

        city = adapter.get('city')
        district = adapter.get('district')
        address_line = adapter.get('address')
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–æ–∫–∞—Ü–∏—é –∏–∑ –ø–æ–ª—è location (–¥–ª—è agency.kg)
        location_str = adapter.get('location')
        spider.logger.info(f"üîç Pipeline location processing: location_str = '{location_str}'")
        if location_str and not city and not district:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä–æ–∫–∞ –ª–æ–∫–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –∫–∞–∫ district
            district = location_str
            address_line = location_str
            spider.logger.info(f"üîç Pipeline location processing: set district = '{district}'")
        
        if not city and not district and address_line:
            parts = [p.strip() for p in address_line.split(',')]
            if len(parts) >= 3:
                city = parts[0]
                district = parts[1]
                address_line = ', '.join(parts[2:])
            elif len(parts) == 2:
                city = parts[0]
                address_line = parts[1]
        location = None
        if city or district or address_line:
            location = {
                "city": city,
                "district": district,
                "address": address_line
            }
            spider.logger.info(f"üîç Pipeline location: created location object = {location}")
        else:
            spider.logger.warning("üîç Pipeline location: no location data found")

        images = adapter.get('images') or ([adapter.get('main_image_url')] if adapter.get('main_image_url') else [])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∫–ª–∞–º–Ω—ã—Ö —Ñ–æ—Ç–æ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        filtered_images = self.filter_photos(images)
        photos = [{"url": url} for url in filtered_images if url]

        # üîß –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–ú–ï–†–û–í –¢–ï–õ–ï–§–û–ù–û–í
        phone_numbers = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª–µ phone_numbers (–∏–∑ —Å–ø–∞–π–¥–µ—Ä–∞)
        phones_from_spider = adapter.get('phone_numbers')
        if phones_from_spider:
            processed_phones = self.process_phone_numbers(phones_from_spider)
            phone_numbers.extend(processed_phones)
            spider.logger.info(f"üîß Phone processing: processed {len(processed_phones)} phones from spider: {processed_phones}")
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ–ª—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        phone = adapter.get('phone')
        if phone:
            processed_phone = self.process_phone_numbers(phone)
            for p in processed_phone:
                if p not in phone_numbers:
                    phone_numbers.append(p)
        
        mobile = adapter.get('mobile')
        if mobile:
            processed_mobile = self.process_phone_numbers(mobile)
            for p in processed_mobile:
                if p not in phone_numbers:
                    phone_numbers.append(p)
        
        spider.logger.info(f"üîß Final phone processing: {len(phone_numbers)} unique phones: {phone_numbers}")

        published_at = None
        created_time = adapter.get('created_at')
        if created_time:
            if isinstance(created_time, (int, float)):
                try:
                    published_at = datetime.fromtimestamp(created_time).isoformat()
                except Exception:
                    published_at = None
            elif isinstance(created_time, str):
                date_match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', created_time)
                if date_match:
                    day, month, year = map(int, date_match.groups())
                    try:
                        published_at = datetime(year, month, day).isoformat()
                    except Exception:
                        published_at = None

        payload = {
            "source_id": adapter.get("source_id"),
            "source_url": adapter.get("link") or adapter.get("url"),
            "source_name": source_name,
            "title": adapter.get("title"),
            "description": adapter.get("description"),
            "price": adapter.get("price"), 
            "currency": adapter.get("currency") or "USD",
            "rooms": adapter.get("rooms"),  
            "area_sqm": adapter.get("area_sqm"), 
            "land_area_sotka": adapter.get("land_area_sotka"),
            "floor": adapter.get("floor"), 
            "total_floors": adapter.get("total_floors"),  
            "series": adapter.get("series"),
            "building_type": adapter.get("building_type") or adapter.get("building"),
                                "condition": adapter.get("condition") or adapter.get("repair"),
            "furniture": adapter.get("furniture"),
            "heating": adapter.get("heating"),
            "hot_water": adapter.get("hot_water"),
            "gas": adapter.get("gas"),
            "ceiling_height": adapter.get("ceiling_height"),
            "phone_numbers": phone_numbers,
            "location": location,
            "photos": photos,
            "attributes": adapter.get("attributes") or {},
            "published_at": published_at,
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
            "property_type": adapter.get("property_type"),
            "listing_type": adapter.get("listing_type"),

        }
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º property_type –∏ listing_type –∏–∑ adapter
        spider.logger.info(f"üîç DIAGNOSTIC: adapter.get('property_type') = {adapter.get('property_type')}")
        spider.logger.info(f"üîç DIAGNOSTIC: adapter.get('listing_type') = {adapter.get('listing_type')}")
        spider.logger.info(f"üîç DIAGNOSTIC: payload property_type = {payload.get('property_type')}")
        spider.logger.info(f"üîç DIAGNOSTIC: payload listing_type = {payload.get('listing_type')}")
        
        payload = {k: v for k, v in payload.items() if v not in [None, ""]}
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        spider.logger.info(f"üîç DIAGNOSTIC: –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - payload property_type = {payload.get('property_type')}")
        spider.logger.info(f"üîç DIAGNOSTIC: –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - payload listing_type = {payload.get('listing_type')}")

        # ü§ñ AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        spider.logger.info(f"üîç AI Debug: AI_ENABLED={AI_ENABLED}, ai_extractor={ai_extractor is not None if ai_extractor else 'None'}")
        
        if AI_ENABLED and ai_extractor:
            try:
                ai_title = adapter.get("title") or ""
                description = adapter.get("description") or ""
                
                spider.logger.info(f"ü§ñ Starting AI processing for title: {ai_title[:50]}...")
                spider.logger.debug(f"ü§ñ Description: {description[:100]}...")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º item_data —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                item_data = payload.copy()  # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤ —Å–ø–∞–π–¥–µ—Ä–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
                if hasattr(spider, 'config') and spider.config:
                    # –î–ª—è –º—É–ª—å—Ç–∏–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã—Ö —Å–ø–∞–π–¥–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤ item
                    config_data = {}
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Å–∞–º–æ–º item
                    if adapter.get('property_type'):
                        config_data['property_type'] = adapter.get('property_type')
                    if adapter.get('listing_type'):
                        config_data['listing_type'] = adapter.get('listing_type')
                    if adapter.get('source'):
                        config_data['source'] = adapter.get('source')
                    if adapter.get('category_name'):
                        config_data['category_name'] = adapter.get('category_name')
                    if adapter.get('category_id'):
                        config_data['category_id'] = adapter.get('category_id')
                    
                    if config_data:
                        item_data.update(config_data)
                        spider.logger.info(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {config_data}")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º AI –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                enhanced_data = ai_extractor.extract_and_classify(
                    title=ai_title,
                    description=description, 
                    existing_data=item_data  # –ü–µ—Ä–µ–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º AI –æ–±—Ä–∞–±–æ—Ç–∫—É
                scraping_logger.log_ai_processing(ai_title, description, enhanced_data)
                
                spider.logger.debug(f"ü§ñ AI extracted data: {enhanced_data}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º payload —Å –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç AI
                payload.update(enhanced_data)
                spider.logger.info(f"‚úÖ AI enhancement completed for: {ai_title[:50]}... | Updated payload keys: {list(enhanced_data.keys())}")
                
                # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –ø–æ—Å–ª–µ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
                spider.logger.info(f"üîç DIAGNOSTIC: –ü–æ—Å–ª–µ AI - payload property_type = {payload.get('property_type')}")
                spider.logger.info(f"üîç DIAGNOSTIC: –ü–æ—Å–ª–µ AI - payload listing_type = {payload.get('listing_type')}")
                
            except Exception as e:
                spider.logger.error(f"‚ùå AI enhancement failed: {e}")
                scraping_logger.log_error(f"AI enhancement failed", f"Title: {title}", e)
                import traceback
                spider.logger.error(f"‚ùå AI traceback: {traceback.format_exc()}")
        else:
            spider.logger.warning(f"‚ö†Ô∏è AI enhancement skipped - AI_ENABLED={AI_ENABLED}, ai_extractor available={ai_extractor is not None if ai_extractor else 'None'}")
            scraping_logger.log_warning("AI enhancement skipped", f"AI_ENABLED={AI_ENABLED}, ai_extractor available={ai_extractor is not None}")

        # üîç –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ payload –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
        spider.logger.info(f"üîç Final payload being sent to API:")
        spider.logger.info(f"  üìù Title: {payload.get('title', 'N/A')}")
        spider.logger.info(f"üîç DIAGNOSTIC: –§–ò–ù–ê–õ–¨–ù–´–ô - payload property_type = {payload.get('property_type')}")
        spider.logger.info(f"üîç DIAGNOSTIC: –§–ò–ù–ê–õ–¨–ù–´–ô - payload listing_type = {payload.get('listing_type')}")
        spider.logger.info(f"  ü§ñ AI Classification: property_type={payload.get('property_type')}, property_origin={payload.get('property_origin')}, listing_type={payload.get('listing_type')}")
        spider.logger.info(f"  üè† Property Data: rooms={payload.get('rooms')}, area_sqm={payload.get('area_sqm')}, floor={payload.get('floor')}, total_floors={payload.get('total_floors')}")
        spider.logger.info(f"  üè° Characteristics: heating={payload.get('heating')}, furniture={payload.get('furniture')}, condition={payload.get('condition')}")
        spider.logger.info(f"  üìç Location: {payload.get('location', 'N/A')}")
        spider.logger.info(f"  üë§ Realtor: {payload.get('realtor_id')}")
        spider.logger.info(f"  üìû Phones: {payload.get('phone_numbers')}")
        
        try:
            response = requests.post(self.API_URL, json=payload, timeout=30)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ç–∞—Ç—É—Å –∫–æ–¥—É
            if response.status_code == 200:
                spider.logger.info(f"‚úÖ Ad processed successfully (created or updated): {payload.get('title')}")
                scraping_logger.log_api_call_success(title, self.API_URL)
                scraping_logger.log_item_success(title, payload)
            else:
                spider.logger.info(f"‚úÖ Ad sent to API with status {response.status_code}: {payload.get('title')}")
                scraping_logger.log_api_call_success(title, self.API_URL)
                scraping_logger.log_item_success(title, payload)
                
        except requests.exceptions.HTTPError as e:
            error_text = ""
            try:
                error_text = response.text
            except Exception:
                pass
            error_msg = f"HTTP Error {e}: {error_text}"
            spider.logger.error(f"Error sending ad to API: {e} | Data: {payload} | Response: {error_text}")
            scraping_logger.log_api_call_failure(title, error_msg, self.API_URL)
        except Exception as e:
            spider.logger.error(f"Error sending ad to API: {e} | Data: {payload}")
            scraping_logger.log_api_call_failure(title, str(e), self.API_URL)
        
        return item

