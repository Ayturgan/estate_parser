#!/bin/bash

# ðŸš€ Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸ Ð½Ð° Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
# ÐÐ²Ñ‚Ð¾Ñ€: AI Assistant
# Ð”Ð°Ñ‚Ð°: $(date)

set -e

echo "ðŸ  ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸ÑŽ Ð½Ð° Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°..."

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð² ÐºÐ¾Ñ€Ð½Ðµ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
if [ ! -f "docker-compose.yml" ]; then
    echo "âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð¸Ð· ÐºÐ¾Ñ€Ð½Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"
    exit 1
fi

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½ÑƒÑŽ ÐºÐ¾Ð¿Ð¸ÑŽ
echo "ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ñ€ÐµÐ·ÐµÑ€Ð²Ð½ÑƒÑŽ ÐºÐ¾Ð¿Ð¸ÑŽ..."
backup_dir="backup_$(date +%Y%m%d_%H%M%S)"
mkdir -p "$backup_dir"
cp -r app "$backup_dir/"
cp -r real_estate_scraper "$backup_dir/"
cp docker-compose.yml "$backup_dir/"
cp requirements.txt "$backup_dir/"
echo "âœ… Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ ÐºÐ¾Ð¿Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°: $backup_dir"

# ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ‹ ÐµÑÐ»Ð¸ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ñ‹
echo "ðŸ›‘ ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ñ‹..."
docker-compose down 2>/dev/null || true

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ
echo "ðŸ“ Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ..."

# Backend
mkdir -p backend/app/{api/v1,core,models,services,websocket} backend/tests
cp -r app/* backend/app/
cp requirements.txt backend/
cp pyproject.toml backend/

# Scraper
mkdir -p scraper/estate_scraper/{spiders,configs,pipelines} scraper/tests
cp -r real_estate_scraper/* scraper/estate_scraper/

# Frontend
mkdir -p frontend/{static,templates}
cp -r app/static/* frontend/static/
cp -r app/templates/* frontend/templates/

# Infrastructure
mkdir -p infrastructure/{docker,k8s}
cp Dockerfile infrastructure/docker/backend.Dockerfile
cp docker-compose.yml infrastructure/

# Configs
mkdir -p configs/{environments,logging,monitoring}
cp .env configs/environments/development.env

# Tools
mkdir -p tools
cp create_admin.py tools/
cp monitor_logs.py tools/
cp view_scraping_logs.py tools/
cp start_test_scraping.py tools/

# Scripts
mkdir -p scripts
mkdir -p docs/{api,deployment,development}

echo "âœ… ÐÐ¾Ð²Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð°"

# ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹
echo "ðŸ”§ ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹..."

# Backend Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹
sed -i 's/from config import/from app.core.config import/g' backend/app/main.py
sed -i 's/from config import/from app.core.config import/g' backend/app/database.py

# Scraper Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹
sed -i 's/from app.services.ai_data_extractor/from backend.app.services.ai_data_extractor/g' scraper/estate_scraper/real_estate_scraper/pipelines.py

echo "âœ… Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹"

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Dockerfile'Ñ‹
echo "ðŸ³ Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Dockerfile'Ñ‹..."

cat > infrastructure/docker/scraper.Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ñ… Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ requirements
COPY scraper/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð´Ð° scraper
COPY scraper/ ./scraper/

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ AI Ð¼Ð¾Ð´ÑƒÐ»Ñ
COPY backend/app/services/ai_data_extractor.py ./backend/app/services/

# Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
WORKDIR /app/scraper

# Ð—Ð°Ð¿ÑƒÑÐº
CMD ["scrapy", "crawl", "generic_api", "-a", "config=lalafo"]
EOF

cat > infrastructure/docker/frontend.Dockerfile << 'EOF'
FROM nginx:alpine

# ÐšÐ¾Ð¿Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
COPY frontend/static/ /usr/share/nginx/html/static/
COPY frontend/templates/ /usr/share/nginx/html/templates/

# ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ nginx
COPY infrastructure/nginx.conf /etc/nginx/nginx.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
EOF

echo "âœ… Dockerfile'Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹"

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ requirements Ð´Ð»Ñ scraper
cat > scraper/requirements.txt << 'EOF'
scrapy==2.11.0
requests==2.31.0
itemadapter==0.8.0
python-dotenv==1.0.0
EOF

echo "âœ… Requirements Ð´Ð»Ñ scraper ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹"

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ nginx ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
cat > infrastructure/nginx.conf << 'EOF'
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  65;

    server {
        listen 80;
        server_name localhost;

        root /usr/share/nginx/html;
        index index.html;

        # Ð¡Ñ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ„Ð°Ð¹Ð»Ñ‹
        location /static/ {
            alias /usr/share/nginx/html/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # API Ð¿Ñ€Ð¾ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        location /api/ {
            proxy_pass http://api:8000;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # WebSocket Ð¿Ñ€Ð¾ÐºÑÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ
        location /ws/ {
            proxy_pass http://api:8000;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Ð’ÑÐµ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº frontend
        location / {
            try_files $uri $uri/ /index.html;
        }
    }
}
EOF

echo "âœ… Nginx ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°"

# ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ docker-compose
echo "ðŸ³ ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ docker-compose.yml..."

cat > infrastructure/docker-compose.yml << 'EOF'
version: '3.8'

services:
  # Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… PostgreSQL
  db:
    image: postgres:15
    container_name: estate_db
    environment:
      POSTGRES_DB: estate_db
      POSTGRES_USER: estate_user
      POSTGRES_PASSWORD: admin123
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - estate_network

  # Redis Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ Ð¾Ñ‡ÐµÑ€ÐµÐ´ÐµÐ¹
  redis:
    image: redis:7-alpine
    container_name: estate_redis
    ports:
      - "6379:6379"
    networks:
      - estate_network

  # Elasticsearch Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: estate_elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - estate_network

  # PgAdmin Ð´Ð»Ñ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð‘Ð”
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: estate_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin123
    ports:
      - "5050:80"
    depends_on:
      - db
    networks:
      - estate_network

  # Backend API
  api:
    build:
      context: .
      dockerfile: infrastructure/docker/backend.Dockerfile
    container_name: estate_api
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=estate_db
      - DB_USER=estate_user
      - DB_PASSWORD=admin123
      - REDIS_URL=redis://redis:6379
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_INDEX=real_estate_ads
      - SCRAPY_API_URL=http://api:8000/api/ads
      - PIPELINE_INTERVAL_HOURS=3
      - RUN_IMMEDIATELY_ON_START=false
      - SCRAPING_SOURCES=house,lalafo,stroka
      - ENABLE_SCRAPING=true
      - ENABLE_PHOTO_PROCESSING=true
      - ENABLE_DUPLICATE_PROCESSING=true
      - ENABLE_REALTOR_DETECTION=true
      - ENABLE_ELASTICSEARCH_REINDEX=true
    ports:
      - "8000:8000"
    depends_on:
      - db
      - redis
      - elasticsearch
    volumes:
      - ./logs:/app/logs
    networks:
      - estate_network

  # Scraper ÑÐµÑ€Ð²Ð¸Ñ
  scraper:
    build:
      context: .
      dockerfile: infrastructure/docker/scraper.Dockerfile
    container_name: estate_scraper
    environment:
      - SCRAPY_API_URL=http://api:8000/api/ads
    depends_on:
      - api
    volumes:
      - ./logs:/app/logs
    networks:
      - estate_network

  # Frontend (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
  frontend:
    build:
      context: .
      dockerfile: infrastructure/docker/frontend.Dockerfile
    container_name: estate_frontend
    ports:
      - "80:80"
    depends_on:
      - api
    networks:
      - estate_network

volumes:
  postgres_data:
  elasticsearch_data:

networks:
  estate_network:
    driver: bridge
EOF

echo "âœ… Docker-compose Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½"

# Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹
echo "ðŸ§ª Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹..."

cd backend
python -c "from app.core.config import *; print('âœ… Backend config imports work')" || echo "âŒ Backend config imports failed"
python -c "from app.services.automation_service import automation_service; print('âœ… Automation service imports work')" || echo "âŒ Automation service imports failed"
cd ..

cd scraper
python -c "import sys; sys.path.append('..'); from estate_scraper.real_estate_scraper.pipelines import DatabasePipeline; print('âœ… Scraper pipelines imports work')" || echo "âŒ Scraper pipelines imports failed"
cd ..

echo "âœ… ÐœÐ¸Ð³Ñ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!"

echo ""
echo "ðŸŽ‰ ÐœÐ¸Ð³Ñ€Ð°Ñ†Ð¸Ñ Ð½Ð° Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°!"
echo ""
echo "ðŸ“ ÐÐ¾Ð²Ð°Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°:"
echo "â”œâ”€â”€ backend/          # Backend API"
echo "â”œâ”€â”€ scraper/          # Scraper ÑÐµÑ€Ð²Ð¸Ñ"
echo "â”œâ”€â”€ frontend/         # Frontend (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)"
echo "â”œâ”€â”€ infrastructure/   # Docker Ð¸ K8s"
echo "â”œâ”€â”€ configs/          # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸"
echo "â”œâ”€â”€ scripts/          # Ð¡ÐºÑ€Ð¸Ð¿Ñ‚Ñ‹"
echo "â”œâ”€â”€ docs/             # Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ"
echo "â””â”€â”€ tools/            # Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹"
echo ""
echo "ðŸš€ Ð”Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ:"
echo "docker-compose -f infrastructure/docker-compose.yml up -d"
echo ""
echo "ðŸ“‹ Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÑÐµÑ€Ð²Ð¸ÑÑ‹:"
echo "- API: http://localhost:8000"
echo "- Frontend: http://localhost:80"
echo "- PgAdmin: http://localhost:5050"
echo "- Elasticsearch: http://localhost:9200"
echo ""
echo "ðŸ’¾ Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ ÐºÐ¾Ð¿Ð¸Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð° Ð²: $backup_dir" 