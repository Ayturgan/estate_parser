#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
1. –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ (pHash, dHash, aHash)
2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
python image_deduplication_test.py
"""

import os
import sys
import logging
import numpy as np
from typing import List, Tuple, Dict, Optional
from pathlib import Path
import cv2
from PIL import Image
import imagehash
from sentence_transformers import SentenceTransformer
import torch
from transformers import CLIPProcessor, CLIPModel
import requests
from io import BytesIO

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ImageDeduplicationTester:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ 2025 –≥–æ–¥–∞"""
        self.clip_model = None
        self.clip_processor = None
        self.text_model = None
        self._load_models()
        
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º CLIP –º–æ–¥–µ–ª—å...")
            self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            logger.info("CLIP –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CLIP –º–æ–¥–µ–ª–∏: {e}")
            self.clip_model = None
            self.clip_processor = None
            
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            self.text_model = SentenceTransformer("BAAI/bge-m3")
            logger.info("–¢–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            self.text_model = None
    
    def calculate_perceptual_hashes(self, image_path: str) -> Dict[str, str]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ö–µ—à–µ–π –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Ö–µ—à–µ–π
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with Image.open(image_path) as img:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ö–µ—à–µ–π
                hashes = {
                    'pHash': str(imagehash.phash(img)),      # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–π —Ö–µ—à
                    'dHash': str(imagehash.dhash(img)),      # –†–∞–∑–Ω–æ—Å—Ç–Ω—ã–π —Ö–µ—à
                    'aHash': str(imagehash.average_hash(img)), # –°—Ä–µ–¥–Ω–∏–π —Ö–µ—à
                    'wHash': str(imagehash.whash(img)),      # –í–µ–π–≤–ª–µ—Ç —Ö–µ—à
                    'cHash': str(imagehash.colorhash(img)),  # –¶–≤–µ—Ç–æ–≤–æ–π —Ö–µ—à
                }
                
                logger.info(f"–í—ã—á–∏—Å–ª–µ–Ω—ã —Ö–µ—à–∏ –¥–ª—è {image_path}: {list(hashes.keys())}")
                return hashes
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ö–µ—à–µ–π –¥–ª—è {image_path}: {e}")
            return {}
    
    def calculate_hamming_distance(self, hash1: str, hash2: str) -> int:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –•—ç–º–º–∏–Ω–≥–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Ö–µ—à–∞–º–∏
        
        Args:
            hash1: –ü–µ—Ä–≤—ã–π —Ö–µ—à
            hash2: –í—Ç–æ—Ä–æ–π —Ö–µ—à
            
        Returns:
            –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –•—ç–º–º–∏–Ω–≥–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–ª–∏—á–∞—é—â–∏—Ö—Å—è –±–∏—Ç–æ–≤)
        """
        if len(hash1) != len(hash2):
            return max(len(hash1), len(hash2))
        
        distance = 0
        for i in range(len(hash1)):
            if hash1[i] != hash2[i]:
                distance += 1
        return distance
    
    def calculate_perceptual_similarity(self, hashes1: Dict[str, str], hashes2: Dict[str, str]) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ö–µ—à–µ–π
        
        Args:
            hashes1: –•–µ—à–∏ –ø–µ—Ä–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            hashes2: –•–µ—à–∏ –≤—Ç–æ—Ä–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Ö–µ—à–∞
        """
        similarities = {}
        
        for hash_type in ['pHash', 'dHash', 'aHash', 'wHash', 'cHash']:
            if hash_type in hashes1 and hash_type in hashes2:
                distance = self.calculate_hamming_distance(hashes1[hash_type], hashes2[hash_type])
                
                # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Ö–µ—à–∞
                max_distances = {
                    'pHash': 64,   # 64 –±–∏—Ç–∞
                    'dHash': 64,   # 64 –±–∏—Ç–∞
                    'aHash': 64,   # 64 –±–∏—Ç–∞
                    'wHash': 64,   # 64 –±–∏—Ç–∞
                    'cHash': 42,   # 42 –±–∏—Ç–∞ –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–≥–æ —Ö–µ—à–∞
                }
                
                max_distance = max_distances.get(hash_type, 64)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ö–æ–∂–µ—Å—Ç—å
                similarity = 1.0 - (distance / max_distance)
                similarities[hash_type] = max(0.0, similarity)
            else:
                similarities[hash_type] = 0.0
        
        return similarities
    
    def get_image_embeddings(self, image_path: str) -> Optional[np.ndarray]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é CLIP –º–æ–¥–µ–ª–∏
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            –≠–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        if self.clip_model is None or self.clip_processor is None:
            logger.warning("CLIP –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
            
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ CLIP
            inputs = self.clip_processor(images=image, return_tensors="pt")
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            with torch.no_grad():
                image_features = self.clip_model.get_image_features(**inputs)
                
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            embeddings = image_features.cpu().numpy()
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
            
            logger.info(f"–ü–æ–ª—É—á–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {image_path}: {embeddings.shape}")
            return embeddings[0]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π (–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π) —ç–º–±–µ–¥–¥–∏–Ω–≥
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {image_path}: {e}")
            return None
    
    def calculate_embedding_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        
        Args:
            embedding1: –≠–º–±–µ–¥–¥–∏–Ω–≥ –ø–µ—Ä–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            embedding2: –≠–º–±–µ–¥–¥–∏–Ω–≥ –≤—Ç–æ—Ä–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–æ—Ç 0 –¥–æ 1)
        """
        if embedding1 is None or embedding2 is None:
            return 0.0
            
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embedding1_norm = embedding1 / np.linalg.norm(embedding1)
            embedding2_norm = embedding2 / np.linalg.norm(embedding2)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = np.dot(embedding1_norm, embedding2_norm)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return 0.0
    
    def analyze_image_pair(self, image1_path: str, image2_path: str, description1: str = "", description2: str = "") -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –æ–±–æ–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤
        
        Args:
            image1_path: –ü—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            image2_path: –ü—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            description1: –û–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            description2: –û–ø–∏—Å–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {image1_path} vs {image2_path}")
        
        results = {
            'image1_path': image1_path,
            'image2_path': image2_path,
            'perceptual_hashes': {},
            'embedding_similarity': 0.0,
            'text_similarity': 0.0,
            'overall_similarity': 0.0,
            'is_duplicate': False
        }
        
        # 1. –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info("–í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏...")
        hashes1 = self.calculate_perceptual_hashes(image1_path)
        hashes2 = self.calculate_perceptual_hashes(image2_path)
        
        if hashes1 and hashes2:
            perceptual_similarities = self.calculate_perceptual_similarity(hashes1, hashes2)
            results['perceptual_hashes'] = {
                'hashes1': hashes1,
                'hashes2': hashes2,
                'similarities': perceptual_similarities
            }
            
            # –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º —Ö–µ—à–µ–π
            avg_perceptual_sim = np.mean(list(perceptual_similarities.values()))
            logger.info(f"–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–º —Ö–µ—à–∞–º: {avg_perceptual_sim:.3f}")
        
        # 2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        logger.info("–í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        embedding1 = self.get_image_embeddings(image1_path)
        embedding2 = self.get_image_embeddings(image2_path)
        
        if embedding1 is not None and embedding2 is not None:
            embedding_sim = self.calculate_embedding_similarity(embedding1, embedding2)
            results['embedding_similarity'] = embedding_sim
            logger.info(f"–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {embedding_sim:.3f}")
        
        # 3. –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è)
        if description1 and description2 and self.text_model:
            logger.info("–í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ...")
            try:
                embeddings1 = self.text_model.encode(description1)
                embeddings2 = self.text_model.encode(description2)
                text_sim = self.calculate_embedding_similarity(embeddings1, embeddings2)
                results['text_similarity'] = text_sim
                logger.info(f"–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: {text_sim:.3f}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
        
        # 4. –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        weights = {
            'perceptual': 0.4,
            'embedding': 0.5,
            'text': 0.1
        }
        
        overall_sim = 0.0
        total_weight = 0.0
        
        # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏
        if results['perceptual_hashes']:
            avg_perceptual = np.mean(list(results['perceptual_hashes']['similarities'].values()))
            overall_sim += avg_perceptual * weights['perceptual']
            total_weight += weights['perceptual']
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        if results['embedding_similarity'] > 0:
            overall_sim += results['embedding_similarity'] * weights['embedding']
            total_weight += weights['embedding']
        
        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        if results['text_similarity'] > 0:
            overall_sim += results['text_similarity'] * weights['text']
            total_weight += weights['text']
        
        if total_weight > 0:
            results['overall_similarity'] = overall_sim / total_weight
            results['is_duplicate'] = results['overall_similarity'] > 0.8  # –ü–æ—Ä–æ–≥ –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–∞
        
        logger.info(f"–û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {results['overall_similarity']:.3f}")
        logger.info(f"–î—É–±–ª–∏–∫–∞—Ç: {results['is_duplicate']}")
        
        return results
    
    def test_with_sample_images(self, test_dir: str = "test_images"):
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        
        Args:
            test_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        """
        test_path = Path(test_dir)
        if not test_path.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {test_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        image_files = [f for f in test_path.iterdir() if f.suffix.lower() in image_extensions]
        
        if len(image_files) < 2:
            logger.error(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {test_dir}. –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2.")
            return
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–∞—Ä—ã
        results = []
        for i in range(len(image_files)):
            for j in range(i + 1, len(image_files)):
                image1 = image_files[i]
                image2 = image_files[j]
                
                logger.info(f"\n{'='*50}")
                logger.info(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—É: {image1.name} vs {image2.name}")
                
                result = self.analyze_image_pair(str(image1), str(image2))
                results.append(result)
                
                # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
                self._print_detailed_report(result)
        
        # –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        self._print_summary_report(results)
    
    def _print_detailed_report(self, result: Dict):
        """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞"""
        print(f"\n{'='*60}")
        print(f"–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        print(f"{'='*60}")
        print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1: {Path(result['image1_path']).name}")
        print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2: {Path(result['image2_path']).name}")
        print(f"{'='*60}")
        
        # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏
        if result['perceptual_hashes']:
            print(f"\n–ü–ï–†–¶–ï–ü–¢–ò–í–ù–´–ï –•–ï–®–ò:")
            print(f"{'-'*30}")
            similarities = result['perceptual_hashes']['similarities']
            for hash_type, similarity in similarities.items():
                print(f"{hash_type:8}: {similarity:.3f}")
            
            avg_perceptual = np.mean(list(similarities.values()))
            print(f"{'–°—Ä–µ–¥–Ω–µ–µ':8}: {avg_perceptual:.3f}")
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        print(f"\n–≠–ú–ë–ï–î–î–ò–ù–ì–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
        print(f"{'-'*30}")
        print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {result['embedding_similarity']:.3f}")
        
        # –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        if result['text_similarity'] > 0:
            print(f"\n–¢–ï–ö–°–¢–û–í–û–ï –°–•–û–î–°–¢–í–û:")
            print(f"{'-'*30}")
            print(f"–°—Ö–æ–∂–µ—Å—Ç—å: {result['text_similarity']:.3f}")
        
        # –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"\n–û–ë–©–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
        print(f"{'-'*30}")
        print(f"–û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {result['overall_similarity']:.3f}")
        print(f"–î—É–±–ª–∏–∫–∞—Ç: {'–î–ê' if result['is_duplicate'] else '–ù–ï–¢'}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        if result['is_duplicate']:
            print(f"‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏")
        else:
            print(f"‚ùå –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ù–ï —è–≤–ª—è—é—Ç—Å—è –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏")
    
    def _print_summary_report(self, results: List[Dict]):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º"""
        print(f"\n{'='*80}")
        print(f"–°–í–û–î–ù–´–ô –û–¢–ß–ï–¢")
        print(f"{'='*80}")
        
        total_pairs = len(results)
        duplicates_found = sum(1 for r in results if r['is_duplicate'])
        
        print(f"–í—Å–µ–≥–æ –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_pairs}")
        print(f"–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_found}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {(duplicates_found/total_pairs)*100:.1f}%")
        
        if results:
            avg_overall_sim = np.mean([r['overall_similarity'] for r in results])
            avg_embedding_sim = np.mean([r['embedding_similarity'] for r in results])
            
            print(f"\n–°—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:")
            print(f"–û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {avg_overall_sim:.3f}")
            print(f"–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {avg_embedding_sim:.3f}")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ö–µ—à–µ–π
            perceptual_sims = []
            for r in results:
                if r['perceptual_hashes']:
                    avg_perceptual = np.mean(list(r['perceptual_hashes']['similarities'].values()))
                    perceptual_sims.append(avg_perceptual)
            
            if perceptual_sims:
                avg_perceptual_sim = np.mean(perceptual_sims)
                print(f"–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–º —Ö–µ—à–∞–º: {avg_perceptual_sim:.3f}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
    print("üß™ –¢–ï–°–¢ –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
    print("="*50)
    print("–≠—Ç–æ—Ç —Ç–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
    print("1. –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ (pHash, dHash, aHash, wHash, cHash)")
    print("2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é CLIP –º–æ–¥–µ–ª–∏")
    print("="*50)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–µ—Ä
    tester = ImageDeduplicationTester()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    test_dir = "test_images"
    if not os.path.exists(test_dir):
        print(f"\n‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {test_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print(f"–°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {test_dir} –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
        print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 2 –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + 1 —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –¥—Ä—É–≥–æ–µ")
        return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç
    print(f"\nüîç –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏–∑ {test_dir}...")
    tester.test_with_sample_images(test_dir)
    
    print(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    main() 