import re
import logging
from typing import Dict, Optional, List, Any, Tuple
from dataclasses import dataclass
import numpy as np
from datetime import datetime
from collections import defaultdict
import json

# –û—Å–Ω–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è E5-Large
from sentence_transformers import SentenceTransformer  
from sklearn.metrics.pairwise import cosine_similarity

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç GLiNER
try:
    from gliner import GLiNER
    GLINER_AVAILABLE = True
except ImportError:
    logging.warning("GLiNER not available, falling back to sentence-transformers only")
    GLINER_AVAILABLE = False

logger = logging.getLogger(__name__)

# –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í –° –í–ï–°–ê–ú–ò –ò –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò
class KeywordSystem:
    """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –≤–µ—Å–∞–º–∏"""
    
    # –ü–†–û–î–ê–ñ–ê - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–≤–µ—Å: 10)
    SALE_CRITICAL = [
        '–ø—Ä–æ–¥–∞—é', '–ø—Ä–æ–¥–∞–µ—Ç—Å—è', '–ø—Ä–æ–¥–∞—ë—Ç—Å—è', '–ø—Ä–æ–¥–∞–º', '–ø—Ä–æ–¥–∞–∂–∞',
        '—Å–∞—Ç—ã–ª–∞—Ç', '—Å–∞—Ç–∞–º', '—Å–∞—Ç–∞–±—ã–∑',  # –∫—ã—Ä–≥—ã–∑—Å–∫–∏–µ
        '–æ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫–∞ –ø—Ä–æ–¥–∞—é', '—Å—Ä–æ—á–Ω–æ –ø—Ä–æ–¥–∞—é', '–ø—Ä–æ–¥–∞—é —Å—Ä–æ—á–Ω–æ',
        '—Ö–æ–∑—è–∏–Ω –ø—Ä–æ–¥–∞–µ—Ç', '–≤–ª–∞–¥–µ–ª–µ—Ü –ø—Ä–æ–¥–∞–µ—Ç'
    ]
    
    # –ü–†–û–î–ê–ñ–ê - –°–∏–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 7)
    SALE_STRONG = [
        '–∫—É–ø–∏—Ç—å', '–ø–æ–∫—É–ø–∫–∞', '–ø—Ä–∏–æ–±—Ä–µ—Å—Ç–∏', '–ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ',
        '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è', '–≤—ã–≥–æ–¥–Ω–∞—è –ø–æ–∫—É–ø–∫–∞', '–¥–ª—è –ø–æ–∫—É–ø–∫–∏',
        '–ø–æ–∫—É–ø–∞—Ç–µ–ª—é', '–ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π', '–∏–Ω–≤–µ—Å—Ç–æ—Ä—É'
    ]
    
    # –ü–†–û–î–ê–ñ–ê - –°—Ä–µ–¥–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 5)
    SALE_MEDIUM = [
        '–≤—ã–≥–æ–¥–Ω–æ', '—Ç–æ—Ä–≥', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞',
        '–¥–æ–∫—É–º–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã', '—á–∏—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã', '–±–µ–∑ –¥–æ–ª–≥–æ–≤',
        '—Å—Ä–æ—á–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞', '—Å–µ–º–µ–π–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞',
        '–ø–µ—Ä–µ–µ–∑–¥', '—Å–º–µ–Ω–∞ –≥–æ—Ä–æ–¥–∞', '–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–¥–∞–∂–∞'
    ]
    
    # –ü–†–û–î–ê–ñ–ê - –°–ª–∞–±—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 3)
    SALE_WEAK = [
        '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å', '–≤ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏', '–ø—Ä–∏–≤–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω',
        '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ', '—Ç–µ—Ö–ø–∞—Å–ø–æ—Ä—Ç', '–æ—Ü–µ–Ω–∫–∞', '–Ω–æ—Ç–∞—Ä–∏—É—Å'
    ]
    
    # –ê–†–ï–ù–î–ê - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–≤–µ—Å: 10)
    RENTAL_CRITICAL = [
        '—Å–¥–∞—é', '—Å–¥–∞–µ—Ç—Å—è', '—Å–¥–∞—ë—Ç—Å—è', '—Å–¥–∞–º', '—Å–¥–∞–µ—Ç—Å—è –≤ –∞—Ä–µ–Ω–¥—É',
        '–±–µ—Ä–∏–ª–µ—Ç', '–±–µ—Ä–µ–º', '–±–µ—Ä–∏–ø –±–µ—Ä–µ—Ç',  # –∫—ã—Ä–≥—ã–∑—Å–∫–∏–µ
        '–≤ –∞—Ä–µ–Ω–¥—É', '–∞—Ä–µ–Ω–¥—É—é', '–¥–ª—è –∞—Ä–µ–Ω–¥—ã',
        '—Å–Ω–∏–º–∞—é', '—Å–Ω—è—Ç—å', '–Ω–∞–π–º—É', '–Ω–∞–µ–º'
    ]
    
    # –ê–†–ï–ù–î–ê - –°–∏–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 7)
    RENTAL_STRONG = [
        '–∞—Ä–µ–Ω–¥–∞', '–∞—Ä–µ–Ω–¥–æ–≤–∞—Ç—å', '–∞—Ä–µ–Ω–¥–∞—Ç–æ—Ä', '–∞—Ä–µ–Ω–¥–æ–¥–∞—Ç–µ–ª—å',
        '—Å—ä–µ–º—â–∏–∫', '–∫–≤–∞—Ä—Ç–∏—Ä–æ—Å—ä–µ–º—â–∏–∫', '–∂–∏–ª—å—Ü—É', '–∂–∏–ª—å—Ü–æ–≤',
        '–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏–µ', '–Ω–∞ –≤—Ä–µ–º—è'
    ]
    
    # –ê–†–ï–ù–î–ê - –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã (–≤–µ—Å: 8)
    RENTAL_PERIODS = [
        '—Å—É—Ç–∫–∏', '—Å—É—Ç–æ—á–Ω–æ', '–ø–æ—Å—É—Ç–æ—á–Ω–æ', '—Å—É—Ç–∫–∞', '–∑–∞ —Å—É—Ç–∫–∏',
        '—á–∞—Å', '—á–∞—Å–æ–≤', '–ø–æ—á–∞—Å–æ–≤–∞—è', '–Ω–∞ —á–∞—Å', '–∑–∞ —á–∞—Å',
        '–Ω–æ—á—å', '–Ω–æ—á—å—é', '–∑–∞ –Ω–æ—á—å', '–Ω–æ—á', '–Ω–æ—á–Ω–∞—è',
        '–º–µ—Å—è—Ü', '–≤ –º–µ—Å—è—Ü', '–∑–∞ –º–µ—Å—è—Ü', '–ø–æ–º–µ—Å—è—á–Ω–æ', '–º–µ—Å—è—á–Ω–∞—è',
        '–Ω–µ–¥–µ–ª—è', '–Ω–∞ –Ω–µ–¥–µ–ª—é', '–∑–∞ –Ω–µ–¥–µ–ª—é', '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–æ',
        '–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ', '–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ', '–¥–ª–∏—Ç–µ–ª—å–Ω–æ'
    ]
    
    # –ê–†–ï–ù–î–ê - –°—Ä–µ–¥–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 5)
    RENTAL_MEDIUM = [
        '–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–æ—á–Ω—ã–º', '–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞', '—Å—Ç—É–¥–µ–Ω—Ç–∞–º', '—Ä–∞–±–æ—á–∏–º',
        '—Å–µ–º–µ–π–Ω—ã–º', '–ø–∞—Ä–∞–º', '–¥–µ–≤—É—à–∫–∞–º', '–º–æ–ª–æ–¥—ã–º –ª—é–¥—è–º',
        '—Å –º–µ–±–µ–ª—å—é', '–º–µ–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è', '–æ–±—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è',
        '–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ –≤–∫–ª—é—á–µ–Ω—ã', '–≤—Å–µ –≤–∫–ª—é—á–µ–Ω–æ', '–∫–æ–º–º—É–Ω–∞–ª–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞'
    ]
    
    # –ò–°–ö–õ–Æ–ß–ê–Æ–©–ò–ï –ü–†–ê–í–ò–õ–ê - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    EXCLUSION_RULES = {
        'always_rental': [
            '—Ü–µ–Ω–∞ –∑–∞ —Å—É—Ç–∫–∏', '—Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞ –Ω–æ—á—å', '–ø–ª–∞—Ç–∞ –∑–∞ –¥–µ–Ω—å',
            '–ø–æ—Å—É—Ç–æ—á–Ω–∞—è –∞—Ä–µ–Ω–¥–∞', '–ø–æ—á–∞—Å–æ–≤–∞—è –æ–ø–ª–∞—Ç–∞', '—Å—É—Ç–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å'
        ],
        'always_sale': [
            '–æ–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–æ–ª–Ω–∞—è —Ü–µ–Ω–∞', '–∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞',
            '–∏–ø–æ—Ç–µ—á–Ω—ã–π –∫—Ä–µ–¥–∏—Ç', '–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –≤–∑–Ω–æ—Å', '—Ä–∞—Å—Å—Ä–æ—á–∫–∞ –ø–ª–∞—Ç–µ–∂–∞'
        ]
    }
    
    # –ö–´–†–ì–´–ó–°–ö–ò–ï –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –¢–ï–†–ú–ò–ù–´
    KYRGYZ_TERMS = {
        '—Å–∞—Ç—ã–ª–∞—Ç': ('–ø—Ä–æ–¥–∞–µ—Ç—Å—è', 'sale'),
        '—Å–∞—Ç–∞–±—ã–∑': ('–ø—Ä–æ–¥–∞–µ–º', 'sale'),
        '—Å–∞—Ç–∞–º': ('–ø—Ä–æ–¥–∞—é', 'sale'),
        '–±–µ—Ä–∏–ª–µ—Ç': ('—Å–¥–∞–µ—Ç—Å—è', 'rental'),
        '–±–µ—Ä–∏–ø –±–µ—Ä–µ—Ç': ('—Å–¥–∞–µ—Ç—Å—è', 'rental'),
        '–±–µ—Ä–µ–º': ('—Å–¥–∞–µ–º', 'rental'),
        '–º–∫—Ä': ('–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω', 'location'),
        '–∫”©—á”©': ('—É–ª–∏—Ü–∞', 'location'),
        '–±–æ—Ä–±–æ—Ä': ('—Ü–µ–Ω—Ç—Ä', 'location'),
        '—à–∞–∞—Ä': ('–≥–æ—Ä–æ–¥', 'location'),
        '“Ø–π': ('–¥–æ–º', 'property'),
        '–±–∞—Ç–∏—Ä': ('–∫–≤–∞—Ä—Ç–∏—Ä–∞', 'property'),
        '–∂–µ—Ä': ('–∑–µ–º–ª—è', 'property'),
        '–æ—Ñ–∏—Å': ('–æ—Ñ–∏—Å', 'property')
    }
    
    @classmethod
    def calculate_listing_type_score(cls, text: str) -> Tuple[str, float, Dict]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–∏–ø –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        text_lower = text.lower()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤
        sale_score = 0
        rental_score = 0
        details = {'reasons': []}
        
        # 1. –ü–†–û–í–ï–†–ö–ê –ò–°–ö–õ–Æ–ß–ê–Æ–©–ò–• –ü–†–ê–í–ò–õ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        for rule_type, patterns in cls.EXCLUSION_RULES.items():
            for pattern in patterns:
                if pattern in text_lower:
                    if rule_type == 'always_rental':
                        details['reasons'].append(f"–ò—Å–∫–ª—é—á–∞—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ –∞—Ä–µ–Ω–¥—ã: '{pattern}'")
                        return '–ê—Ä–µ–Ω–¥–∞', 1.0, details
                    elif rule_type == 'always_sale':
                        details['reasons'].append(f"–ò—Å–∫–ª—é—á–∞—é—â–µ–µ –ø—Ä–∞–≤–∏–ª–æ –ø—Ä–æ–¥–∞–∂–∏: '{pattern}'")
                        return '–ü—Ä–æ–¥–∞–∂–∞', 1.0, details
        
        # 2. –ê–ù–ê–õ–ò–ó –ö–´–†–ì–´–ó–°–ö–ò–• –¢–ï–†–ú–ò–ù–û–í
        for kyrgyz_term, (russian_term, category) in cls.KYRGYZ_TERMS.items():
            if kyrgyz_term in text_lower:
                if category == 'sale':
                    sale_score += 10
                    details['reasons'].append(f"–ö—ã—Ä–≥—ã–∑—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω –ø—Ä–æ–¥–∞–∂–∏: '{kyrgyz_term}' ‚Üí '{russian_term}'")
                elif category == 'rental':
                    rental_score += 10
                    details['reasons'].append(f"–ö—ã—Ä–≥—ã–∑—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω –∞—Ä–µ–Ω–¥—ã: '{kyrgyz_term}' ‚Üí '{russian_term}'")
        
        # 3. –ê–ù–ê–õ–ò–ó –ü–†–û–î–ê–ñ–ò –ü–û –£–†–û–í–ù–Ø–ú
        for pattern in cls.SALE_CRITICAL:
            if pattern in text_lower:
                sale_score += 10
                details['reasons'].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–æ–¥–∞–∂–∏: '{pattern}'")
        
        for pattern in cls.SALE_STRONG:
            if pattern in text_lower:
                sale_score += 7
                details['reasons'].append(f"–°–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–æ–¥–∞–∂–∏: '{pattern}'")
        
        for pattern in cls.SALE_MEDIUM:
            if pattern in text_lower:
                sale_score += 5
                details['reasons'].append(f"–°—Ä–µ–¥–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–æ–¥–∞–∂–∏: '{pattern}'")
        
        for pattern in cls.SALE_WEAK:
            if pattern in text_lower:
                sale_score += 3
                details['reasons'].append(f"–°–ª–∞–±—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–æ–¥–∞–∂–∏: '{pattern}'")
        
        # 4. –ê–ù–ê–õ–ò–ó –ê–†–ï–ù–î–´ –ü–û –£–†–û–í–ù–Ø–ú  
        for pattern in cls.RENTAL_CRITICAL:
            if pattern in text_lower:
                rental_score += 10
                details['reasons'].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –∞—Ä–µ–Ω–¥—ã: '{pattern}'")
        
        for pattern in cls.RENTAL_STRONG:
            if pattern in text_lower:
                rental_score += 7
                details['reasons'].append(f"–°–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –∞—Ä–µ–Ω–¥—ã: '{pattern}'")
        
        for pattern in cls.RENTAL_PERIODS:
            if pattern in text_lower:
                rental_score += 8
                details['reasons'].append(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –∞—Ä–µ–Ω–¥—ã: '{pattern}'")
        
        for pattern in cls.RENTAL_MEDIUM:
            if pattern in text_lower:
                rental_score += 5
                details['reasons'].append(f"–°—Ä–µ–¥–Ω–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –∞—Ä–µ–Ω–¥—ã: '{pattern}'")
        
        # 5. –£–ú–ù–´–ô –ê–ù–ê–õ–ò–ó –¶–ï–ù
        price_analysis = cls._analyze_price_context(text_lower)
        if price_analysis['type'] == 'sale':
            sale_score += price_analysis['confidence']
            details['reasons'].append(f"–¶–µ–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑: –ø—Ä–æ–¥–∞–∂–∞ ({price_analysis['reason']})")
        elif price_analysis['type'] == 'rental':
            rental_score += price_analysis['confidence']
            details['reasons'].append(f"–¶–µ–Ω–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑: –∞—Ä–µ–Ω–¥–∞ ({price_analysis['reason']})")
        
        # 6. –ö–û–ù–¢–ï–ö–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó
        context_analysis = cls._analyze_context(text_lower)
        sale_score += context_analysis['sale_context']
        rental_score += context_analysis['rental_context']
        details['reasons'].extend(context_analysis['reasons'])
        
        # 7. –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï
        total_sale = max(sale_score, 0)
        total_rental = max(rental_score, 0)
        
        details['scores'] = {'sale': total_sale, 'rental': total_rental}
        
        if total_sale == 0 and total_rental == 0:
            # –ê–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            default_analysis = cls._default_analysis(text_lower)
            details['reasons'].append(f"–ê–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_analysis['reason']}")
            return default_analysis['type'], default_analysis['confidence'], details
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if total_sale > total_rental:
            confidence = min(total_sale / (total_sale + total_rental + 1), 1.0)
            return '–ü—Ä–æ–¥–∞–∂–∞', confidence, details
        elif total_rental > total_sale:
            confidence = min(total_rental / (total_sale + total_rental + 1), 1.0)
            return '–ê—Ä–µ–Ω–¥–∞', confidence, details
        else:
            # –†–∞–≤–Ω—ã–µ –±–∞–ª–ª—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É
            tie_breaker = cls._tie_breaker_analysis(text_lower)
            details['reasons'].append(f"–†–∞–≤–Ω—ã–µ –±–∞–ª–ª—ã, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {tie_breaker['reason']}")
            return tie_breaker['type'], 0.5, details
    
    @classmethod
    def _analyze_price_context(cls, text: str) -> Dict:
        """–£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–µ–Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∞–ª—é—Ç –∏ —Ü–µ–Ω
        usd_matches = re.findall(r'(\$\s*)?(\d+(?:\s*\d+)*)\s*(?:\$|–¥–æ–ª–ª|USD)', text)
        som_matches = re.findall(r'(\d+(?:\s*\d+)*)\s*(?:—Å–æ–º|c)', text)
        
        max_price = 0
        currency = None
        
        # –ù–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
        for match in usd_matches:
            price_str = match[1] if match[1] else match[0]
            try:
                price = int(price_str.replace(' ', ''))
                if price > max_price:
                    max_price = price
                    currency = 'USD'
            except ValueError:
                continue
        
        for match in som_matches:
            try:
                price = int(match.replace(' ', ''))
                if price > max_price:
                    max_price = price
                    currency = 'SOM'
            except ValueError:
                continue
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω—ã
        if max_price == 0:
            return {'type': None, 'confidence': 0, 'reason': '–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}
        
        # –õ–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —Ü–µ–Ω–µ
        if currency == 'USD':
            if max_price > 50000:
                return {'type': 'sale', 'confidence': 6, 'reason': f'–í—ã—Å–æ–∫–∞—è —Ü–µ–Ω–∞ –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö: ${max_price}'}
            elif max_price < 500:
                return {'type': 'rental', 'confidence': 6, 'reason': f'–ù–∏–∑–∫–∞—è —Ü–µ–Ω–∞ –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö: ${max_price}'}
        elif currency == 'SOM':
            if max_price > 3000000:  # > 3 –º–ª–Ω —Å–æ–º
                return {'type': 'sale', 'confidence': 6, 'reason': f'–í—ã—Å–æ–∫–∞—è —Ü–µ–Ω–∞ –≤ —Å–æ–º–∞—Ö: {max_price} —Å–æ–º'}
            elif max_price < 100000:  # < 100k —Å–æ–º
                return {'type': 'rental', 'confidence': 6, 'reason': f'–ù–∏–∑–∫–∞—è —Ü–µ–Ω–∞ –≤ —Å–æ–º–∞—Ö: {max_price} —Å–æ–º'}
        
        return {'type': None, 'confidence': 0, 'reason': f'–¶–µ–Ω–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è: {max_price} {currency}'}
    
    @classmethod
    def _analyze_context(cls, text: str) -> Dict:
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        sale_context = 0
        rental_context = 0
        reasons = []
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–¥–∞–∂–∏
        sale_contexts = [
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–æ—Ä—è–¥–∫–µ', '—Å–≤–æ–±–æ–¥–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞', '—Ç–æ—Ä–≥ —É–º–µ—Å—Ç–µ–Ω',
            '—Å—Ä–æ—á–Ω—ã–π –ø–µ—Ä–µ–µ–∑–¥', '—Å–º–µ–Ω–∞ –º–µ—Å—Ç–∞ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–∞', '—Å–µ–º–µ–π–Ω—ã–µ –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞',
            '–≤–ª–æ–∂–µ–Ω–∏–µ –¥–µ–Ω–µ–≥', '—Ö–æ—Ä–æ—à–∞—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è', '—Ä–∞—Å—Ç–µ—Ç –≤ —Ü–µ–Ω–µ'
        ]
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞—Ä–µ–Ω–¥—ã
        rental_contexts = [
            '–¥–ª—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è', '–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∂–∏–ª—å–µ', '–Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π —Å—Ä–æ–∫',
            '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞', '–∑–∞–ª–æ–≥', '–∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏',
            '–ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è', '–±–µ–∑ –∂–∏–≤–æ—Ç–Ω—ã—Ö', '–Ω–µ–∫—É—Ä—è—â–∏–º'
        ]
        
        for context in sale_contexts:
            if context in text:
                sale_context += 2
                reasons.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–¥–∞–∂–∏: '{context}'")
        
        for context in rental_contexts:
            if context in text:
                rental_context += 2
                reasons.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∞—Ä–µ–Ω–¥—ã: '{context}'")
        
        return {
            'sale_context': sale_context,
            'rental_context': rental_context,
            'reasons': reasons
        }
    
    @classmethod
    def _default_analysis(cls, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–æ–≥–¥–∞ –Ω–µ—Ç —è–≤–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö - —Å–∫–æ—Ä–µ–µ –ø—Ä–æ–¥–∞–∂–∞
        if any(word in text for word in ['–¥–æ–∫—É–º–µ–Ω—Ç', '–ø–∞—Å–ø–æ—Ä—Ç', '—Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ']):
            return {'type': '–ü—Ä–æ–¥–∞–∂–∞', 'confidence': 0.3, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤'}
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –º–µ–±–µ–ª—å –∏–ª–∏ —É–¥–æ–±—Å—Ç–≤–∞ - —Å–∫–æ—Ä–µ–µ –∞—Ä–µ–Ω–¥–∞
        if any(word in text for word in ['–º–µ–±–µ–ª—å', '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫', '—Å—Ç–∏—Ä–∞–ª—å–Ω–∞—è', '–ø–æ—Å—É–¥–∞']):
            return {'type': '–ê—Ä–µ–Ω–¥–∞', 'confidence': 0.3, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –º–µ–±–µ–ª–∏/—É–¥–æ–±—Å—Ç–≤'}
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–¥–∞–∂–∞ (–∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
        return {'type': '–ü—Ä–æ–¥–∞–∂–∞', 'confidence': 0.2, 'reason': '–ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é'}
    
    @classmethod
    def _tie_breaker_analysis(cls, text: str) -> Dict:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö –±–∞–ª–ª–∞—Ö"""
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        if len(text) > 500:
            return {'type': '–ü—Ä–æ–¥–∞–∂–∞', 'reason': '–î–ª–∏–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–æ–±—ã—á–Ω–æ –ø—Ä–æ–¥–∞–∂–∞)'}
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–ª–∏—á–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        if re.search(r'\+996|0\d{9}', text):
            return {'type': '–ê—Ä–µ–Ω–¥–∞', 'reason': '–ú–Ω–æ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ (–æ–±—ã—á–Ω–æ –∞—Ä–µ–Ω–¥–∞)'}
        
        return {'type': '–ü—Ä–æ–¥–∞–∂–∞', 'reason': '–°–ª—É—á–∞–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏'}

# –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –¢–ò–ü–û–í –ù–ï–î–í–ò–ñ–ò–ú–û–°–¢–ò  
class PropertyTypeSystem:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    PROPERTY_TYPES = {
        '–ö–≤–∞—Ä—Ç–∏—Ä–∞': {
            'primary': [
                '–∫–≤–∞—Ä—Ç–∏—Ä', '–∫–≤–∞—Ä—Ç–∏—Ä–∞', '–∫–≤–∞—Ä—Ç–∏—Ä—É', '–∫–≤–∞—Ä—Ç–∏—Ä–µ', '–∫–≤–∞—Ä—Ç–∏—Ä—ã',
                '–∫–æ–º–Ω–∞—Ç', '–∫–æ–º–Ω–∞—Ç–∞', '–∫–æ–º–Ω–∞—Ç—É', '–∫–æ–º–Ω–∞—Ç—ã', '–∫–æ–º–Ω–∞—Ç–µ',
                '–∫–≤.', '–∫–≤', '–∫–≤,', '–∫–≤–∞—Ä—Ç–∏—Ä.', # –í–∞–∂–Ω–æ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ç–∏–ø–∞ "1-–∫–æ–º–Ω. –∫–≤."
                '–±–∞—Ç–∏—Ä'  # –∫—ã—Ä–≥—ã–∑—Å–∫–∏–π
            ],
            'secondary': [
                '—Å—Ç—É–¥–∏', '—Å—Ç—É–¥–∏—è', '—Å—Ç—É–¥–∏—é', '–ø–µ–Ω—Ç—Ö–∞—É—Å', '–ø–µ–Ω—Ç—Ö–∞—É–∑', '–ª–æ—Ñ—Ç',
                '–º–∞–ª–æ—Å–µ–º–µ–π–∫–∞', '–º–∞–ª–æ–≥–∞–±–∞—Ä–∏—Ç–Ω–∞—è', '—Ö—Ä—É—â–µ–≤–∫–∞', '–±—Ä–µ–∂–Ω–µ–≤–∫–∞',
                '–Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞—Ä—Ç–∏—Ä–∞', '—ç–ª–∏—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞', '–µ–≤—Ä–æ–¥–≤—É—à–∫–∞',
                '–∂–∫', '–∂–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å', '–∂–∏–ª–∫–æ–º'
            ],
            'context': [
                '–∂–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å', '–∂–∏–ª—ã—Ö –∫–æ–º–Ω–∞—Ç', '—Å–ø–∞–ª—å–Ω', '–≥–æ—Å—Ç–∏–Ω', '–ø—Ä–∏—Ö–æ–∂–∞—è',
                '–±–∞–ª–∫–æ–Ω', '–ª–æ–¥–∂–∏—è', '—Å–∞–Ω—É–∑–µ–ª', '–≤–∞–Ω–Ω–∞—è –∫–æ–º–Ω–∞—Ç–∞', '–∫—É—Ö–Ω—è',
                '—ç—Ç–∞–∂', '–ø–æ–¥—ä–µ–∑–¥', '–ª–∏—Ñ—Ç', '–ø–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –æ–∫–Ω–∞'
            ],
            'modifiers': [
                '–æ–¥–Ω–æ–∫–æ–º–Ω–∞—Ç', '1-–∫–æ–º–Ω–∞—Ç', '1–∫', '1-–∫', '–æ–¥–Ω–æ –∫–æ–º–Ω–∞—Ç', '1 –∫–æ–º–Ω',
                '–¥–≤—É—Ö–∫–æ–º–Ω–∞—Ç', '2-–∫–æ–º–Ω–∞—Ç', '2–∫', '2-–∫', '–¥–≤—É—Ö –∫–æ–º–Ω–∞—Ç', '2 –∫–æ–º–Ω', 
                '—Ç—Ä–µ—Ö–∫–æ–º–Ω–∞—Ç', '3-–∫–æ–º–Ω–∞—Ç', '3–∫', '3-–∫', '—Ç—Ä–µ—Ö –∫–æ–º–Ω–∞—Ç', '3 –∫–æ–º–Ω',
                '—á–µ—Ç—ã—Ä–µ—Ö–∫–æ–º–Ω–∞—Ç', '4-–∫–æ–º–Ω–∞—Ç', '4–∫', '4-–∫', '—á–µ—Ç—ã—Ä–µ—Ö –∫–æ–º–Ω–∞—Ç', '4 –∫–æ–º–Ω',
                '–ø—è—Ç–∏–∫–æ–º–Ω–∞—Ç', '5-–∫–æ–º–Ω–∞—Ç', '5–∫', '5-–∫', '–ø—è—Ç–∏ –∫–æ–º–Ω–∞—Ç', '5 –∫–æ–º–Ω'
            ],
            'weight': 1.0
        },
        
        '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º': {
            'primary': [
                '–¥–æ–º', '–¥–æ–º–∞', '–¥–æ–º–æ–º', '–¥–æ–º–µ', '–∫–æ—Ç—Ç–µ–¥–∂', '–∫–æ—Ç—Ç–µ–¥–∂–∞', '–∫–æ—Ç—Ç–µ–¥–∂–µ–º',
                '“Ø–π', '“Ø–π–¥“Ø'  # –∫—ã—Ä–≥—ã–∑—Å–∫–∏–π
            ],
            'secondary': [
                '–æ—Å–æ–±–Ω—è–∫', '–æ—Å–æ–±–Ω—è–∫–∞', '—É—Å–∞–¥—å–±–∞', '—É—Å–∞–¥—å–±—É', '—É—Å–∞–¥—å–±–æ–π',
                '–¥–∞—á–∞', '–¥–∞—á—É', '–¥–∞—á–µ–π', '–¥–∞—á–Ω—ã–π –¥–æ–º', '–∑–∞–≥–æ—Ä–æ–¥–Ω—ã–π –¥–æ–º',
                '—Ç–∞—É–Ω—Ö–∞—É—Å', '—Ç–∞—É–Ω—Ö–∞—É–∑', '–≤–∏–ª–ª–∞', '–≤–∏–ª–ª—É', '–∫–æ—Ç—Ç–µ–¥–∂–Ω—ã–π –ø–æ—Å–µ–ª–æ–∫'
            ],
            'context': [
                '—É—á–∞—Å—Ç–æ–∫', '–¥–≤–æ—Ä', '—Å–∞–¥', '–æ–≥–æ—Ä–æ–¥', '—Ç–µ–ø–ª–∏—Ü–∞', '–≥–∞—Ä–∞–∂',
                '–±–∞–Ω—è', '—Å–∞—É–Ω–∞', '–±–∞—Å—Å–µ–π–Ω', '–∑–∞–±–æ—Ä', '–≤–æ—Ä–æ—Ç–∞', '–∫–∞–ª–∏—Ç–∫–∞',
                '—ç—Ç–∞–∂–∞', '—ç—Ç–∞–∂–Ω—ã–π', '–º–∞–Ω—Å–∞—Ä–¥–∞', '–ø–æ–¥–≤–∞–ª', '—Ü–æ–∫–æ–ª—å'
            ],
            'modifiers': [
                '–æ–¥–Ω–æ—ç—Ç–∞–∂–Ω—ã–π', '1-—ç—Ç–∞–∂–Ω—ã–π', '–¥–≤—É—Ö—ç—Ç–∞–∂–Ω—ã–π', '2-—ç—Ç–∞–∂–Ω—ã–π',
                '—Ç—Ä–µ—Ö—ç—Ç–∞–∂–Ω—ã–π', '3-—ç—Ç–∞–∂–Ω—ã–π', '–∫–∏—Ä–ø–∏—á–Ω—ã–π', '–¥–µ—Ä–µ–≤—è–Ω–Ω—ã–π',
                '–±–ª–æ—á–Ω—ã–π', '–ø–∞–Ω–µ–ª—å–Ω—ã–π', '–º–æ–Ω–æ–ª–∏—Ç–Ω—ã–π'
            ],
            'weight': 1.0
        },
        
        '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫': {
            'primary': [
                '—É—á–∞—Å—Ç–æ–∫', '—É—á–∞—Å—Ç–∫–∞', '—É—á–∞—Å—Ç–∫–æ–º', '—É—á–∞—Å—Ç–∫–µ', '–∑–µ–º–ª—è', '–∑–µ–º–ª–∏', '–∑–µ–º–ª–µ–π',
                '–∂–µ—Ä', '–∂–µ—Ä–¥–∏'  # –∫—ã—Ä–≥—ã–∑—Å–∫–∏–π
            ],
            'secondary': [
                '–∑–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', '–∑–µ–º–µ–ª—å–Ω—ã–π –Ω–∞–¥–µ–ª', '–Ω–∞–¥–µ–ª', '–¥–µ–ª—è–Ω–∫–∞',
                '—Å–æ—Ç–∫–∞', '—Å–æ—Ç–∫–∏', '—Å–æ—Ç–æ–∫', '–≥–µ–∫—Ç–∞—Ä', '–≥–µ–∫—Ç–∞—Ä–∞', '–≥–∞'
            ],
            'context': [
                '–ø–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø–æ–¥ –∑–∞—Å—Ç—Ä–æ–π–∫—É', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫',
                '–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '–≥–∞–∑', '–≤–æ–¥–∞', '–∫–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è',
                '–æ–≥–æ—Ä–æ–∂–µ–Ω', '—Ä–æ–≤–Ω—ã–π', '—Å —É–∫–ª–æ–Ω–æ–º', '–ø–ª–æ–¥–æ—Ä–æ–¥–Ω–∞—è'
            ],
            'modifiers': [
                '–ò–ñ–°', '–õ–ü–•', '—Å–∞–¥–æ–≤—ã–π', '–¥–∞—á–Ω—ã–π', '—Å–µ–ª—å—Ö–æ–∑', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π',
                '–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π', '–ø–æ–¥ –±–∏–∑–Ω–µ—Å'
            ],
            'weight': 1.2  # –ë–æ–ª—å—à–µ –≤–µ—Å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        },
        
        '–û—Ñ–∏—Å': {
            'primary': [
                '–æ—Ñ–∏—Å', '–æ—Ñ–∏—Å–∞', '–æ—Ñ–∏—Å–æ–º', '–æ—Ñ–∏—Å–µ', '–æ—Ñ–∏—Å—ã', '–æ—Ñ–∏—Å–æ–≤',
                '–∫–∞–±–∏–Ω–µ—Ç', '–∫–∞–±–∏–Ω–µ—Ç–∞', '–∫–∞–±–∏–Ω–µ—Ç–æ–º'
            ],
            'secondary': [
                '–æ—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ', '—Ä–∞–±–æ—á–µ–µ –º–µ—Å—Ç–æ', '—Ä–∞–±–æ—á–µ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ',
                '–¥–µ–ª–æ–≤–æ–π —Ü–µ–Ω—Ç—Ä', '–±–∏–∑–Ω–µ—Å —Ü–µ–Ω—Ç—Ä', '–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä',
                '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ'
            ],
            'context': [
                '–ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω–∞—è', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü-–∑–∞–ª', '–ø—Ä–∏–µ–º–Ω–∞—è', 'open space',
                '–æ–ø–µ–Ω —Å–ø–µ–π—Å', '–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–ø–∞—Ä–∫–æ–≤–∫–∞',
                '–æ—Ö—Ä–∞–Ω–∞', '–ª–∏—Ñ—Ç', '—Ü–µ–Ω—Ç—Ä –≥–æ—Ä–æ–¥–∞'
            ],
            'modifiers': [
                '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å–∫–∏–π', '–∫–ª–∞—Å—Å–∞ –ê', '–∫–ª–∞—Å—Å–∞ –í', '–∫–ª–∞—Å—Å–∞ –°',
                '—ç–ª–∏—Ç–Ω—ã–π', '–ø—Ä–µ–º–∏—É–º', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '—ç–∫–æ–Ω–æ–º'
            ],
            'weight': 1.1
        },
        
        '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å': {
            'primary': [
                '–º–∞–≥–∞–∑–∏–Ω', '–º–∞–≥–∞–∑–∏–Ω–∞', '–º–∞–≥–∞–∑–∏–Ω–æ–º', '—Ç–æ—Ä–≥–æ–≤–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ',
                '—Ç–æ—Ä–≥–æ–≤–∞—è –ø–ª–æ—â–∞–¥—å', '—Ç–æ—Ä–≥–æ–≤—ã–π –∑–∞–ª'
            ],
            'secondary': [
                '—Å–∫–ª–∞–¥', '—Å–∫–ª–∞–¥–æ–º', '—Å–∫–ª–∞–¥—Å–∫–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ', '–∞–Ω–≥–∞—Ä', '–∞–Ω–≥–∞—Ä–∞',
                '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ', '—Ü–µ—Ö', '–º–∞—Å—Ç–µ—Ä—Å–∫–∞—è',
                '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '—Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞', '–∫–∞—Ñ–µ', '–∫–∞—Ñ–µ–º', '—Å—Ç–æ–ª–æ–≤–∞—è',
                '—Å–∞–ª–æ–Ω', '—Å–∞–ª–æ–Ω–∞', '—Å—Ç—É–¥–∏—è', '–∞—Ç–µ–ª—å–µ', '–º–∏–Ω–∏-–≥–æ—Å—Ç–∏–Ω–∏—Ü–∞'
            ],
            'context': [
                '—Ç–æ—Ä–≥–æ–≤–ª—è', '–±–∏–∑–Ω–µ—Å', '–∫–æ–º–º–µ—Ä—Ü–∏—è', '–∞—Ä–µ–Ω–¥–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è',
                '–ø—Ä–æ—Ö–æ–¥–∏–º–æ—Å—Ç—å', '–≤–∏—Ç—Ä–∏–Ω–∞', '–≤—ã–≤–µ—Å–∫–∞', '–ø–∞—Ä–∫–æ–≤–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤',
                '–æ—Ç–¥–µ–ª—å–Ω—ã–π –≤—Ö–æ–¥', '–ø–µ—Ä–≤–∞—è –ª–∏–Ω–∏—è', '–∫—Ä–∞—Å–Ω–∞—è –ª–∏–Ω–∏—è'
            ],
            'modifiers': [
                '—Å—Ç—Ä–∏—Ç-—Ä–∏—Ç–µ–π–ª', '—Ñ—É–¥-–∫–æ—Ä—Ç', '—Ç–æ—Ä–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä', '–¢–¶',
                '—Ç–æ—Ä–≥–æ–≤—ã–π –∫–æ–º–ø–ª–µ–∫—Å', '—Ä—ã–Ω–æ–∫', '–ø–∞–≤–∏–ª—å–æ–Ω', '–∫–∏–æ—Å–∫'
            ],
            'weight': 1.0
        },
        
        '–ì–∞—Ä–∞–∂': {
            'primary': [
                '–≥–∞—Ä–∞–∂', '–≥–∞—Ä–∞–∂–∞', '–≥–∞—Ä–∞–∂–æ–º', '–≥–∞—Ä–∞–∂–µ', '–≥–∞—Ä–∞–∂–∏',
                '–±–æ–∫—Å', '–±–æ–∫—Å–∞', '–±–æ–∫—Å–æ–º', '–±–æ–∫—Å–µ'
            ],
            'secondary': [
                '–º–∞—à–∏–Ω–æ-–º–µ—Å—Ç–æ', '–º–∞—à–∏–Ω–æ–º–µ—Å—Ç–æ', '–ø–∞—Ä–∫–æ–≤–æ—á–Ω–æ–µ –º–µ—Å—Ç–æ',
                '—Å—Ç–æ—è–Ω–∫–∞', '–ø–∞—Ä–∫–∏–Ω–≥', '–ø–æ–¥–∑–µ–º–Ω—ã–π –≥–∞—Ä–∞–∂'
            ],
            'context': [
                '–æ—Ö—Ä–∞–Ω—è–µ–º—ã–π', '–∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤', '–≥–∞—Ä–∞–∂–Ω—ã–π –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤',
                '—Å–º–æ—Ç—Ä–æ–≤–∞—è —è–º–∞', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '–æ—Ç–æ–ø–ª–µ–Ω–∏–µ', '–≤–µ–Ω—Ç–∏–ª—è—Ü–∏—è'
            ],
            'modifiers': [
                '–∫–∏—Ä–ø–∏—á–Ω—ã–π', '–º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏–π', '–∂–µ–ª–µ–∑–Ω—ã–π', '–∫–∞–ø–∏—Ç–∞–ª—å–Ω—ã–π',
                '–≤—Ä–µ–º–µ–Ω–Ω—ã–π', '—Ä–∞–∑–±–æ—Ä–Ω—ã–π'
            ],
            'weight': 1.1
        }
    }
    
    @classmethod
    def classify_property_type(cls, text: str) -> Tuple[Optional[str], float, Dict]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        text_lower = text.lower()
        scores = {}
        details = {'matches': [], 'reasons': []}
        
        # –ü–æ–¥—Å—á–µ—Ç –±–∞–ª–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        for prop_type, config in cls.PROPERTY_TYPES.items():
            score = 0
            matches = []
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–≤–µ—Å: 10)
            for term in config['primary']:
                if term in text_lower:
                    score += 10 * config['weight']
                    matches.append(f"–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω: '{term}'")
            
            # –í—Ç–æ—Ä–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–≤–µ—Å: 7)
            for term in config['secondary']:
                if term in text_lower:
                    score += 7 * config['weight']
                    matches.append(f"–í—Ç–æ—Ä–∏—á–Ω—ã–π —Ç–µ—Ä–º–∏–Ω: '{term}'")
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–≤–µ—Å: 3)
            for term in config['context']:
                if term in text_lower:
                    score += 3 * config['weight']
                    matches.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: '{term}'")
            
            # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–≤–µ—Å: 2)
            for term in config['modifiers']:
                if term in text_lower:
                    score += 2 * config['weight']
                    matches.append(f"–ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä: '{term}'")
            
            if score > 0:
                scores[prop_type] = score
                details['matches'].extend([f"{prop_type}: {match}" for match in matches])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        cls._apply_additional_rules(text_lower, scores, details)
        
        if not scores:
            # Fallback –∞–Ω–∞–ª–∏–∑
            fallback = cls._fallback_analysis(text_lower)
            details['reasons'].append(f"Fallback –∞–Ω–∞–ª–∏–∑: {fallback['reason']}")
            return fallback['type'], fallback['confidence'], details
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ —Ç–∏–ø–∞
        best_type = max(scores, key=scores.get)
        best_score = scores[best_type]
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if best_score >= 40:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
            confidence = 0.95
        elif best_score >= 20:  # –°–∏–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            confidence = 0.85
        elif best_score >= 10:  # –°—Ä–µ–¥–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            confidence = 0.75
        else:
            # –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –Ω–∏–∑–∫–∏—Ö –±–∞–ª–ª–æ–≤
            total_score = sum(scores.values())
            confidence = min(best_score / (total_score + 5), 0.65)
        
        details['scores'] = scores
        details['reasons'].append(f"–í—ã–±—Ä–∞–Ω '{best_type}' —Å –±–∞–ª–ª–æ–º {best_score}")
        
        return best_type, confidence, details
    
    @classmethod
    def _apply_additional_rules(cls, text: str, scores: Dict, details: Dict):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –∫–≤–∞—Ä—Ç–∏—Ä–∞–º–∏ - —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –∫–≤–∞—Ä—Ç–∏—Ä–∞
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        apartment_patterns = [
            r'\d+\s*-?\s*–∫–æ–º–Ω\.\s*–∫–≤\.',  # "1-–∫–æ–º–Ω. –∫–≤."
            r'\d+\s*-?\s*–∫\.\s*–∫–≤\.',     # "1-–∫. –∫–≤."  
            r'\d+\s*-?\s*–∫\.–∫–≤\.',        # "1-–∫.–∫–≤."
            r'\d+\s*-?\s*–∫\.–∫–≤',          # "1-–∫.–∫–≤" (–±–µ–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–∏)
            r'\d+\s*-?\s*–∫\s+–∫–≤\.',       # "1-–∫ –∫–≤."
            r'\d+\s*-?\s*–∫\s+—Å—Ç—É–¥–∏',      # "2-–∫ —Å—Ç—É–¥–∏—é"
            r'—Å—Ç—É–¥–∏[–∏—è—é–µ]',               # "—Å—Ç—É–¥–∏—è", "—Å—Ç—É–¥–∏—é" –∏ —Ç.–¥.
        ]
        
        for pattern in apartment_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = 50  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª
                details['reasons'].append(f"–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ü–†–ê–í–ò–õ–û: –Ω–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω '{pattern}' ‚Üí –∫–≤–∞—Ä—Ç–∏—Ä–∞")
                # –£–±–∏—Ä–∞–µ–º –±–∞–ª–ª—ã —É –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –µ—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–≤–∞—Ä—Ç–∏—Ä—ã
                for other_type in ['–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º', '–û—Ñ–∏—Å', '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å']:
                    scores.pop(other_type, None)
                break
        
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å "—Å–æ—Ç–∫–∏" –∏–ª–∏ "–≥–µ–∫—Ç–∞—Ä—ã" - —ç—Ç–æ —Ç–æ—á–Ω–æ —É—á–∞—Å—Ç–æ–∫
        if any(term in text for term in ['—Å–æ—Ç–∫', '–≥–µ–∫—Ç–∞—Ä', '–≥–∞ ', ' –≥–∞']):
            if '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫' in scores:
                scores['–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫'] *= 2
            else:
                scores['–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫'] = 20
            details['reasons'].append("–ü—Ä–∞–≤–∏–ª–æ: —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å–æ—Ç–æ–∫/–≥–µ–∫—Ç–∞—Ä–æ–≤ ‚Üí —É—á–∞—Å—Ç–æ–∫")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –∫–≤–∞—Ä—Ç–∏—Ä—ã - —ç—Ç–æ –∫–≤–∞—Ä—Ç–∏—Ä–∞
        apt_number = re.search(r'–∫–≤\.?\s*\d+|–∫–≤–∞—Ä—Ç–∏—Ä–∞\s*\d+|‚Ññ\s*\d+', text)
        if apt_number:
            if '–ö–≤–∞—Ä—Ç–∏—Ä–∞' in scores:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] += 15
            else:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = 20
            details['reasons'].append(f"–ü—Ä–∞–≤–∏–ª–æ: –Ω–æ–º–µ—Ä –∫–≤–∞—Ä—Ç–∏—Ä—ã '{apt_number.group()}' ‚Üí –∫–≤–∞—Ä—Ç–∏—Ä–∞")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å —ç—Ç–∞–∂–∏ (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã) - —Å–∫–æ—Ä–µ–µ –∫–≤–∞—Ä—Ç–∏—Ä–∞
        floor_patterns = [
            r'\d+\s*—ç—Ç–∞–∂\s*–∏–∑\s*\d+',      # "11 —ç—Ç–∞–∂ –∏–∑ 15"
            r'—ç—Ç–∞–∂\s*\d+\s*–∏–∑\s*\d+',      # "–≠—Ç–∞–∂ 11 –∏–∑ 15"  
            r'—ç—Ç–∞–∂\s*\d+/\d+',             # "–≠—Ç–∞–∂ 11/14"
            r'\d+/\d+\s*—ç—Ç–∞–∂',             # "11/14 —ç—Ç–∞–∂"
        ]
        
        floor_found = False
        for pattern in floor_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                if '–ö–≤–∞—Ä—Ç–∏—Ä–∞' in scores:
                    scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] += 15
                else:
                    scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = 18
                details['reasons'].append(f"–ü—Ä–∞–≤–∏–ª–æ: –Ω–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —ç—Ç–∞–∂–µ–π '{pattern}' ‚Üí –∫–≤–∞—Ä—Ç–∏—Ä–∞")
                floor_found = True
                break
        
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å "—ç—Ç–∞–∂" –±–µ–∑ "–¥–æ–º" - —Å–∫–æ—Ä–µ–µ –∫–≤–∞—Ä—Ç–∏—Ä–∞
        if not floor_found and '—ç—Ç–∞–∂' in text.lower() and '–¥–æ–º' not in text.lower():
            if '–ö–≤–∞—Ä—Ç–∏—Ä–∞' in scores:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] += 8
            else:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = 10
            details['reasons'].append("–ü—Ä–∞–≤–∏–ª–æ: —ç—Ç–∞–∂ –±–µ–∑ –¥–æ–º–∞ ‚Üí –∫–≤–∞—Ä—Ç–∏—Ä–∞")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å "–¥–≤–æ—Ä" —Å "–¥–æ–º–æ–º" - —Ç–æ—á–Ω–æ –¥–æ–º
        if '–¥–≤–æ—Ä' in text and any(term in text for term in ['–¥–æ–º', '–∫–æ—Ç—Ç–µ–¥–∂']):
            if '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º' in scores:
                scores['–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º'] += 8
            else:
                scores['–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º'] = 12
            details['reasons'].append("–ü—Ä–∞–≤–∏–ª–æ: –¥–≤–æ—Ä + –¥–æ–º ‚Üí —á–∞—Å—Ç–Ω—ã–π –¥–æ–º")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –ø–ª–æ—â–∞–¥—å –≤ –º2 + —ç—Ç–∞–∂ + –±–µ–∑ —É—á–∞—Å—Ç–∫–∞/—Å–æ—Ç–æ–∫ - —Å–∫–æ—Ä–µ–µ –∫–≤–∞—Ä—Ç–∏—Ä–∞
        if (re.search(r'\d+\.?\d*\s*–º2', text) and '—ç—Ç–∞–∂' in text and 
            not any(term in text for term in ['—É—á–∞—Å—Ç–æ–∫', '—Å–æ—Ç–∫', '–¥–≤–æ—Ä', '–∫–æ—Ç—Ç–µ–¥–∂'])):
            if '–ö–≤–∞—Ä—Ç–∏—Ä–∞' in scores:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] += 5
            else:
                scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = 8
            details['reasons'].append("–ü—Ä–∞–≤–∏–ª–æ: –º2 + —ç—Ç–∞–∂ –±–µ–∑ —É—á–∞—Å—Ç–∫–∞ ‚Üí –∫–≤–∞—Ä—Ç–∏—Ä–∞")
    
    @classmethod
    def _fallback_analysis(cls, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∫–≤–∞—Ä—Ç–∏—Ä
        apartment_patterns = [
            (r'\d+\s*-?\s*–∫–æ–º–Ω\.\s*–∫–≤\.', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å "–∫–æ–º–Ω. –∫–≤."'),
            (r'\d+\s*-?\s*–∫\.\s*–∫–≤\.', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å "–∫. –∫–≤."'),
            (r'\d+\s*-?\s*–∫\.–∫–≤\.', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å "–∫.–∫–≤"'),
            (r'\d+\s*-?\s*–∫\s+—Å—Ç—É–¥–∏', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ —Å—Ç—É–¥–∏–µ–π'),
            (r'—Å—Ç—É–¥–∏[–∏—è—é–µ]', '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å—Ç—É–¥–∏–∏'),
        ]
        
        for pattern, reason in apartment_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return {'type': '–ö–≤–∞—Ä—Ç–∏—Ä–∞', 'confidence': 0.9, 'reason': reason}
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if any(word in text.lower() for word in ['1–∫', '2–∫', '3–∫', '4–∫', '5–∫', '1-–∫', '2-–∫', '3-–∫', '4-–∫', '5-–∫']):
            return {'type': '–ö–≤–∞—Ä—Ç–∏—Ä–∞', 'confidence': 0.85, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç'}
            
        if '–∫–æ–º–Ω' in text and '—ç—Ç–∞–∂' in text:
            return {'type': '–ö–≤–∞—Ä—Ç–∏—Ä–∞', 'confidence': 0.85, 'reason': '–ö–æ–º–Ω–∞—Ç—ã + —ç—Ç–∞–∂'}
        
        if any(word in text for word in ['—É—á–∞—Å—Ç–æ–∫', '—Å–æ—Ç–∫', '–∑–µ–º–ª']):
            return {'type': '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', 'confidence': 0.8, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ —É—á–∞—Å—Ç–∫–∞/–∑–µ–º–ª–∏'}
        
        if any(word in text for word in ['–¥–æ–º', '–∫–æ—Ç—Ç–µ–¥–∂']) and '–¥–≤–æ—Ä' in text:
            return {'type': '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º', 'confidence': 0.8, 'reason': '–î–æ–º —Å –¥–≤–æ—Ä–æ–º'}
        
        if any(word in text for word in ['–¥–æ–º', '–∫–æ—Ç—Ç–µ–¥–∂']) and not '—ç—Ç–∞–∂' in text:
            return {'type': '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º', 'confidence': 0.7, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –¥–æ–º–∞'}
        
        if any(word in text for word in ['–æ—Ñ–∏—Å', '–∫–∞–±–∏–Ω–µ—Ç', '–±–∏–∑–Ω–µ—Å']):
            return {'type': '–û—Ñ–∏—Å', 'confidence': 0.7, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ—Ñ–∏—Å–∞'}
        
        if any(word in text for word in ['–º–∞–≥–∞–∑–∏–Ω', '—Ç–æ—Ä–≥–æ–≤–æ–µ', '–∫–æ–º–º–µ—Ä—á–µ—Å–∫']):
            return {'type': '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', 'confidence': 0.7, 'reason': '–£–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–ª–∏'}
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–≤–∞—Ä—Ç–∏—Ä–∞ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        return {'type': '–ö–≤–∞—Ä—Ç–∏—Ä–∞', 'confidence': 0.4, 'reason': '–¢–∏–ø –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é'}

# –°–ò–°–¢–ï–ú–ê –í–ê–õ–ò–î–ê–¶–ò–ò –ò –ö–ê–ß–ï–°–¢–í–ê
class DataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def validate_area(area: float) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–ª–æ—â–∞–¥–∏"""
        return 5 <= area <= 10000
    
    @staticmethod
    def validate_floor(floor: int, total_floors: Optional[int] = None) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —ç—Ç–∞–∂–∞"""
        if not (1 <= floor <= 50):
            return False
        if total_floors and floor > total_floors:
            return False
        return True
    
    @staticmethod
    def validate_rooms(rooms: int) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç"""
        return 1 <= rooms <= 20
    
    @staticmethod
    def validate_phone(phone: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–∞"""
        # –ö—ã—Ä–≥—ã–∑—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç +996XXXXXXXXX
        pattern = r'^\+996[0-9]{9}$'
        return bool(re.match(pattern, phone))
    
    @staticmethod
    def calculate_quality_score(extracted_data: Dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        quality_factors = {}
        total_weight = 0
        weighted_score = 0
        
        # –û—Ü–µ–Ω–∫–∞ —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–≤–µ—Å: 3)
        if extracted_data.get('property_type_confidence'):
            quality_factors['property_type'] = extracted_data['property_type_confidence']
            weighted_score += extracted_data['property_type_confidence'] * 3
            total_weight += 3
        
        # –û—Ü–µ–Ω–∫–∞ —Ç–∏–ø–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (–≤–µ—Å: 3)
        if extracted_data.get('listing_type_confidence'):
            quality_factors['listing_type'] = extracted_data['listing_type_confidence']
            weighted_score += extracted_data['listing_type_confidence'] * 3
            total_weight += 3
        
        # –ù–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–≤–µ—Å: –ø–æ 1)
        key_fields = ['area_sqm', 'rooms', 'phones', 'location']
        for field in key_fields:
            if extracted_data.get(field):
                quality_factors[field] = 1.0
                weighted_score += 1.0
                total_weight += 1
            else:
                quality_factors[field] = 0.0
        
        # –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö (–≤–µ—Å: 2)
        validation_score = DataValidator._validate_extracted_data(extracted_data)
        quality_factors['validation'] = validation_score
        weighted_score += validation_score * 2
        total_weight += 2
        
        final_score = weighted_score / total_weight if total_weight > 0 else 0
        
        return min(final_score, 1.0)
    
    @staticmethod
    def _validate_extracted_data(data: Dict) -> float:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        validations = []
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–ª–æ—â–∞–¥–∏
        if data.get('area_sqm'):
            validations.append(DataValidator.validate_area(data['area_sqm']))
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —ç—Ç–∞–∂–∞
        if data.get('floor'):
            validations.append(DataValidator.validate_floor(
                data['floor'], data.get('total_floors')
            ))
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–º–Ω–∞—Ç
        if data.get('rooms'):
            validations.append(DataValidator.validate_rooms(data['rooms']))
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        if data.get('phones'):
            phone_validations = [DataValidator.validate_phone(phone) for phone in data['phones']]
            validations.extend(phone_validations)
        
        if not validations:
            return 0.8  # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
        return sum(validations) / len(validations)

@dataclass
class RealEstateData:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    property_type: Optional[str] = None
    property_type_confidence: Optional[float] = None
    property_origin: Optional[str] = None
    listing_type: Optional[str] = None
    listing_type_confidence: Optional[float] = None
    rooms: Optional[int] = None
    area_sqm: Optional[float] = None
    living_area: Optional[float] = None
    kitchen_area: Optional[float] = None
    land_area_sotka: Optional[float] = None
    floor: Optional[int] = None
    total_floors: Optional[int] = None
    phones: Optional[List[str]] = None
    location: Optional[Dict] = None
    heating: Optional[str] = None
    furniture: Optional[str] = None
    condition: Optional[str] = None
    amenities: Optional[Dict] = None
    extraction_quality: Optional[float] = None
    extraction_details: Optional[Dict] = None

class PropertyTypeClassifier:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self, model, model_type='e5large'):
        self.model = model
        self.model_type = model_type
        self.confidence_threshold = 0.25
        self.property_system = PropertyTypeSystem()
        self.keyword_system = KeywordSystem()
        
    def classify_property_type(self, text: str) -> Tuple[Optional[str], float]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            prop_type, confidence, details = self.property_system.classify_property_type(text)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            logger.debug(f"üè† –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: {prop_type} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            logger.debug(f"üìä –î–µ—Ç–∞–ª–∏: {details.get('reasons', [])}")
            
            return prop_type, confidence
            
        except Exception as e:
            logger.error(f"Error in property type classification: {e}")
            # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ
            return self._fallback_classification(text)
    
    def _fallback_classification(self, text: str) -> Tuple[Optional[str], float]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        try:
            if self.model_type == 'gliner':
                return self._classify_with_gliner(text)
            else:
                return self._classify_with_e5large(text)
        except Exception as e:
            logger.error(f"Fallback classification failed: {e}")
            return None, 0.0
    
    def _classify_with_gliner(self, text: str) -> Tuple[Optional[str], float]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é GLiNER"""
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è GLiNER
        labels = [
            "apartment", "–∫–≤–∞—Ä—Ç–∏—Ä–∞", "–∫–æ–º–Ω–∞—Ç–∞", "—Å—Ç—É–¥–∏—è",
            "house", "—á–∞—Å—Ç–Ω—ã–π –¥–æ–º", "–¥–æ–º", "–∫–æ—Ç—Ç–µ–¥–∂", 
            "land", "–∑–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫", "—É—á–∞—Å—Ç–æ–∫", "–∑–µ–º–ª—è",
            "office", "–æ—Ñ–∏—Å", "–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å",
            "commercial", "–º–∞–≥–∞–∑–∏–Ω", "–ø–æ–º–µ—â–µ–Ω–∏–µ"
        ]
        
        entities = self.model.predict_entities(text.lower(), labels)
        
        if not entities:
            # Fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            return self._classify_by_keywords(text)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        type_scores = {}
        for entity in entities:
            entity_text = entity['text'].lower()
            score = entity['score']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
            if any(word in entity_text for word in ['–∫–≤–∞—Ä—Ç–∏—Ä', '–∫–æ–º–Ω–∞—Ç', '—Å—Ç—É–¥–∏']):
                type_scores['–ö–≤–∞—Ä—Ç–∏—Ä–∞'] = max(type_scores.get('–ö–≤–∞—Ä—Ç–∏—Ä–∞', 0), score)
            elif any(word in entity_text for word in ['–¥–æ–º', '–∫–æ—Ç—Ç–µ–¥–∂', 'house']):
                type_scores['–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º'] = max(type_scores.get('–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º', 0), score)
            elif any(word in entity_text for word in ['—É—á–∞—Å—Ç–æ–∫', '–∑–µ–º–ª', 'land']):
                type_scores['–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫'] = max(type_scores.get('–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', 0), score)
            elif any(word in entity_text for word in ['–æ—Ñ–∏—Å', 'office']):
                type_scores['–û—Ñ–∏—Å'] = max(type_scores.get('–û—Ñ–∏—Å', 0), score)
            elif any(word in entity_text for word in ['–º–∞–≥–∞–∑–∏–Ω', '–ø–æ–º–µ—â–µ–Ω', 'commercial']):
                type_scores['–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å'] = max(type_scores.get('–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', 0), score)
        
        if type_scores:
            best_type = max(type_scores.items(), key=lambda x: x[1])
            if best_type[1] >= self.confidence_threshold:
                return best_type[0], best_type[1]
        
        # Fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        return self._classify_by_keywords(text)
    
    def _classify_with_e5large(self, text: str) -> Tuple[Optional[str], float]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é E5-Large –º–æ–¥–µ–ª–∏"""
        try:
            # –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏
            property_examples = {
                '–ö–≤–∞—Ä—Ç–∏—Ä–∞': [
                    "1-–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞ –≤ –Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–µ",
                    "–ü—Ä–æ–¥–∞–µ—Ç—Å—è —Ç—Ä–µ—Ö–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞",
                    "–°–¥–∞–µ—Ç—Å—è –æ–¥–Ω–æ–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞",
                    "–°—Ç—É–¥–∏—è —Å —Ä–µ–º–æ–Ω—Ç–æ–º"
                ],
                '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º': [
                    "–ü—Ä–æ–¥–∞–µ—Ç—Å—è —á–∞—Å—Ç–Ω—ã–π –¥–æ–º",
                    "–ö–æ—Ç—Ç–µ–¥–∂ —Å —É—á–∞—Å—Ç–∫–æ–º",
                    "–î–≤—É—Ö—ç—Ç–∞–∂–Ω—ã–π –¥–æ–º"
                ],
                '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫': [
                    "–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫ –ø–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
                    "–£—á–∞—Å—Ç–æ–∫ 10 —Å–æ—Ç–æ–∫"
                ],
                '–û—Ñ–∏—Å': [
                    "–û—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ –≤ —Ü–µ–Ω—Ç—Ä–µ",
                    "–°–¥–∞–µ—Ç—Å—è –æ—Ñ–∏—Å"
                ],
                '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å': [
                    "–¢–æ—Ä–≥–æ–≤–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ",
                    "–ú–∞–≥–∞–∑–∏–Ω –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–∂–µ"
                ]
            }
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            input_embedding = self.model.encode([text])[0]
            
            best_type = None
            best_score = 0.0
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
            for prop_type, examples in property_examples.items():
                example_embeddings = self.model.encode(examples)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
                similarities = cosine_similarity([input_embedding], example_embeddings)[0]
                max_similarity = max(similarities)
                
                if max_similarity > best_score:
                    best_score = max_similarity
                    best_type = prop_type
            
            if best_score >= self.confidence_threshold:
                return best_type, best_score
            else:
                # Fallback –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                return self._classify_by_keywords(text)
                
        except Exception as e:
            logger.error(f"Error in E5-Large classification: {e}")
            return self._classify_by_keywords(text)
    
    def _classify_by_keywords(self, text: str) -> Tuple[Optional[str], float]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∫–∞–∫ fallback"""
        text_lower = text.lower()
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        patterns = {
            '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫': [
                '–∑–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', '—É—á–∞—Å—Ç–æ–∫', '–∑–µ–º–ª—è', '—Å–æ—Ç–∫', '–≥–µ–∫—Ç–∞—Ä', 
                '–ø–æ–¥ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø–æ–¥ –∑–∞—Å—Ç—Ä–æ–π–∫—É'
            ],
            '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º': [
                '—á–∞—Å—Ç–Ω—ã–π –¥–æ–º', '–¥–æ–º', '–∫–æ—Ç—Ç–µ–¥–∂', '–æ—Å–æ–±–Ω—è–∫', '—É—Å–∞–¥—å–±–∞',
                '–¥–≤—É—Ö—ç—Ç–∞–∂–Ω—ã–π', '–æ–¥–Ω–æ—ç—Ç–∞–∂–Ω—ã–π', '–∑–∞–≥–æ—Ä–æ–¥–Ω—ã–π –¥–æ–º'
            ],
            '–û—Ñ–∏—Å': [
                '–æ—Ñ–∏—Å', '–æ—Ñ–∏—Å–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ', '–±–∏–∑–Ω–µ—Å-—Ü–µ–Ω—Ç—Ä', '–¥–µ–ª–æ–≤–æ–π —Ü–µ–Ω—Ç—Ä'
            ],
            '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å': [
                '–º–∞–≥–∞–∑–∏–Ω', '—Ç–æ—Ä–≥–æ–≤–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ', '—Å–∫–ª–∞–¥', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–º–µ—â–µ–Ω–∏–µ',
                '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', '–∫–∞—Ñ–µ', '—Å–∞–ª–æ–Ω'
            ],
            '–ö–≤–∞—Ä—Ç–∏—Ä–∞': [
                '–∫–≤–∞—Ä—Ç–∏—Ä', '–∫–æ–º–Ω–∞—Ç', '—Å—Ç—É–¥–∏', '–æ–¥–Ω–æ–∫–æ–º–Ω–∞—Ç', '–¥–≤—É—Ö–∫–æ–º–Ω–∞—Ç', 
                '—Ç—Ä–µ—Ö–∫–æ–º–Ω–∞—Ç', '—á–µ—Ç—ã—Ä–µ—Ö–∫–æ–º–Ω–∞—Ç', '–º–∞–ª–æ—Å–µ–º–µ–π–∫–∞'
            ]
        }
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–∏–ø—ã, –ø–æ—Ç–æ–º –æ–±—â–∏–µ
        for prop_type in ['–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫', '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º', '–û—Ñ–∏—Å', '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '–ö–≤–∞—Ä—Ç–∏—Ä–∞']:
            for pattern in patterns[prop_type]:
                if pattern in text_lower:
                    return prop_type, 0.8  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
        return '–ö–≤–∞—Ä—Ç–∏—Ä–∞', 0.3  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def classify_property_origin(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in [
            '–Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞', '–Ω–æ–≤–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–æ—Ç –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞', 
            '–ø–µ—Ä–≤–∏—á–Ω', '–Ω–æ–≤—ã–π –¥–æ–º', '—Å–¥–∞—á–∞ –≤'
        ]):
            return "–ù–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞"
        elif any(word in text_lower for word in [
            '–≤—Ç–æ—Ä–∏—á–∫–∞', '–≤—Ç–æ—Ä–∏—á–Ω', '–±/—É', '–±—ã–≤—à', '—Å—Ç–∞—Ä—ã–π —Ñ–æ–Ω–¥'
        ]):
            return "–í—Ç–æ—Ä–∏—á–Ω–∞—è"
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def classify_listing_type(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        text_lower = text.lower()
        
        # –£—Å–∏–ª–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∞—Ä–µ–Ω–¥—ã - –ø—Ä–æ–≤–µ—Ä—è–µ–º –ü–ï–†–í–´–ú–ò
        rental_keywords = [
            '—Å–¥–∞–µ—Ç—Å—è', '—Å–¥–∞—ë—Ç—Å—è', '—Å–¥–∞–º', '—Å–¥–∞—é', '–∞—Ä–µ–Ω–¥–∞', '—Å–Ω—è—Ç—å', '–∞—Ä–µ–Ω–¥–æ–≤–∞—Ç—å',
            '–Ω–∞–π–º', '–ø–æ—Å—É—Ç–æ—á–Ω', '–Ω–∞ –º–µ—Å—è—Ü', '–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω', '–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω'
        ]
        
        if any(word in text_lower for word in rental_keywords):
            return "–ê—Ä–µ–Ω–¥–∞"
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–æ–¥–∞–∂–∏
        sale_keywords = [
            '–ø—Ä–æ–¥–∞–µ—Ç—Å—è', '–ø—Ä–æ–¥–∞—ë—Ç—Å—è', '–ø—Ä–æ–¥–∞–º', '–ø—Ä–æ–¥–∞—é', '–Ω–∞ –ø—Ä–æ–¥–∞–∂—É', '–∫—É–ø–∏—Ç—å',
            '–ø—Ä–æ–¥–∞–∂–∞', '—Ä–µ–∞–ª–∏–∑', '–ø—Ä–æ–¥–∞–∂'
        ]
        
        if any(word in text_lower for word in sale_keywords):
            return "–ü—Ä–æ–¥–∞–∂–∞"
        
        # Fallback - –µ—Å–ª–∏ –µ—Å—Ç—å —Ü–µ–Ω–∞ –≤ –¥–æ–ª–ª–∞—Ä–∞—Ö –∏ –±–æ–ª—å—à–∞—è —Å—É–º–º–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ –ø—Ä–æ–¥–∞–∂–∞
        if re.search(r'(\$|–¥–æ–ª–ª|USD).*(\d{4,})', text_lower):
            return "–ü—Ä–æ–¥–∞–∂–∞"
        
        return "–ü—Ä–æ–¥–∞–∂–∞"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

class AreaExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –ø–ª–æ—â–∞–¥–µ–π"""
    
    def extract_areas(self, text: str) -> Dict[str, Optional[float]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –ø–ª–æ—â–∞–¥–µ–π"""
        areas = {
            'area_sqm': None,
            'living_area': None,
            'kitchen_area': None,
            'land_area_sotka': None
        }
        
        # –û–±—â–∞—è –ø–ª–æ—â–∞–¥—å - —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        total_patterns = [
            r'(?:–æ–±—â–∞—è\s+)?–ø–ª–æ—â–∞–¥—å[:\s]*(\d+(?:[.,]\d+)?)',
            r'(\d+(?:[.,]\d+)?)\s*–º¬≤?\s*(?:–æ–±—â|–∫–≤\.?–º)',
            r'(\d+(?:[.,]\d+)?)\s*–º¬≤',  # "34 –º¬≤"
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?m',  # "94 –∫–≤.–º"
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤–∞–¥—Ä–∞—Ç',  # "220 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–µ—Ç—Ä–æ–≤"
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\s*–º',  # "94 –∫–≤ –º"
            r'(\d+(?:[.,]\d+)?)\s*–º2',  # "172–º2"
            r'(\d+(?:[.,]\d+)?)\s*–º\^2',  # "172–º^2"
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?–º',  # "94 –∫–≤.–º"
            r'S[:\s]*(\d+(?:[.,]\d+)?)',
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö\s+–º–µ—Ç—Ä–æ–≤',  # "220 –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö –º–µ—Ç—Ä–æ–≤"
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?\s*–º–µ—Ç—Ä–æ–≤',  # "94 –∫–≤. –º–µ—Ç—Ä–æ–≤"
        ]
        
        for pattern in total_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                areas['area_sqm'] = self._parse_area(match.group(1))
                break
        
        # –ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å
        living_patterns = [
            r'–∂–∏–ª–∞—è\s+–ø–ª–æ—â–∞–¥—å[:\s]*(\d+(?:[.,]\d+)?)',
            r'(\d+(?:[.,]\d+)?)\s*–º¬≤?\s*–∂–∏–ª',
            r'–∂–∏–ª–∞—è[:\s]*(\d+(?:[.,]\d+)?)'
        ]
        
        for pattern in living_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                areas['living_area'] = self._parse_area(match.group(1))
                break
        
        # –ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏
        kitchen_patterns = [
            r'–∫—É—Ö–Ω—è[:\s]*(\d+(?:[.,]\d+)?)',
            r'–ø–ª–æ—â–∞–¥—å\s+–∫—É—Ö–Ω–∏[:\s]*(\d+(?:[.,]\d+)?)',
            r'–∫—É—Ö[:\s]*(\d+(?:[.,]\d+)?)\s*–º'
        ]
        
        for pattern in kitchen_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                areas['kitchen_area'] = self._parse_area(match.group(1))
                break
        
        # –ü–ª–æ—â–∞–¥—å —É—á–∞—Å—Ç–∫–∞ –≤ —Å–æ—Ç–∫–∞—Ö
        land_patterns = [
            r'—É—á–∞—Å—Ç–æ–∫[:\s]*(\d+(?:[.,]\d+)?)\s*(?:—Å–æ—Ç|—Å–æ—Ç–∫)',
            r'(\d+(?:[.,]\d+)?)\s*—Å–æ—Ç(?:–æ–∫|–∫–∞|–∫–∏)?',
            r'–∑–µ–º–µ–ª—å–Ω—ã–π\s+—É—á–∞—Å—Ç–æ–∫[:\s]*(\d+(?:[.,]\d+)?)',
            r'–ø–ª–æ—â–∞–¥—å\s+—É—á–∞—Å—Ç–∫–∞[:\s]*(\d+(?:[.,]\d+)?)',
            r'(\d+(?:[.,]\d+)?)\s*—Å–æ—Ç–∫–∏',
            r'–∑–µ–º–ª—è[:\s]*(\d+(?:[.,]\d+)?)\s*—Å–æ—Ç'
        ]
        
        for pattern in land_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                sotka_value = self._parse_land_area(match.group(1))
                if sotka_value:
                    areas['land_area_sotka'] = sotka_value
                    break
        
        return areas
    
    def _parse_area(self, area_str: str) -> Optional[float]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–æ–∫–∏ –ø–ª–æ—â–∞–¥–∏"""
        try:
            area = float(area_str.replace(',', '.'))
            return area if 10 <= area <= 10000 else None
        except (ValueError, AttributeError):
            return None
    
    def _parse_land_area(self, area_str: str) -> Optional[float]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–ª–æ—â–∞–¥–∏ —É—á–∞—Å—Ç–∫–∞ –≤ —Å–æ—Ç–∫–∞—Ö"""
        try:
            area = float(area_str.replace(',', '.'))
            return area if 0.1 <= area <= 1000 else None  # –û—Ç 0.1 –¥–æ 1000 —Å–æ—Ç–æ–∫
        except (ValueError, AttributeError):
            return None

class LocationExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.cities = [
            "–ë–∏—à–∫–µ–∫", "–¢–æ–∫–º–æ–∫", "–ö–∞—Ä–∞-–ë–∞–ª—Ç–∞", "–ö–∞–Ω—Ç", "–û—à", "–ö—ã–∑—ã–ª-–ö–∏—è", 
            "–°—É–ª—é–∫—Ç–∞", "–ë–∞—Ç–∫–µ–Ω", "–ò—Å—Ñ–∞–Ω–∞", "–î–∂–∞–ª–∞–ª-–ê–±–∞–¥", "–¢–∞—à-–ö—É–º—ã—Ä", 
            "–ö–∞—Ä–∞-–ö—É–ª—å", "–ö–∞—Ä–∞–∫–æ–ª", "–ë–∞–ª—ã–∫—á—ã", "–ß–æ–ª–ø–æ–Ω-–ê—Ç–∞", "–¢–∞–ª–∞—Å", "–ù–∞—Ä—ã–Ω"
        ]
        
        self.districts = [
            "–ê–ª–∞–º–µ–¥–∏–Ω—Å–∫–∏–π", "–°–æ–∫—É–ª—É–∫—Å–∫–∏–π", "–´—Å—ã–∫-–ê—Ç–∏–Ω—Å–∫–∏–π", "–ú–æ—Å–∫–æ–≤—Å–∫–∏–π",
            "–ñ–∞–π—ã–ª—Å–∫–∏–π", "–ü–∞–Ω—Ñ–∏–ª–æ–≤—Å–∫–∏–π", "–ö–µ–º–∏–Ω—Å–∫–∏–π", "–ß—É–π—Å–∫–∏–π",
            "–ö–∞—Ä–∞-–°—É—É–π—Å–∫–∏–π", "–ù–æ–æ–∫–∞—Ç—Å–∫–∏–π", "–ê—Ä–∞–≤–∞–Ω—Å–∫–∏–π", "–£–∑–≥–µ–Ω—Å–∫–∏–π"
        ]
    
    def extract_location(self, text: str, existing_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        location_data = {}
        current_location = existing_data.get('location', {})
        text_lower = text.lower()
        
        # –ì–æ—Ä–æ–¥ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω)
        if not current_location.get('city'):
            for city in self.cities:
                if city.lower() in text_lower:
                    if 'location' not in location_data:
                        location_data['location'] = {}
                    location_data['location']['city'] = city
                    logger.debug(f"üèôÔ∏è –ù–∞–π–¥–µ–Ω –≥–æ—Ä–æ–¥: {city}")
                    break
        
        # –†–∞–π–æ–Ω
        if not current_location.get('district'):
            for district in self.districts:
                variations = [district, district.replace("—Å–∫–∏–π", ""), district + " —Ä–∞–π–æ–Ω"]
                for variation in variations:
                    if variation.lower() in text_lower:
                        if 'location' not in location_data:
                            location_data['location'] = {}
                        location_data['location']['district'] = district
                        logger.debug(f"üèòÔ∏è –ù–∞–π–¥–µ–Ω —Ä–∞–π–æ–Ω: {district}")
                        break
                if location_data.get('location', {}).get('district'):
                    break
        
        # –ê–¥—Ä–µ—Å (—Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è)
        if not current_location.get('address'):
            address = self._extract_address(text)
            if address:
                if 'location' not in location_data:
                    location_data['location'] = {}
                location_data['location']['address'] = address
                logger.debug(f"üìç –ò–∑–≤–ª–µ—á–µ–Ω –∞–¥—Ä–µ—Å: {address}")
        
        return location_data
    
    def _extract_address(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–¥—Ä–µ—Å–∞"""
        # –£–ª–∏—Ü—ã —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ - —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        street_patterns = [
            r'—É–ª\.?\s*([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',
            r'—É–ª–∏—Ü–∞\s+([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',
            r'–ø—Ä–æ—Å–ø–µ–∫—Ç\s+([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',
            r'–ø—Ä\.?\s*([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',
            r'([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})\s*/\s*([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',  # "–ß—ã–Ω–≥—ã–∑ –ê–π—Ç–º–∞—Ç–æ–≤–∞ / –ú–∞—Å–∞–ª–∏–µ–≤–∞"
            r'—Ä—è–¥–æ–º\s+—Å\s+([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})',  # "—Ä—è–¥–æ–º —Å –§–∏–ª–∞—Ä–º–æ–Ω–∏–µ–π"
            r'([–ê-–Ø–∞-—è—ë–Å][–ê-–Ø–∞-—è—ë–Å\s\-]{3,25})\s*[0-9]+',  # "–ê—Ö—É–Ω–±–∞–µ–≤–∞ 28"
        ]
        
        # –ú–∏–∫—Ä–æ—Ä–∞–π–æ–Ω—ã
        micro_patterns = [
            r'(\d+)\s*–º–∫—Ä',
            r'–º–∫—Ä\.?\s*([–ê-–Ø–∞-—è—ë–Å0-9\s\-]{1,15})',
            r'–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω\s+([–ê-–Ø–∞-—è—ë–Å0-9\s\-]{1,15})'
        ]
        
        potential_addresses = []
        
        # –ò—â–µ–º —É–ª–∏—Ü—ã
        for pattern in street_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                street_name = match.group(1).strip()
                if len(street_name) > 3 and street_name.count(' ') <= 3:
                    potential_addresses.append(f"—É–ª. {street_name}")
        
        # –ò—â–µ–º –º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω—ã
        for pattern in micro_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                micro_name = match.group(1).strip()
                if len(micro_name) <= 15:
                    if micro_name.isdigit():
                        potential_addresses.append(f"{micro_name} –º–∫—Ä")
                    else:
                        potential_addresses.append(f"–º–∫—Ä {micro_name}")
        
        if potential_addresses:
            # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–¥—Ä–µ—Å
            best_address = potential_addresses[0]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            if (len(best_address) > 5 and len(best_address) < 50 and
                not any(word in best_address.lower() for word in ['–æ–¥–∞–µ—Ç—Å—è', '–æ–¥–∞—é', '–µ–º–∏–º—É–º'])):
                return best_address
        
        return None
    
    def extract_location_info(self, text: str, current_location: dict) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–æ–∫–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        return self.extract_location(text, current_location)

class ContactExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    
    def extract_phones(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞"""
        phones = set()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        patterns = [
            r'\+996[\s\-]?([0-9\s\-]{9,12})',  # +996 —Å –∫–æ–¥–æ–º
            r'0([0-9\s\-]{8,11})',  # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0
            r'([5-7][0-9\s\-]{8,9})',  # –ú–æ–±–∏–ª—å–Ω—ã–µ 5xx, 6xx, 7xx
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –¥–µ—Ñ–∏—Å–æ–≤
                clean_number = re.sub(r'[\s\-]', '', match)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–æ–º–µ—Ä
                if len(clean_number) == 9:  # 555123456
                    normalized = f"+996{clean_number}"
                elif len(clean_number) == 10 and clean_number.startswith('0'):  # 0555123456
                    normalized = f"+996{clean_number[1:]}"
                else:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å (9 —Ü–∏—Ñ—Ä –ø–æ—Å–ª–µ +996)
                if len(normalized) == 13 and normalized.startswith('+996'):
                    phones.add(normalized)
        
        return list(phones)
    
    def extract_phone_numbers(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–æ–≤–∞—Ä—è"""
        phones = self.extract_phones(text)
        return {'phones': phones} if phones else {}

class AmenitiesExtractor:
    """–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —É–¥–æ–±—Å—Ç–≤ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
    
    def __init__(self):
        self.heating_types = {
            "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ": ['—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ –æ—Ç–æ–ø–ª–µ–Ω–∏–µ', '—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ', '—Ü–æ'],
            "–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ": ['–∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ', '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–µ', '–∫–æ—Ç–µ–ª'],
            "–ì–∞–∑–æ–≤–æ–µ": ['–≥–∞–∑–æ–≤–æ–µ –æ—Ç–æ–ø–ª–µ–Ω–∏–µ', '–≥–∞–∑', '–≥–∞–∑–æ–≤—ã–π –∫–æ—Ç–µ–ª'],
            "–≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–æ–µ": ['—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–æ–µ', '—ç–ª–µ–∫—Ç—Ä–æ', '—ç–ª–µ–∫—Ç—Ä–æ–∫–æ—Ç–µ–ª']
        }
        
        self.furniture_types = {
            "–° –º–µ–±–µ–ª—å—é": ['—Å –º–µ–±–µ–ª—å—é', '–º–µ–±–ª–∏—Ä–æ–≤–∞–Ω', '–æ–±—Å—Ç–∞–≤–ª–µ–Ω', '–º–µ–±–µ–ª—å –µ—Å—Ç—å'],
            "–ë–µ–∑ –º–µ–±–µ–ª–∏": ['–±–µ–∑ –º–µ–±–µ–ª–∏', '–Ω–µ –º–µ–±–ª–∏—Ä–æ–≤–∞–Ω', '–ø—É—Å—Ç–∞—è'],
            "–ß–∞—Å—Ç–∏—á–Ω–æ –º–µ–±–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è": ['—á–∞—Å—Ç–∏—á–Ω–æ', '–Ω–µ–º–Ω–æ–≥–æ –º–µ–±–µ–ª–∏', '–∫–æ–µ-—á—Ç–æ –µ—Å—Ç—å']
        }
        
        self.condition_types = {
            "–ï–≤—Ä–æ—Ä–µ–º–æ–Ω—Ç": ['–µ–≤—Ä–æ—Ä–µ–º–æ–Ω—Ç', '–¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∏–π', '—ç–ª–∏—Ç–Ω—ã–π —Ä–µ–º–æ–Ω—Ç', '–ø—Ä–µ–º–∏—É–º'],
            "–•–æ—Ä–æ—à–µ–µ": ['—Ö–æ—Ä–æ—à–∏–π —Ä–µ–º–æ–Ω—Ç', '–æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–º–æ–Ω—Ç', '—Ö–æ—Ä–æ—à–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ'],
            "–°—Ä–µ–¥–Ω–µ–µ": ['—Å—Ä–µ–¥–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ', '–∂–∏–ª–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ', '–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ'],
            "–¢—Ä–µ–±—É–µ—Ç —Ä–µ–º–æ–Ω—Ç–∞": ['—Ç—Ä–µ–±—É–µ—Ç —Ä–µ–º–æ–Ω—Ç', '–ø–æ–¥ —Ä–µ–º–æ–Ω—Ç', '—á–µ—Ä–Ω–æ–≤–∞—è']
        }
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —É–¥–æ–±—Å—Ç–≤–∞
        self.amenities_categories = {
            'bathroom': {
                'separate': ['—Ä–∞–∑–¥–µ–ª—å–Ω—ã–π —Å–∞–Ω—É–∑–µ–ª', '—Ä–∞–∑–¥–µ–ª—å–Ω—ã–π —Å/—É', '—Ç—É–∞–ª–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ'],
                'combined': ['—Å–æ–≤–º–µ—â–µ–Ω–Ω—ã–π —Å–∞–Ω—É–∑–µ–ª', '—Å–æ–≤–º–µ—â–µ–Ω–Ω—ã–π —Å/—É', '—Å–∞–Ω—É–∑–µ–ª —Å–æ–≤–º–µ—â–µ–Ω'],
                'multiple': ['–¥–≤–∞ —Å–∞–Ω—É–∑–ª–∞', '2 —Å–∞–Ω—É–∑–ª–∞', '–Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∞–Ω—É–∑–ª–æ–≤']
            },
            'balcony': {
                'balcony': ['–±–∞–ª–∫–æ–Ω'],
                'loggia': ['–ª–æ–¥–∂–∏—è'],
                'multiple': ['–¥–≤–∞ –±–∞–ª–∫–æ–Ω–∞', '2 –±–∞–ª–∫–æ–Ω–∞', '–±–∞–ª–∫–æ–Ω –∏ –ª–æ–¥–∂–∏—è']
            },
            'parking': {
                'garage': ['–≥–∞—Ä–∞–∂', '—Å –≥–∞—Ä–∞–∂–æ–º'],
                'parking_space': ['–ø–∞—Ä–∫–æ–≤–æ—á–Ω–æ–µ –º–µ—Å—Ç–æ', '–ø–∞—Ä–∫–∏–Ω–≥', '—Å—Ç–æ—è–Ω–∫–∞'],
                'yard': ['–≤–æ –¥–≤–æ—Ä–µ', '–ø–∞—Ä–∫–æ–≤–∫–∞ –≤–æ –¥–≤–æ—Ä–µ']
            },
            'security': {
                'intercom': ['–¥–æ–º–æ—Ñ–æ–Ω'],
                'security': ['–æ—Ö—Ä–∞–Ω–∞', '–∫–æ–Ω—Å—å–µ—Ä–∂'],
                'video': ['–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ', '–∫–∞–º–µ—Ä—ã']
            },
            'internet': {
                'internet': ['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', 'wi-fi', 'wifi', '–≤–∞–π-—Ñ–∞–π'],
                'cable': ['–∫–∞–±–µ–ª—å–Ω–æ–µ —Ç–≤', '—Ç–µ–ª–µ–≤–∏–¥–µ–Ω–∏–µ']
            },
            'appliances': {
                'kitchen': ['–∫—É—Ö–æ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞', '–ø–ª–∏—Ç–∞', '—Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫', '–¥—É—Ö–æ–≤–∫–∞'],
                'washing': ['—Å—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞', '—Å—Ç–∏—Ä–∞–ª–∫–∞'],
                'dishwasher': ['–ø–æ—Å—É–¥–æ–º–æ–π–∫–∞', '–ø–æ—Å—É–¥–æ–º–æ–µ—á–Ω–∞—è']
            }
        }
    
    def extract_amenities(self, text: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —É–¥–æ–±—Å—Ç–≤–∞ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"""
        result = {}
        text_lower = text.lower()
        
        # –û—Ç–æ–ø–ª–µ–Ω–∏–µ
        result['heating'] = self._classify_by_keywords(text_lower, self.heating_types)
        
        # –ú–µ–±–µ–ª—å
        result['furniture'] = self._classify_by_keywords(text_lower, self.furniture_types)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        result['condition'] = self._classify_by_keywords(text_lower, self.condition_types)
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —É–¥–æ–±—Å—Ç–≤–∞
        amenities = {}
        for category, subcategories in self.amenities_categories.items():
            for amenity_type, keywords in subcategories.items():
                if any(keyword in text_lower for keyword in keywords):
                    if category not in amenities:
                        amenities[category] = []
                    amenities[category].append(amenity_type)
        
        if amenities:
            result['amenities'] = amenities
        
        return result
    
    def _classify_by_keywords(self, text: str, categories: Dict) -> Optional[str]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        return None
    
    def _classify_amenity_type(self, text: str) -> Optional[str]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —É–¥–æ–±—Å—Ç–≤–∞ –ø–æ —Ç–µ–∫—Å—Ç—É"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —É–¥–æ–±—Å—Ç–≤
        for category, subcategories in self.amenities_categories.items():
            for amenity_type, keywords in subcategories.items():
                if any(keyword in text_lower for keyword in keywords):
                    return category
                    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        all_categories = {**self.heating_types, **self.furniture_types, **self.condition_types}
        for amenity_type, keywords in all_categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return amenity_type.lower().replace(' ', '_')
                
        return None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫—ç—à–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
_gliner_model = None
_e5_model = None
_extractor_instance = None

def get_cached_gliner_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å GLiNER"""
    global _gliner_model
    if _gliner_model is None:
        try:
            logger.info("Loading GLiNER model for detailed data extraction...")
            from gliner import GLiNER
            _gliner_model = GLiNER.from_pretrained("urchade/gliner_medium-v2.1")
            logger.info("‚úÖ GLiNER model loaded successfully")
        except Exception as e:
            logger.warning(f"GLiNER not available: {e}")
            _gliner_model = None
    return _gliner_model

def get_cached_e5_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å E5-Large"""
    global _e5_model
    if _e5_model is None:
        try:
            logger.info("Loading E5-Large model for property type classification...")
            from sentence_transformers import SentenceTransformer
            _e5_model = SentenceTransformer('intfloat/multilingual-e5-large')
            logger.info("‚úÖ E5-Large model loaded successfully")
        except Exception as e:
            logger.warning(f"E5-Large not available: {e}")
            _e5_model = None
    return _e5_model

def get_cached_extractor():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    global _extractor_instance
    if _extractor_instance is None:
        _extractor_instance = RealEstateDataExtractor()
    return _extractor_instance

class RealEstateDataExtractor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
        self.gliner_model = None
        self.e5_model = None
        self.gliner_available = False
        self.e5_available = False
        
        self._initialize_models()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
        self.keyword_system = KeywordSystem()
        self.property_system = PropertyTypeSystem()
        self.data_validator = DataValidator()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏ —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏
        # PropertyTypeClassifier –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç E5-Large –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
        self.property_classifier = PropertyTypeClassifier(
            self.e5_model if self.e5_available else self.gliner_model, 
            'e5large' if self.e5_available else 'gliner'
        )
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
        self.area_extractor = AreaExtractor()
        self.location_extractor = LocationExtractor()
        self.contact_extractor = ContactExtractor()
        self.amenities_extractor = AmenitiesExtractor()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        system_type = "GLiNER + E5-Large + Enhanced Keywords" if self.gliner_available and self.e5_available else \
                     "GLiNER + Enhanced Keywords" if self.gliner_available else "E5-Large + Enhanced Keywords"
        
        logger.info(f"üöÄ RealEstateDataExtractor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —Å–∏—Å—Ç–µ–º–æ–π: {system_type}")
        logger.info(f"üìä –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—Ä–æ–¥–∞–∂–∏: {len(self.keyword_system.SALE_CRITICAL + self.keyword_system.SALE_STRONG)}")
        logger.info(f"üìä –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∞—Ä–µ–Ω–¥—ã: {len(self.keyword_system.RENTAL_CRITICAL + self.keyword_system.RENTAL_STRONG)}")
        logger.info(f"üìä –¢–∏–ø–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: {len(self.property_system.PROPERTY_TYPES)}")
        logger.info(f"üìä –ö—ã—Ä–≥—ã–∑—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(self.keyword_system.KYRGYZ_TERMS)}")
        
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.gliner_model = get_cached_gliner_model()
        self.gliner_available = self.gliner_model is not None
        
        self.e5_model = get_cached_e5_model()
        self.e5_available = self.e5_model is not None
            
        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
        if not self.gliner_available and not self.e5_available:
            raise RuntimeError("Neither GLiNER nor E5-Large models could be loaded")
            
    def extract_comprehensive_data(self, text: str, item_data: dict = None) -> dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π (–±–µ–∑ —Ç–∏–ø–∞, —Å–¥–µ–ª–∫–∏ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤)"""
        try:
            result = {'original_text': text}  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            extraction_start = datetime.now()
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")

            # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            if self.area_extractor:
                areas = self.area_extractor.extract_areas(text)
                result.update(areas)
                if areas.get('area_sqm'):
                    logger.debug(f"üìê –ü–ª–æ—â–∞–¥—å: {areas['area_sqm']} –º¬≤")
                if areas.get('land_area_sotka'):
                    logger.debug(f"üåø –£—á–∞—Å—Ç–æ–∫: {areas['land_area_sotka']} —Å–æ—Ç.")

            # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–Ω–∞—Ç –∏ —ç—Ç–∞–∂–µ–π —Å GLiNER
            rooms_floors = self._extract_rooms_floors_with_gliner(text)
            result.update(rooms_floors)
            if rooms_floors.get('rooms'):
                logger.debug(f"üè† –ö–æ–º–Ω–∞—Ç: {rooms_floors['rooms']}")
            if rooms_floors.get('floor'):
                floor_info = f"–≠—Ç–∞–∂: {rooms_floors['floor']}"
                if rooms_floors.get('total_floors'):
                    floor_info += f"/{rooms_floors['total_floors']}"
                logger.debug(f"üè¢ {floor_info}")

            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–¥–æ–±—Å—Ç–≤ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            if self.amenities_extractor:
                amenities = self._extract_amenities_with_gliner(text)
                result.update(amenities)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                if 'amenities' in amenities:
                    amenities_dict = amenities['amenities']
                    if 'heating' in amenities_dict and amenities_dict['heating']:
                        result['heating'] = amenities_dict['heating']
                        logger.debug(f"üî• –û—Ç–æ–ø–ª–µ–Ω–∏–µ: {amenities_dict['heating']}")
                    if 'furniture' in amenities_dict and amenities_dict['furniture']:
                        result['furniture'] = amenities_dict['furniture']
                        logger.debug(f"ü™ë –ú–µ–±–µ–ª—å: {amenities_dict['furniture']}")
                    if 'condition' in amenities_dict and amenities_dict['condition']:
                        result['condition'] = amenities_dict['condition']
                        logger.debug(f"üîß –°–æ—Å—Ç–æ—è–Ω–∏–µ: {amenities_dict['condition']}")

            # 4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏
            if self.location_extractor and item_data:
                current_location = item_data.get('location', {})
                location_updates = self._extract_location_with_gliner(text, current_location)
                if location_updates:
                    result['location'] = location_updates
                    logger.debug(f"üìç –õ–æ–∫–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {location_updates}")

            # 5. –í–ê–õ–ò–î–ê–¶–ò–Ø –ò –ö–ê–ß–ï–°–¢–í–û
            self._validate_and_fix_data(result)
            # 6. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            quality_score = DataValidator.calculate_quality_score(result)
            result['extraction_quality'] = quality_score
            # 7. –î–µ—Ç–∞–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            extraction_time = (datetime.now() - extraction_start).total_seconds()
            result['extraction_details'] = {
                'extraction_time_seconds': extraction_time,
                'text_length': len(text),
                'quality_score': quality_score,
                'timestamp': datetime.now().isoformat()
            }
            # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            extracted_fields = [k for k, v in result.items() if v is not None and k not in ['extraction_quality', 'extraction_details', 'original_text']]
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {extraction_time:.2f}—Å. –ö–∞—á–µ—Å—Ç–≤–æ: {quality_score:.2f}. –ü–æ–ª–µ–π: {len(extracted_fields)}")
            # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
            result.pop('original_text', None)
            return result
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return {'extraction_quality': 0.0, 'extraction_details': {'error': str(e)}}

    def extract_and_classify(self, title: str, description: str, existing_data: Dict) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –≤—ã–∑—ã–≤–∞–µ—Ç extract_comprehensive_data"""
        text = f"{title or ''} {description or ''}".strip()
        return self.extract_comprehensive_data(text, existing_data)

    def _extract_contacts_with_gliner(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é GLiNER"""
        if not self.gliner_available:
            return {}
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º GLiNER –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            entities = self.gliner_model.predict_entities(
                text, 
                ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞", "–∫–æ–Ω—Ç–∞–∫—Ç", "phone", "–º–æ–±–∏–ª—å–Ω—ã–π"]
            )
            
            contacts = {}
            for entity in entities:
                if entity['label'] in ['—Ç–µ–ª–µ—Ñ–æ–Ω', '–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞', '–∫–æ–Ω—Ç–∞–∫—Ç', 'phone', '–º–æ–±–∏–ª—å–Ω—ã–π']:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
                    phone = self.contact_extractor.extract_phone_numbers(entity['text'])
                    if phone:
                        contacts.update(phone)
                        
            return contacts
            
        except Exception as e:
            logger.error(f"Error extracting contacts with GLiNER: {e}")
            return {}
            
    def _extract_amenities_with_gliner(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–¥–æ–±—Å—Ç–≤ —Å –ø–æ–º–æ—â—å—é GLiNER"""
        if not self.gliner_available:
            return self.amenities_extractor.extract_amenities(text)
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º GLiNER –¥–ª—è –ø–æ–∏—Å–∫–∞ —É–¥–æ–±—Å—Ç–≤
            amenity_labels = [
                "–æ—Ç–æ–ø–ª–µ–Ω–∏–µ", "–º–µ–±–µ–ª—å", "—Å–æ—Å—Ç–æ—è–Ω–∏–µ", "—Ä–µ–º–æ–Ω—Ç", "–±–∞–ª–∫–æ–Ω", "–ª–æ–¥–∂–∏—è",
                "–ø–∞—Ä–∫–æ–≤–∫–∞", "–≥–∞—Ä–∞–∂", "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", "—Ç–µ—Ö–Ω–∏–∫–∞"
            ]
            
            entities = self.gliner_model.predict_entities(text, amenity_labels)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º GLiNER —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å regex –ø–æ–¥—Ö–æ–¥–æ–º
            gliner_amenities = {}
            for entity in entities:
                amenity_type = self.amenities_extractor._classify_amenity_type(entity['text'])
                if amenity_type:
                    if amenity_type not in gliner_amenities:
                        gliner_amenities[amenity_type] = []
                    gliner_amenities[amenity_type].append(entity['text'])
                    
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å regex —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            regex_amenities = self.amenities_extractor.extract_amenities(text)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_amenities = regex_amenities.copy()
            for amenity_type, values in gliner_amenities.items():
                if amenity_type in combined_amenities:
                    combined_amenities[amenity_type].extend(values)
                    combined_amenities[amenity_type] = list(set(combined_amenities[amenity_type]))
                else:
                    combined_amenities[amenity_type] = values
                    
            return {'amenities': combined_amenities} if combined_amenities else {}
            
        except Exception as e:
            logger.error(f"Error extracting amenities with GLiNER: {e}")
            return self.amenities_extractor.extract_amenities(text)
            
    def _extract_location_with_gliner(self, text: str, current_location: dict) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é GLiNER"""
        if not self.gliner_available:
            return self.location_extractor.extract_location_info(text, current_location)
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º GLiNER –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
            location_labels = [
                "–≥–æ—Ä–æ–¥", "—Ä–∞–π–æ–Ω", "–º–∏–∫—Ä–æ—Ä–∞–π–æ–Ω", "—É–ª–∏—Ü–∞", "–∞–¥—Ä–µ—Å", 
                "–ª–æ–∫–∞—Ü–∏—è", "–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ", "–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è"
            ]
            
            entities = self.gliner_model.predict_entities(text, location_labels)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å regex –ø–æ–¥—Ö–æ–¥–æ–º
            gliner_location = current_location.copy()
            
            for entity in entities:
                location_text = entity['text']
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–≥–∏–∫—É –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                location_update = self.location_extractor.extract_location_info(location_text, gliner_location)
                if location_update:
                    gliner_location.update(location_update)
                    
            return gliner_location if gliner_location != current_location else {}
            
        except Exception as e:
            logger.error(f"Error extracting location with GLiNER: {e}")
            return self.location_extractor.extract_location_info(text, current_location)
            
    def _extract_rooms_floors_with_gliner(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–Ω–∞—Ç –∏ —ç—Ç–∞–∂–µ–π —Å –ø–æ–º–æ—â—å—é GLiNER"""
        result = {}
        
        try:
            if self.gliner_available:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º GLiNER –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–º–Ω–∞—Ç –∏ —ç—Ç–∞–∂–µ–π
                entities = self.gliner_model.predict_entities(
                    text, 
                    ["–∫–æ–º–Ω–∞—Ç—ã", "—ç—Ç–∞–∂", "–∫–æ–º–Ω–∞—Ç–∞", "—ç—Ç–∞–∂–Ω–æ—Å—Ç—å", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç"]
                )
                
                for entity in entities:
                    entity_text = entity['text'].lower()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
                    rooms_match = re.search(r'(\d+)\s*–∫–æ–º–Ω', entity_text)
                    if rooms_match:
                        result['rooms'] = int(rooms_match.group(1))
                        
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç—Ç–∞–∂
                    floor_match = re.search(r'(\d+)\s*—ç—Ç–∞–∂', entity_text)
                    if floor_match:
                        result['floor'] = int(floor_match.group(1))
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ regex –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
            # –ö–æ–º–Ω–∞—Ç—ã
            if 'rooms' not in result:
                rooms_patterns = [
                    r'(\d+)\s*–∫–æ–º–Ω(?:–∞—Ç)?',
                    r'(\d+)-?–∫–æ–º–Ω–∞—Ç–Ω',
                    r'(\d+)\s*–∫(?:–æ–º–Ω)?\.?'
                ]
                
                for pattern in rooms_patterns:
                    match = re.search(pattern, text.lower())
                    if match:
                        rooms = int(match.group(1))
                        if 1 <= rooms <= 10:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
                            result['rooms'] = rooms
                            break
            
            # –≠—Ç–∞–∂–∏
            if 'floor' not in result:
                floor_patterns = [
                    r'(\d+)\s*—ç—Ç–∞–∂',
                    r'(\d+)/(\d+)',  # —ç—Ç–∞–∂/–≤—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π
                    r'—ç—Ç–∞–∂\s*(\d+)'
                ]
                
                for pattern in floor_patterns:
                    match = re.search(pattern, text.lower())
                    if match:
                        if '/' in pattern:  # –§–æ—Ä–º–∞—Ç —ç—Ç–∞–∂/–≤—Å–µ–≥–æ
                            floor = int(match.group(1))
                            total_floors = int(match.group(2))
                            if 1 <= floor <= total_floors <= 50:
                                result['floor'] = floor
                                result['total_floors'] = total_floors
                        else:
                            floor = int(match.group(1))
                            if 1 <= floor <= 50:
                                result['floor'] = floor
                        break
                        
            return result
            
        except Exception as e:
            logger.error(f"Error extracting rooms/floors with GLiNER: {e}")
            return {}
            
    def _classify_property_origin(self, text: str) -> Optional[str]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è E5-Large –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)"""
        if not self.e5_available:
            return None
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º E5-Large –¥–ª—è —Ç–æ—á–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            patterns = {
                '–ù–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞': ['–Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞', '–æ—Ç –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞', '–ø–µ—Ä–≤–∞—è —Å–¥–∞—á–∞', '—Å–¥–∞—á–∞ –æ–±—ä–µ–∫—Ç–∞', '–øso'],
                '–í—Ç–æ—Ä–∏—á–Ω–∞—è': ['–≤—Ç–æ—Ä–∏—á–Ω–∞—è', '–≤—Ç–æ—Ä–∏—á–Ω—ã–π —Ä—ã–Ω–æ–∫', '–±/—É', '–∂–∏–ª–∞—è', '–æ–±–∂–∏—Ç–∞—è']
            }
            
            text_lower = text.lower()
            for origin, keywords in patterns.items():
                if any(keyword in text_lower for keyword in keywords):
                    return origin
                    
            return None
            
        except Exception as e:
            logger.error(f"Error classifying property origin: {e}")
            return None
    
    def _classify_listing_type(self, text: str) -> Tuple[str, float, Dict]:
        """–¢–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Å–¥–µ–ª–∫–∏ —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            listing_type, confidence, details = self.keyword_system.calculate_listing_type_score(text)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞  
            logger.debug(f"üìã –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {listing_type} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            logger.debug(f"üìä –ü—Ä–∏—á–∏–Ω—ã: {details.get('reasons', [])[:3]}")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏—á–∏–Ω—ã
            
            return listing_type, confidence, details
            
        except Exception as e:
            logger.error(f"Error classifying listing type: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
            return self._fallback_listing_type(text), 0.3, {'reasons': ['Fallback –∞–Ω–∞–ª–∏–∑']}
    
    def _fallback_listing_type(self, text: str) -> str:
        """Fallback –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        text_lower = text.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if any(word in text_lower for word in ['—Å–¥–∞—é', '—Å–¥–∞–µ—Ç—Å—è', '–∞—Ä–µ–Ω–¥–∞', '—Å–Ω—è—Ç—å']):
            return "–ê—Ä–µ–Ω–¥–∞"
        elif any(word in text_lower for word in ['–ø—Ä–æ–¥–∞—é', '–ø—Ä–æ–¥–∞–µ—Ç—Å—è', '–∫—É–ø–∏—Ç—å', '–ø—Ä–æ–¥–∞–∂–∞']):
            return "–ü—Ä–æ–¥–∞–∂–∞"
        else:
            return "–ü—Ä–æ–¥–∞–∂–∞"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _validate_and_fix_data(self, result: Dict) -> None:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        issues_fixed = []
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ vs –¥–∞–Ω–Ω—ã—Ö
        original_text = result.get('original_text', '')
        if original_text:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_match = re.search(r'(\d+)\s*-?\s*–∫–æ–º–Ω', original_text.lower())
            if title_match:
                title_rooms = int(title_match.group(1))
                extracted_rooms = result.get('rooms')
                if extracted_rooms and extracted_rooms != title_rooms:
                    logger.warning(f"‚ùå –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –≤ –∫–æ–º–Ω–∞—Ç–∞—Ö: –∑–∞–≥–æ–ª–æ–≤–æ–∫={title_rooms}, –∏–∑–≤–ª–µ—á–µ–Ω–æ={extracted_rooms}")
                    result['rooms'] = title_rooms  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫—É
                    issues_fixed.append(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç: {extracted_rooms} ‚Üí {title_rooms}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if re.search(r'\d+\s*-?\s*–∫–æ–º–Ω\.\s*–∫–≤\.', original_text.lower()):
                if result.get('property_type') != '–ö–≤–∞—Ä—Ç–∏—Ä–∞':
                    old_type = result.get('property_type', 'None')
                    result['property_type'] = '–ö–≤–∞—Ä—Ç–∏—Ä–∞'
                    result['property_type_confidence'] = 0.95
                    logger.warning(f"‚ùå –ò—Å–ø—Ä–∞–≤–ª–µ–Ω —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: {old_type} ‚Üí –ö–≤–∞—Ä—Ç–∏—Ä–∞")
                    issues_fixed.append(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω —Ç–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: {old_type} ‚Üí –ö–≤–∞—Ä—Ç–∏—Ä–∞")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏
        if result.get('area_sqm'):
            if not self.data_validator.validate_area(result['area_sqm']):
                logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ø–ª–æ—â–∞–¥—å: {result['area_sqm']} –º¬≤")
                if result['area_sqm'] < 5:
                    result['area_sqm'] = None
                    issues_fixed.append("–£–¥–∞–ª–µ–Ω–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –ø–ª–æ—â–∞–¥—å")
                elif result['area_sqm'] > 10000:
                    result['area_sqm'] = None
                    issues_fixed.append("–£–¥–∞–ª–µ–Ω–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –ø–ª–æ—â–∞–¥—å")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —ç—Ç–∞–∂–µ–π
        if result.get('floor') and result.get('total_floors'):
            if result['floor'] > result['total_floors']:
                logger.warning(f"‚ö†Ô∏è –≠—Ç–∞–∂ {result['floor']} –±–æ–ª—å—à–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ {result['total_floors']}")
                # –ú–µ–Ω—è–µ–º –º–µ—Å—Ç–∞–º–∏ –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
                if result['total_floors'] <= 50:
                    result['floor'], result['total_floors'] = result['total_floors'], result['floor']
                    issues_fixed.append("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–æ—Ä—è–¥–æ–∫ —ç—Ç–∞–∂–µ–π")
                else:
                    result['total_floors'] = None
                    issues_fixed.append("–£–¥–∞–ª–µ–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–∂–µ–π")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
        if result.get('rooms'):
            if not self.data_validator.validate_rooms(result['rooms']):
                logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç: {result['rooms']}")
                if result['rooms'] < 1 or result['rooms'] > 20:
                    result['rooms'] = None
                    issues_fixed.append("–£–¥–∞–ª–µ–Ω–æ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        if result.get('phones'):
            valid_phones = []
            for phone in result['phones']:
                if self.data_validator.validate_phone(phone):
                    valid_phones.append(phone)
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω: {phone}")
                    issues_fixed.append(f"–£–¥–∞–ª–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω: {phone}")
            
            result['phones'] = valid_phones if valid_phones else None
        
        # –õ–æ–≥–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        self._validate_property_logic(result, issues_fixed)
        
        # –õ–æ–≥ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        if issues_fixed:
            logger.info(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(issues_fixed)}")
            for issue in issues_fixed[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                logger.debug(f"   ‚Ä¢ {issue}")
    
    def _validate_property_logic(self, result: Dict, issues_fixed: List[str]) -> None:
        """–õ–æ–≥–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏"""
        
        # –ü—Ä–∞–≤–∏–ª–æ: —É—á–∞—Å—Ç–æ–∫ –Ω–µ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —ç—Ç–∞–∂–∏
        if result.get('property_type') == '–ó–µ–º–µ–ª—å–Ω—ã–π —É—á–∞—Å—Ç–æ–∫':
            if result.get('floor') or result.get('total_floors'):
                result['floor'] = None
                result['total_floors'] = None
                issues_fixed.append("–£–¥–∞–ª–µ–Ω—ã —ç—Ç–∞–∂–∏ –¥–ª—è –∑–µ–º–µ–ª—å–Ω–æ–≥–æ —É—á–∞—Å—Ç–∫–∞")
            
            if result.get('rooms'):
                result['rooms'] = None
                issues_fixed.append("–£–¥–∞–ª–µ–Ω—ã –∫–æ–º–Ω–∞—Ç—ã –¥–ª—è –∑–µ–º–µ–ª—å–Ω–æ–≥–æ —É—á–∞—Å—Ç–∫–∞")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –∫–≤–∞—Ä—Ç–∏—Ä–∞ –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç —ç—Ç–∞–∂
        if result.get('property_type') == '–ö–≤–∞—Ä—Ç–∏—Ä–∞':
            if result.get('total_floors') and not result.get('floor'):
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–∂–µ–π, –Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω —ç—Ç–∞–∂ –∫–≤–∞—Ä—Ç–∏—Ä—ã
                logger.debug("üè¢ –ö–≤–∞—Ä—Ç–∏—Ä–∞ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —ç—Ç–∞–∂–∞ –ø—Ä–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ–π —ç—Ç–∞–∂–Ω–æ—Å—Ç–∏ –∑–¥–∞–Ω–∏—è")
        
        # –ü—Ä–∞–≤–∏–ª–æ: –¥–æ–º –æ–±—ã—á–Ω–æ –Ω–µ –∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–π —ç—Ç–∞–∂–Ω–æ—Å—Ç–∏
        if result.get('property_type') == '–ß–∞—Å—Ç–Ω—ã–π –¥–æ–º':
            if result.get('total_floors') and result['total_floors'] > 5:
                logger.warning(f"‚ö†Ô∏è –ß–∞—Å—Ç–Ω—ã–π –¥–æ–º —Å {result['total_floors']} —ç—Ç–∞–∂–∞–º–∏ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å")
        
        # –ü—Ä–∞–≤–∏–ª–æ: —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–ª–æ—â–∞–¥–µ–π
        if result.get('area_sqm') and result.get('living_area'):
            if result['living_area'] > result['area_sqm']:
                logger.warning(f"‚ö†Ô∏è –ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å ({result['living_area']}) –±–æ–ª—å—à–µ –æ–±—â–µ–π ({result['area_sqm']})")
                # –ú–µ–Ω—è–µ–º –º–µ—Å—Ç–∞–º–∏ –µ—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –Ω–µ–±–æ–ª—å—à–∞—è
                if result['living_area'] - result['area_sqm'] < 20:
                    result['area_sqm'], result['living_area'] = result['living_area'], result['area_sqm']
                    issues_fixed.append("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω –ø–æ—Ä—è–¥–æ–∫ –ø–ª–æ—â–∞–¥–µ–π")
                else:
                    result['living_area'] = None
                    issues_fixed.append("–£–¥–∞–ª–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∂–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å")

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class AIDataExtractor(RealEstateDataExtractor):
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º"""
    
    def extract_and_classify(self, title: str, description: str, existing_data: Dict) -> Dict:
        """–ú–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        # –ü–µ—Ä–µ–¥–∞–µ–º existing_data –∫–∞–∫ item_data –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–∞–Ω–Ω—ã–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–æ–≤
        return self.extract_comprehensive_data(f"{title} {description}".strip(), item_data=existing_data) 