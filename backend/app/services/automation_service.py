import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from sqlalchemy.orm import Session
import aiohttp
from enum import Enum
from dotenv import load_dotenv
from app.core.config import (
    get_auto_mode, get_pipeline_interval_minutes, get_run_immediately_on_start,
    get_scraping_sources, get_enable_scraping, get_enable_photo_processing,
    get_enable_duplicate_processing, get_enable_realtor_detection,
    get_enable_elasticsearch_reindex
)
from app.services.scrapy_manager import ScrapyManager
from app.services.event_emitter import event_emitter
from app.services.websocket_manager import websocket_manager

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º event_emitter –¥–ª—è WebSocket —Å–æ–±—ã—Ç–∏–π
from app.services.event_emitter import event_emitter
from app.services.scrapy_manager import scrapy_manager
from app.services.websocket_manager import websocket_manager

class PipelineStatus(Enum):
    IDLE = "idle"
    RUNNING = "running" 
    COMPLETED = "completed"
    ERROR = "error"
    PAUSED = "paused"

class PipelineStage(Enum):
    SCRAPING = "scraping"
    PHOTO_PROCESSING = "photo_processing"
    DUPLICATE_PROCESSING = "duplicate_processing"
    REALTOR_DETECTION = "realtor_detection"
    ELASTICSEARCH_REINDEX = "elasticsearch_reindex"

def to_iso(dt):
    if isinstance(dt, str):
        return dt
    if dt is not None:
        return dt.isoformat()
    return None

class AutomationService:
    """–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.session: Optional[aiohttp.ClientSession] = None
        self.pipeline_status = PipelineStatus.IDLE
        self.current_stage: Optional[PipelineStage] = None
        self.last_run_start: Optional[datetime] = None
        self.last_run_end: Optional[datetime] = None
        self.next_run_scheduled: Optional[datetime] = None
        # is_auto_mode –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç RUN_IMMEDIATELY_ON_START)
        # RUN_IMMEDIATELY_ON_START –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞
        self.is_auto_mode = get_auto_mode()  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –ë–î
        self.scrapy_manager = scrapy_manager
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –∏–∑ –ë–î
        self.interval_minutes = get_pipeline_interval_minutes()
        self.interval_hours = self.interval_minutes / 60.0
        
        self.stage_details = {
            PipelineStage.SCRAPING: {
                "name": "–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤",
                "status": "idle",
                "started_at": None,
                "completed_at": None,
                "error": None,
                "progress": {
                    "total": 0, 
                    "completed": 0, 
                    "failed": 0,
                    "new_ads": 0,
                    "processed_ads": 0,
                    "sources_active": 0,
                    "sources_completed": 0
                }
            },
            PipelineStage.PHOTO_PROCESSING: {
                "name": "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π", 
                "status": "idle",
                "started_at": None,
                "completed_at": None,
                "error": None,
                "progress": {
                    "processed": 0, 
                    "total": 0, 
                    "photos_downloaded": 0,
                    "photos_optimized": 0
                }
            },
            PipelineStage.DUPLICATE_PROCESSING: {
                "name": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤",
                "status": "idle", 
                "started_at": None,
                "completed_at": None,
                "error": None,
                "progress": {
                    "processed": 0, 
                    "remaining": 0, 
                    "duplicates_found": 0,
                    "groups_created": 0
                }
            },
            PipelineStage.REALTOR_DETECTION: {
                "name": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤",
                "status": "idle",
                "started_at": None, 
                "completed_at": None,
                "error": None,
                "progress": {
                    "detected": 0, 
                    "processed": 0,
                    "total": 0
                }
            },
            PipelineStage.ELASTICSEARCH_REINDEX: {
                "name": "–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞",
                "status": "idle",
                "started_at": None,
                "completed_at": None, 
                "error": None,
                "progress": {
                    "indexed": 0, 
                    "total": 0
                }
            }
        }
        
        self.enabled_stages = {
            PipelineStage.SCRAPING: get_enable_scraping(),
            PipelineStage.PHOTO_PROCESSING: get_enable_photo_processing(), 
            PipelineStage.DUPLICATE_PROCESSING: get_enable_duplicate_processing(),
            PipelineStage.REALTOR_DETECTION: get_enable_realtor_detection(),
            PipelineStage.ELASTICSEARCH_REINDEX: get_enable_elasticsearch_reindex()
        }
        
        self.scraping_sources = get_scraping_sources()
        self._background_task: Optional[asyncio.Task] = None
        self.initial_stats = None
        self._last_new_ads = 0
        self._last_processed_ads = 0
        self._last_duplicates_found = 0
        self._last_realtors_found = 0
        
    async def start_service(self):
        """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        if not self._background_task:
            self._background_task = asyncio.create_task(self._background_scheduler())
            
        if self.is_auto_mode and self.pipeline_status == PipelineStatus.IDLE:
            run_immediately = get_run_immediately_on_start()
            if run_immediately:
                logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞ (RUN_IMMEDIATELY_ON_START=true)")
                asyncio.create_task(self._delayed_start())
            
        logger.info("üöÄ –°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—É—â–µ–Ω")
    
    async def stop_service(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
        if self._background_task:
            self._background_task.cancel()
            try:
                await self._background_task
            except asyncio.CancelledError:
                pass
            self._background_task = None
            
        if self.session:
            await self.session.close()
            self.session = None
            
        logger.info("üõë –°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    async def _delayed_start(self):
        """–û—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞"""
        try:
            await asyncio.sleep(10)
            if self.pipeline_status == PipelineStatus.IDLE:
                logger.info("‚ö° –ó–∞–ø—É—Å–∫ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ (RUN_IMMEDIATELY_ON_START)")
                await self.start_pipeline(manual=False)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞: {e}")

    async def _background_scheduler(self):
        """–§–æ–Ω–æ–≤—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        while True:
            try:
                if (self.is_auto_mode and 
                    self.pipeline_status in [PipelineStatus.IDLE, PipelineStatus.COMPLETED] and
                    self.next_run_scheduled and
                    datetime.now() >= self.next_run_scheduled):
                    
                    logger.info("‚è∞ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é")
                    await self.start_pipeline(manual=False)
                
                # –ï—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω, –Ω–æ –Ω–µ—Ç –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞, –ø–ª–∞–Ω–∏—Ä—É–µ–º –µ–≥–æ
                elif (self.is_auto_mode and 
                      self.pipeline_status in [PipelineStatus.IDLE, PipelineStatus.COMPLETED] and
                      not self.next_run_scheduled):
                    
                    self.next_run_scheduled = datetime.now() + timedelta(minutes=self.interval_minutes)
                    logger.info(f"‚è∞ –ü–ª–∞–Ω–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ –Ω–∞ {self.next_run_scheduled.strftime('%H:%M:%S')}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                if websocket_manager.is_anyone_online():
                    await self._update_stats()
                    await event_emitter.emit_automation_status(self.get_status())
                
                await asyncio.sleep(30)  # –£–º–µ–Ω—å—à–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ: {e}")
                await asyncio.sleep(30)
    
    async def start_pipeline(self, manual: bool = False) -> bool:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        if self.pipeline_status == PipelineStatus.RUNNING:
            return False
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–ø—É—Å–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞
        await event_emitter.emit_automation_status(self.get_status())
        self.pipeline_status = PipelineStatus.RUNNING
        self.last_run_start = datetime.now()
        self.last_run_end = None
        
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ ({'—Ä—É—á–Ω–æ–π' if manual else '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π'})")
        
        try:
            success = True
            for stage in PipelineStage:
                if not self.enabled_stages.get(stage, False):
                    continue
                    
                self.current_stage = stage
                stage_success = await self._execute_stage(stage)
                
                if not stage_success:
                    success = False
                    break
            
            self.pipeline_status = PipelineStatus.COMPLETED if success else PipelineStatus.ERROR
            self.current_stage = None
            self.last_run_end = datetime.now()
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
            await event_emitter.emit_automation_status(self.get_status())
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
            if success:
                await event_emitter.emit_automation_completed()
            else:
                await event_emitter.emit_automation_error("–ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏")
            
            scraping_progress = self.stage_details[PipelineStage.SCRAPING]["progress"]
            duplicate_progress = self.stage_details[PipelineStage.DUPLICATE_PROCESSING]["progress"]
            realtor_progress = self.stage_details[PipelineStage.REALTOR_DETECTION]["progress"]
            
            self._last_new_ads = scraping_progress.get("new_ads", 0)
            self._last_processed_ads = scraping_progress.get("processed_ads", 0)  
            self._last_duplicates_found = duplicate_progress.get("duplicates_found", 0)
            self._last_realtors_found = realtor_progress.get("detected", 0)
            if not manual and self.is_auto_mode:
                self.next_run_scheduled = datetime.now() + timedelta(minutes=self.interval_minutes)
                logger.info(f"‚è∞ –°–ª–µ–¥—É—é—â–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.next_run_scheduled.strftime('%H:%M:%S')}")
            
            logger.info(f"‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω {'—É—Å–ø–µ—à–Ω–æ' if success else '—Å –æ—à–∏–±–∫–∞–º–∏'}")
            return success
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
            self.pipeline_status = PipelineStatus.ERROR
            self.current_stage = None
            return False
    
    async def _execute_stage(self, stage: PipelineStage) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        stage_info = self.stage_details[stage]
        stage_info["status"] = "running"
        stage_info["started_at"] = datetime.now()
        stage_info["error"] = None
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –Ω–∞—á–∞–ª–∞ —ç—Ç–∞–ø–∞
        await event_emitter.emit_automation_progress(stage.value, 0, {"stage": stage.value, "status": "started"})
        
        try:
            if stage == PipelineStage.SCRAPING:
                success = await self._execute_scraping()
            elif stage == PipelineStage.PHOTO_PROCESSING:
                success = await self._execute_photo_processing()
            elif stage == PipelineStage.DUPLICATE_PROCESSING:
                success = await self._execute_duplicate_processing()
            elif stage == PipelineStage.REALTOR_DETECTION:
                success = await self._execute_realtor_detection()
            elif stage == PipelineStage.ELASTICSEARCH_REINDEX:
                success = await self._execute_elasticsearch_reindex()
            else:
                success = False
                
            stage_info["status"] = "completed" if success else "error"
            stage_info["completed_at"] = datetime.now()
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —ç—Ç–∞–ø–∞
            progress = 100 if success else 0
            await event_emitter.emit_automation_progress(stage.value, progress, {"stage": stage.value, "status": "completed" if success else "error"})
            
            return success
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ {stage.value}: {e}")
            self.stage_details[stage]["status"] = "error"
            self.stage_details[stage]["error"] = str(e)
            self.stage_details[stage]["completed_at"] = datetime.now()
            return False
    
    async def _execute_scraping(self) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á—ë–Ω –ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö
        if not self.enabled_stages[PipelineStage.SCRAPING]:
            logger.info("üö´ –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∫–ª—é—á—ë–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∞–ø")
            self.stage_details[PipelineStage.SCRAPING]["status"] = "skipped"
            self.stage_details[PipelineStage.SCRAPING]["started_at"] = datetime.now().isoformat()
            self.stage_details[PipelineStage.SCRAPING]["completed_at"] = datetime.now().isoformat()
            return True
        
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        self.stage_details[PipelineStage.SCRAPING]["status"] = "running"
        self.stage_details[PipelineStage.SCRAPING]["started_at"] = datetime.now().isoformat()
        
        progress = self.stage_details[PipelineStage.SCRAPING]["progress"]
        progress["total"] = len(self.scraping_sources)
        progress["completed"] = 0
        progress["failed"] = 0
        progress["sources_active"] = 0
        progress["sources_completed"] = 0
        progress["new_ads"] = 0
        progress["processed_ads"] = 0
        
        job_ids = []
        
        for source in self.scraping_sources:
            try:
                async with self.session.post(f"{self.api_base_url}/api/scraping/start/{source}") as response:
                    if response.status in [200, 201, 202]:
                        data = await response.json()
                        job_id = data.get('job_id')
                        if job_id:
                            job_ids.append((source, job_id))
                            progress["sources_active"] += 1
                            logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {source} –∑–∞–ø—É—â–µ–Ω (job_id: {job_id})")
                            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–µ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ - ScrapyManager —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ
                    else:
                        progress["failed"] += 1
                        error_text = await response.text()
                        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source}: {response.status} - {error_text}")
            except Exception as e:
                progress["failed"] += 1
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source}: {e}")
        
        if not job_ids:
            return False
        return await self._wait_for_scraping_completion(job_ids)
    
    async def _wait_for_scraping_completion(self, job_ids: list) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        progress = self.stage_details[PipelineStage.SCRAPING]["progress"]
        completed_jobs = set()
        
        while len(completed_jobs) < len(job_ids):
            await self._update_stats()
            
            for source, job_id in job_ids:
                if job_id in completed_jobs:
                    continue
                    
                try:
                    async with self.session.get(f"{self.api_base_url}/api/scraping/status/{job_id}") as response:
                        if response.status == 200:
                            data = await response.json()
                            status = data.get('status', 'unknown')
                            
                            if status in ['–∑–∞–≤–µ—Ä—à–µ–Ω–æ', '–æ—à–∏–±–∫–∞', '–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ']:
                                completed_jobs.add(job_id)
                                progress["sources_active"] -= 1
                                progress["sources_completed"] += 1
                                
                                if status == '–∑–∞–≤–µ—Ä—à–µ–Ω–æ':
                                    progress["completed"] += 1
                                    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {source}: –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                                else:
                                    progress["failed"] += 1
                                    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {source}: –∑–∞–≤–µ—Ä—à–µ–Ω —Å –æ—à–∏–±–∫–æ–π ({status})")
                            elif status == '–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è':
                                logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {source}: –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
                                
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ {source}: {e}")
            
            if len(completed_jobs) < len(job_ids):
                await asyncio.sleep(30)
        await self._update_stats()
        return progress["failed"] == 0
    
    async def _wait_for_process_completion(self, process_type: str) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ñ–æ–Ω–æ–≤—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"""
        max_wait_time = 3600  
        check_interval = 30  
        elapsed_time = 0
        
        while elapsed_time < max_wait_time:
            try:
                endpoint_map = {
                    "photos": "/api/process/photos/status",
                    "duplicates": "/api/process/duplicates/status", 
                    "realtors": "/api/process/realtors/status",
                    "elasticsearch": "/api/elasticsearch/health"
                }
                
                endpoint = endpoint_map.get(process_type)
                if not endpoint:
                    logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–∞: {process_type}")
                    return False
                
                async with self.session.get(f"{self.api_base_url}{endpoint}") as response:
                    if response.status == 200:
                        data = await response.json()
                        if process_type == "photos":
                            status = data.get('status', 'unknown')
                            if status == 'completed':
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                                return True
                            elif status == 'running':
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
                            elif status == 'idle':
                                # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å idle, –∑–Ω–∞—á–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—Å—Ç–∞—Ç—É—Å: idle)")
                                return True
                            elif status == 'error':
                                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
                                return False
                                
                        elif process_type == "duplicates":
                            status = data.get('status', 'unknown')
                            if status == 'completed':
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                                return True
                            elif status == 'running':
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
                            elif status == 'idle':
                                # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å idle, –∑–Ω–∞—á–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω
                                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—Å—Ç–∞—Ç—É—Å: idle)")
                                return True
                            elif status == 'error':
                                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
                                return False
                                
                        elif process_type == "realtors":
                            status = data.get('status', 'unknown')
                            if status == 'completed':
                                logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
                                return True
                            elif status == 'running':
                                logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...")
                            elif status == 'idle':
                                # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å idle, –∑–Ω–∞—á–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω
                                logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ (—Å—Ç–∞—Ç—É—Å: idle)")
                                return True
                            elif status == 'error':
                                logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤")
                                return False
                                
                        elif process_type == "elasticsearch":
                            logger.info(f"–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                            return True
                            
                        logger.info(f"{process_type} –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è, –æ–∂–∏–¥–∞–Ω–∏–µ...")
                        
                    else:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å {process_type}")
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ {process_type}: {e}")
            
            await asyncio.sleep(check_interval)
            elapsed_time += check_interval
        
        logger.warning(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è {process_type}")
        return False
    
    async def _execute_photo_processing(self) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
        try:
            async with self.session.post(f"{self.api_base_url}/api/process/photos") as response:
                if response.status not in [200, 201, 202]:
                    return False
                    
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∑–∞–ø—É—â–µ–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
            return await self._wait_for_process_completion("photos")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ: {e}")
            return False
    
    async def _execute_duplicate_processing(self) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        try:
            async with self.session.post(f"{self.api_base_url}/api/process/duplicates") as response:
                if response.status not in [200, 201, 202]:
                    return False
                    
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–ø—É—â–µ–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
            return await self._wait_for_process_completion("duplicates")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            return False
    
    async def _execute_realtor_detection(self) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤"""
        try:
            async with self.session.post(f"{self.api_base_url}/api/process/realtors/detect") as response:
                if response.status not in [200, 201, 202]:
                    return False
                    
            logger.info("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –∑–∞–ø—É—â–µ–Ω–æ, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
            return await self._wait_for_process_completion("realtors")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤: {e}")
            return False
    
    async def _execute_elasticsearch_reindex(self) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        try:
            async with self.session.post(f"{self.api_base_url}/api/elasticsearch/reindex") as response:
                if response.status not in [200, 201, 202]:
                    return False
                    
            logger.info("–ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
            return await self._wait_for_process_completion("elasticsearch")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            return False
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
        scraping_progress = self.stage_details[PipelineStage.SCRAPING]["progress"]
        duplicate_progress = self.stage_details[PipelineStage.DUPLICATE_PROCESSING]["progress"]
        realtor_progress = self.stage_details[PipelineStage.REALTOR_DETECTION]["progress"]
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏—è—Ö
        stats = {
            "new_ads": scraping_progress.get("new_ads", 0),
            "processed_ads": scraping_progress.get("processed_ads", 0),
            "duplicates_found": duplicate_progress.get("duplicates_found", 0),
            "realtors_found": realtor_progress.get("detected", 0)
        }
        
        return {
            "pipeline_status": self.pipeline_status.value,
            "current_stage": self.current_stage.value if self.current_stage else None,
            "is_auto_mode": self.is_auto_mode,
            "interval_hours": self.interval_hours,
            "interval_minutes": self.interval_minutes,
            "scraping_sources": self.scraping_sources,
            "last_run_start": self.last_run_start.isoformat() if self.last_run_start else None,
            "last_run_end": self.last_run_end.isoformat() if self.last_run_end else None,
            "next_run_scheduled": self.next_run_scheduled.isoformat() if self.next_run_scheduled else None,
            "stats": stats,
            "enabled_stages": {stage.value: enabled for stage, enabled in self.enabled_stages.items()},
            "stage_details": {
                stage.value: {
                    "name": details["name"],
                    "status": details["status"],
                    "started_at": to_iso(details["started_at"]),
                    "completed_at": to_iso(details["completed_at"]),
                    "error": details["error"],
                    "progress": details["progress"],
                }
                for stage, details in self.stage_details.items()
            }
        }
    
    def set_auto_mode(self, enabled: bool):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        self.is_auto_mode = enabled
        if enabled and self.pipeline_status in [PipelineStatus.IDLE, PipelineStatus.COMPLETED]:
            # –ï—Å–ª–∏ –≤–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –∏ –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø–ª–∞–Ω–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫
            self.next_run_scheduled = datetime.now() + timedelta(minutes=self.interval_minutes)
            logger.info(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω. –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫: {self.next_run_scheduled.strftime('%H:%M:%S')}")
        elif not enabled:
            # –ï—Å–ª–∏ –≤—ã–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º, –æ—Ç–º–µ–Ω—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.next_run_scheduled = None
            logger.info("üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –æ—Ç–∫–ª—é—á–µ–Ω")
    
    def get_auto_mode(self) -> bool:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        return self.is_auto_mode
    
    def reload_settings(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î"""
        self.is_auto_mode = get_auto_mode()
        self.interval_minutes = get_pipeline_interval_minutes()
        self.interval_hours = self.interval_minutes / 60.0
        self.enabled_stages = {
            PipelineStage.SCRAPING: get_enable_scraping(),
            PipelineStage.PHOTO_PROCESSING: get_enable_photo_processing(),
            PipelineStage.DUPLICATE_PROCESSING: get_enable_duplicate_processing(),
            PipelineStage.REALTOR_DETECTION: get_enable_realtor_detection(),
            PipelineStage.ELASTICSEARCH_REINDEX: get_enable_elasticsearch_reindex()
        }
        self.scraping_sources = get_scraping_sources()
        
        # –ï—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω –∏ –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø–ª–∞–Ω–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫
        if (self.is_auto_mode and 
            self.pipeline_status in [PipelineStatus.IDLE, PipelineStatus.COMPLETED] and
            not self.next_run_scheduled):
            self.next_run_scheduled = datetime.now() + timedelta(minutes=self.interval_minutes)
            logger.info(f"üîÑ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω—ã. –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.next_run_scheduled.strftime('%H:%M:%S')}")
        elif not self.is_auto_mode:
            # –ï—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –æ—Ç–∫–ª—é—á–µ–Ω, –æ—Ç–º–µ–Ω—è–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            self.next_run_scheduled = None
            logger.info("üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –æ—Ç–∫–ª—é—á–µ–Ω –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —á–µ—Ä–µ–∑ WebSocket
        try:
            asyncio.create_task(event_emitter.emit_automation_status(self.get_status()))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫: {e}")
    
    
    def pause_pipeline(self):
        """–ü—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        if self.pipeline_status == PipelineStatus.RUNNING:
            self.pipeline_status = PipelineStatus.PAUSED
            logger.info("‚è∏Ô∏è –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def resume_pipeline(self):
        """–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        if self.pipeline_status == PipelineStatus.PAUSED:
            self.pipeline_status = PipelineStatus.RUNNING
            logger.info("‚ñ∂Ô∏è –ü–∞–π–ø–ª–∞–π–Ω –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω")
            # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
            if not self._background_task or self._background_task.done():
                self._background_task = asyncio.create_task(self._run_pipeline_stages())

    async def stop_pipeline(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if self.pipeline_status != PipelineStatus.RUNNING:
            logger.warning("–ü–∞–π–ø–ª–∞–π–Ω –Ω–µ –∑–∞–ø—É—â–µ–Ω, –Ω–µ—á–µ–≥–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å.")
            return

        logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        
        # 1. –û—Ç–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –ø–∞–π–ø–ª–∞–π–Ω–∞
        if self._background_task and not self._background_task.done():
            self._background_task.cancel()
            try:
                await self._background_task
            except asyncio.CancelledError:
                logger.info("–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ —É—Å–ø–µ—à–Ω–æ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
            self._background_task = None

        # 2. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ ScrapyManager
        try:
            active_jobs = await self.scrapy_manager.get_all_jobs()
            running_jobs = [job for job in active_jobs if job.get('status') == '–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è']
            
            if running_jobs:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(running_jobs)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞. –û—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É...")
                for job in running_jobs:
                    job_id = job.get('id')
                    if job_id:
                        await self.scrapy_manager.stop_job(job_id)
                        logger.info(f"  - –ö–æ–º–∞–Ω–¥–∞ –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ {job_id} ({job.get('config')})")
            else:
                logger.info("–ê–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")

        # 3. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —ç—Ç–∞–ø—ã
        self.pipeline_status = PipelineStatus.IDLE
        self.current_stage = None
        
        for stage in self.stage_details:
            self.stage_details[stage]["status"] = "idle"
            self.stage_details[stage]["error"] = None
            
        logger.info("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ —É—Å–ø–µ—à–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã.")

        # 4. –ü–ª–∞–Ω–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º
        if self.is_auto_mode:
            self.next_run_scheduled = datetime.now() + timedelta(minutes=self.interval_minutes)
            logger.info(f"–°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.next_run_scheduled.strftime('%Y-%m-%d %H:%M:%S')}")

        # 5. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —á–µ—Ä–µ–∑ WebSocket
        await self.update_stage_status()
    
    async def _update_stats(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            async with self.session.get(f"{self.api_base_url}/api/stats") as response:
                if response.status == 200:
                    current_stats = await response.json()
                    
                    if self.initial_stats is None:
                        self.initial_stats = current_stats.copy()
                    
                    if self.current_stage == PipelineStage.SCRAPING:
                        progress = self.stage_details[PipelineStage.SCRAPING]["progress"]
                        progress["new_ads"] = current_stats.get("total_original_ads", 0) - self.initial_stats.get("total_original_ads", 0)
                        progress["processed_ads"] = current_stats.get("total_unique_ads", 0) - self.initial_stats.get("total_unique_ads", 0)
                        
                    return current_stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return None

    async def update_stage_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        if not self.session:
            return
            
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if websocket_manager.is_anyone_online():
                await self._update_duplicates_status()
                await self._update_photos_status()
                await self._update_realtors_status()
                await self._update_scraping_status()
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–æ–ª–µ–µ —á–∞—Å—Ç–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                await self._update_stats()
            else:
                # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –æ—Ñ–ª–∞–π–Ω, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å—ã –≤ idle
                for stage_enum in [PipelineStage.DUPLICATE_PROCESSING, PipelineStage.PHOTO_PROCESSING, PipelineStage.REALTOR_DETECTION]:
                    stage = self.stage_details[stage_enum]
                    if stage["status"] == "running":
                        stage["status"] = "idle"
                        stage["started_at"] = None
                        stage["completed_at"] = None
                        stage["error"] = None
                
                # –î–ª—è scraping –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å —á–µ—Ä–µ–∑ WebSocket
            await event_emitter.emit_automation_status(self.get_status())
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —ç—Ç–∞–ø–æ–≤: {e}")
    
    async def _update_duplicates_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        try:
            async with self.session.get(f"{self.api_base_url}/api/process/duplicates/status") as response:
                if response.status == 200:
                    data = await response.json()
                    stage = self.stage_details[PipelineStage.DUPLICATE_PROCESSING]
                    status = data.get('status', 'idle')
                    
                    if status == 'running':
                        stage["status"] = "running"
                        if not stage["started_at"]:
                            stage["started_at"] = datetime.now()
                        progress = data.get('progress', {})
                        if progress:
                            stage["progress"].update(progress)
                            
                    elif status == 'completed':
                        stage["status"] = "completed"
                        if not stage["completed_at"]:
                            stage["completed_at"] = datetime.now()
                        last_completed = data.get('last_completed')
                        if last_completed:
                            try:
                                from dateutil import parser
                                completed_time = parser.parse(last_completed)
                                if (datetime.now(completed_time.tzinfo) - completed_time).total_seconds() > 10:
                                    stage["status"] = "idle"
                                    stage["started_at"] = None
                                    stage["completed_at"] = None
                            except:
                                pass
                                
                    elif status == 'idle':
                        stage["status"] = "idle"
                        stage["started_at"] = None
                        stage["completed_at"] = None
                        
                    elif status == 'error':
                        stage["status"] = "error"
                        stage["error"] = data.get('error', '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞')
                        stage["completed_at"] = datetime.now()
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if websocket_manager.is_anyone_online():
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            else:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –æ—Ñ–ª–∞–π–Ω): {e}")
    
    async def _update_photos_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
        try:
            async with self.session.get(f"{self.api_base_url}/api/process/photos/status") as response:
                if response.status == 200:
                    data = await response.json()
                    stage = self.stage_details[PipelineStage.PHOTO_PROCESSING]
                    status = data.get('status', 'idle')
                    
                    if status == 'running':
                        stage["status"] = "running"
                        if not stage["started_at"]:
                            stage["started_at"] = datetime.now()
                        progress = data.get('progress', {})
                        if progress:
                            stage["progress"].update(progress)
                            
                    elif status == 'completed':
                        stage["status"] = "completed"
                        if not stage["completed_at"]:
                            stage["completed_at"] = datetime.now()
                        last_completed = data.get('last_completed')
                        if last_completed:
                            try:
                                from dateutil import parser
                                completed_time = parser.parse(last_completed)
                                if (datetime.now(completed_time.tzinfo) - completed_time).total_seconds() > 10:
                                    stage["status"] = "idle"
                                    stage["started_at"] = None
                                    stage["completed_at"] = None
                            except:
                                pass
                                
                    elif status == 'idle':
                        stage["status"] = "idle"
                        stage["started_at"] = None
                        stage["completed_at"] = None
                        
                    elif status == 'error':
                        stage["status"] = "error"
                        stage["error"] = data.get('error', '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞')
                        stage["completed_at"] = datetime.now()
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if websocket_manager.is_anyone_online():
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: {e}")
            else:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –æ—Ñ–ª–∞–π–Ω): {e}")
    
    async def _update_realtors_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤"""
        try:
            async with self.session.get(f"{self.api_base_url}/api/process/realtors/status") as response:
                if response.status == 200:
                    data = await response.json()
                    stage = self.stage_details[PipelineStage.REALTOR_DETECTION]
                    status = data.get('status', 'idle')
                    
                    if status == 'running':
                        stage["status"] = "running"
                        if not stage["started_at"]:
                            stage["started_at"] = datetime.now()
                        progress = data.get('progress', {})
                        if progress:
                            stage["progress"].update(progress)
                            
                    elif status == 'completed':
                        stage["status"] = "completed"
                        if not stage["completed_at"]:
                            stage["completed_at"] = datetime.now()
                        last_completed = data.get('last_completed')
                        if last_completed:
                            try:
                                from dateutil import parser
                                completed_time = parser.parse(last_completed)
                                if (datetime.now(completed_time.tzinfo) - completed_time).total_seconds() > 10:
                                    stage["status"] = "idle"
                                    stage["started_at"] = None
                                    stage["completed_at"] = None
                            except:
                                pass
                                
                    elif status == 'idle':
                        stage["status"] = "idle"
                        stage["started_at"] = None
                        stage["completed_at"] = None
                        
                    elif status == 'error':
                        stage["status"] = "error"
                        stage["error"] = data.get('error', '–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞')
                        stage["completed_at"] = datetime.now()
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if websocket_manager.is_anyone_online():
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ realtors: {e}")
            else:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –æ—Ñ–ª–∞–π–Ω): {e}")
    
    async def _update_scraping_status(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            async with self.session.get(f"{self.api_base_url}/api/scraping/jobs") as response:
                if response.status == 200:
                    jobs = await response.json()
                    stage = self.stage_details[PipelineStage.SCRAPING]
                    running_jobs = [job for job in jobs if job.get('status') == '–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è']
                    
                    if running_jobs:
                        stage["status"] = "running"
                        if not stage["started_at"]:
                            stage["started_at"] = datetime.now()
                        progress = stage["progress"]
                        progress["sources_active"] = len(running_jobs)
                        progress["total"] = len(self.scraping_sources)
                        completed_sources = 0
                        for source in self.scraping_sources:
                            source_jobs = [j for j in jobs if j.get('config') == source]
                            if source_jobs:
                                latest_job = max(source_jobs, key=lambda x: x.get('created_at', ''))
                                if latest_job.get('status') == '–∑–∞–≤–µ—Ä—à–µ–Ω–æ':
                                    completed_sources += 1
                        
                        progress["sources_completed"] = completed_sources
                        
                    else:
                        recent_completed = [job for job in jobs if job.get('status') == '–∑–∞–≤–µ—Ä—à–µ–Ω–æ']
                        if recent_completed and stage["status"] == "running":
                            stage["status"] = "completed"
                            stage["completed_at"] = datetime.now()
                        elif stage["status"] not in ["completed"]:
                            stage["status"] = "idle"
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ WebSocket
                    await event_emitter.emit_scraping_sources_update({
                        "sources": self.scraping_sources,
                        "jobs": jobs
                    })
                            
        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            if websocket_manager.is_anyone_online():
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            else:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –æ—Ñ–ª–∞–π–Ω): {e}")

automation_service = AutomationService() 