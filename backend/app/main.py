from fastapi import FastAPI, HTTPException, status, Depends, Query, BackgroundTasks, APIRouter, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from typing import List, Dict, Optional, Union, Any
from datetime import datetime
from sqlalchemy.orm import Session, selectinload
from sqlalchemy import and_, func, desc
import logging
from contextlib import asynccontextmanager
import redis
import json
from cachetools import TTLCache
import asyncio

from app.database.models import Ad, AdCreateRequest, PaginatedUniqueAdsResponse, AdSource, DuplicateInfo, StatsResponse
from app.database import get_db, SessionLocal
from app.database import db_models
from app.utils.transform import transform_ad, transform_unique_ad, transform_realtor
from app.core.config import API_HOST, API_PORT, REDIS_URL, ELASTICSEARCH_HOSTS, ELASTICSEARCH_INDEX
from app.utils.duplicate_processor import DuplicateProcessor
from app.services.photo_service import PhotoService
from app.services.duplicate_service import DuplicateService
from app.services.elasticsearch_service import ElasticsearchService
from app.services.scrapy_manager import ScrapyManager, SCRAPY_CONFIG_TO_SPIDER
from app.services.automation_service import automation_service
from app.web_routes import web_router
from app.websocket import websocket_router
from app.services.event_emitter import event_emitter


# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ API Ñ€Ð¾ÑƒÑ‚ÐµÑ€Ñ‹
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

memory_cache = TTLCache(maxsize=1000, ttl=300)  

try:
    redis_client = redis.from_url(REDIS_URL) if REDIS_URL else None
except:
    redis_client = None
    logger.warning("Redis not available, using memory cache only")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ñ‹Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ"""
    
    # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ† Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð·Ð°Ð¿ÑƒÑÐºÐµ
    try:
        from app.database import engine
        from app.database import db_models
        logger.info("Creating database tables if they don't exist...")
        db_models.Base.metadata.create_all(bind=engine)
        logger.info("âœ… Database tables created/verified successfully!")
        
        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
        logger.info("Initializing default settings...")
        from app.services.settings_service import settings_service
        settings_service.initialize_default_settings()
        logger.info("âœ… Default settings initialized successfully!")
    except Exception as e:
        logger.error(f"âŒ Error creating database tables: {e}")
        raise e
    
    # ÐŸÑ€ÐµÐ´Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ML-Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
    try:
        logger.info("ðŸ”„ Preloading SentenceTransformer model for duplicates...")
        from app.utils.duplicate_processor import get_text_model
        model = get_text_model()
        logger.info("âœ… SentenceTransformer model for duplicates preloaded successfully!")
    except Exception as e:
        logger.error(f"âŒ Error preloading SentenceTransformer model for duplicates: {e}")
        # ÐÐµ Ð¿Ñ€ÐµÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ð·Ð°Ð¿ÑƒÑÐº Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ, Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑÑ Ð¿Ñ€Ð¸ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ð¸

    # Ð—Ð°Ð¿ÑƒÑÐº ÑÐµÑ€Ð²Ð¸ÑÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    await automation_service.start_service()
    
    logger.info("Starting Real Estate API...")
    yield
    
    # ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐµÑ€Ð²Ð¸ÑÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    await automation_service.stop_service()
    logger.info("Shutting down Real Estate API...")

app = FastAPI(
    title="Real Estate Aggregator API",
    description="API for aggregated real estate listings with duplicate detection and clustering.",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð²
app.mount("/static", StaticFiles(directory="app/static"), name="static")

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð²ÐµÐ±-Ð¸Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹ÑÐ°
app.include_router(web_router)

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ WebSocket
app.include_router(websocket_router)

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ API Ñ€Ð¾ÑƒÑ‚ÐµÑ€Ð¾Ð²


# Ð¡ÐµÑ€Ð²Ð¸ÑÑ‹
photo_service = PhotoService()
duplicate_service = DuplicateService()
es_service = ElasticsearchService(hosts=ELASTICSEARCH_HOSTS, index_name=ELASTICSEARCH_INDEX)

scrapy_manager = ScrapyManager(redis_url=f"{REDIS_URL}/0")

# Ð•Ð´Ð¸Ð½Ñ‹Ð¹ API Ñ€Ð¾ÑƒÑ‚ÐµÑ€
api_router = APIRouter(prefix="/api", tags=["api"])

# Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
async def get_from_cache(key: str) -> Optional[str]:
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÐºÑÑˆÐ°"""
    if key in memory_cache:
        return memory_cache[key]
    
    if redis_client:
        try:
            return await redis_client.get(key)
        except:
            pass
    return None

async def set_cache(key: str, value: str, ttl: int = 300):
    """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÐºÑÑˆ"""
    memory_cache[key] = value
    if redis_client:
        try:
            await redis_client.setex(key, ttl, value)
        except:
            pass

def build_unique_ads_query(
    db: Session,
    filters: Dict = None,
    include_relations: bool = True
):
    """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹"""
    query = db.query(db_models.DBUniqueAd)
    
    if include_relations:
        query = query.options(
            selectinload(db_models.DBUniqueAd.location),
            selectinload(db_models.DBUniqueAd.photos),
            selectinload(db_models.DBUniqueAd.realtor)
        )
    
    if not filters:
        return query
        

    
    if filters.get('city'):
        query = query.join(db_models.DBUniqueAd.location).filter(
            db_models.DBLocation.city == filters['city']
        )
    
    if filters.get('district'):
        query = query.join(db_models.DBUniqueAd.location).filter(
            db_models.DBLocation.district == filters['district']
        )
    
    if filters.get('min_price') is not None:
        query = query.filter(db_models.DBUniqueAd.price >= filters['min_price'])
    
    if filters.get('max_price') is not None:
        query = query.filter(db_models.DBUniqueAd.price <= filters['max_price'])
    
    if filters.get('min_area') is not None:
        query = query.filter(db_models.DBUniqueAd.area_sqm >= filters['min_area'])
    
    if filters.get('max_area') is not None:
        query = query.filter(db_models.DBUniqueAd.area_sqm <= filters['max_area'])
    
    if filters.get('rooms') is not None:
        query = query.filter(db_models.DBUniqueAd.rooms == filters['rooms'])
    
    if filters.get('has_duplicates'):
        if filters['has_duplicates']:
            query = query.filter(db_models.DBUniqueAd.duplicates_count > 0)
        else:
            query = query.filter(db_models.DBUniqueAd.duplicates_count == 0)
    
    if filters.get('is_realtor') is not None:
        if filters['is_realtor']:
            query = query.filter(db_models.DBUniqueAd.realtor_id.isnot(None))
        else:
            query = query.filter(db_models.DBUniqueAd.realtor_id.is_(None))
    
    return query

# === ÐžÐ¡ÐÐžÐ’ÐÐ«Ð• Ð­ÐÐ”ÐŸÐžÐ˜ÐÐ¢Ð« ===
@api_router.get("/", response_model=Dict[str, str])
async def read_root():
    """ÐšÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¹ ÑÐ½Ð´Ð¿Ð¾Ð¸Ð½Ñ‚ API"""
    return {
        "message": "Real Estate Aggregator API",
        "version": "1.0.0",
        "docs": "/docs"
    }

@api_router.get("/status", response_model=Dict[str, Union[str, int]])
async def get_status(db: Session = Depends(get_db)):
    """Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹"""
    try:
        total_unique = db.query(func.count(db_models.DBUniqueAd.id)).scalar()
        total_ads = db.query(func.count(db_models.DBAd.id)).scalar()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Redis
        redis_status = "connected" if redis_client else "disconnected"
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Elasticsearch
        es_health = es_service.health_check()
        es_status = es_health.get('status', 'unknown')
        
        return {
            "status": "healthy",
            "total_unique_ads": total_unique,
            "total_ads": total_ads,
            "redis": redis_status,
            "elasticsearch": es_status,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Status check failed: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@api_router.get("/stats", response_model=StatsResponse)
async def get_statistics(db: Session = Depends(get_db)):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð¾Ð±Ñ‰ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹"""
    
    cache_key = "system_stats"
    cached = await get_from_cache(cache_key)
    if cached:
        return json.loads(cached)
    processor = DuplicateProcessor(db)
    duplicate_stats = processor.get_duplicate_statistics()
    realtor_stats = processor.get_realtor_statistics()
    
    result = StatsResponse(
        total_unique_ads=duplicate_stats['total_unique_ads'],
        total_original_ads=duplicate_stats['total_original_ads'],
        total_duplicates=duplicate_stats['duplicate_ads'],
        realtor_ads=realtor_stats['realtor_unique_ads'],
        deduplication_ratio=duplicate_stats['deduplication_ratio']
    )
    await set_cache(cache_key, result.json(), ttl=300)
    
    return result

# === ÐžÐ‘ÐªÐ¯Ð’Ð›Ð•ÐÐ˜Ð¯ ===
@api_router.get("/ads/unique", response_model=PaginatedUniqueAdsResponse)
async def get_unique_ads(
    db: Session = Depends(get_db),
    city: Optional[str] = Query(None, description="Ð“Ð¾Ñ€Ð¾Ð´"),
    district: Optional[str] = Query(None, description="Ð Ð°Ð¹Ð¾Ð½"),
    min_price: Optional[float] = Query(None, ge=0, description="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ†ÐµÐ½Ð°"),
    max_price: Optional[float] = Query(None, ge=0, description="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ†ÐµÐ½Ð°"),
    min_area: Optional[float] = Query(None, ge=0, description="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ"),
    max_area: Optional[float] = Query(None, ge=0, description="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ"),
    rooms: Optional[int] = Query(None, ge=0, le=10, description="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ¾Ð¼Ð½Ð°Ñ‚"),
    has_duplicates: Optional[bool] = Query(None, description="Ð•ÑÑ‚ÑŒ Ð»Ð¸ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹"),
    is_realtor: Optional[bool] = Query(None, description="Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð°Ð¼"),
    sort_by: Optional[str] = Query("created_at", description="Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°: price, area_sqm, created_at, duplicates_count"),
    sort_order: Optional[str] = Query("desc", description="ÐŸÐ¾Ñ€ÑÐ´Ð¾Ðº: asc, desc"),
    limit: int = Query(50, ge=1, le=500, description="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"),
    offset: int = Query(0, ge=0, description="Ð¡Ð¼ÐµÑ‰ÐµÐ½Ð¸Ðµ")
):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹"""
    cache_key = f"unique_ads:{hash(str(locals()))}"
    cached = await get_from_cache(cache_key)
    if cached:
        return json.loads(cached)
    
    filters = {
        'city': city,
        'district': district,
        'min_price': min_price,
        'max_price': max_price,
        'min_area': min_area,
        'max_area': max_area,
        'rooms': rooms,
        'has_duplicates': has_duplicates,
        'is_realtor': is_realtor
    }
    
    query = build_unique_ads_query(db, filters)
    
    if sort_by == "price":
        order_col = db_models.DBUniqueAd.price
    elif sort_by == "area_sqm":
        order_col = db_models.DBUniqueAd.area_sqm
    elif sort_by == "duplicates_count":
        order_col = db_models.DBUniqueAd.duplicates_count
    else:
        order_col = db_models.DBUniqueAd.id 
    
    if sort_order == "desc":
        query = query.order_by(desc(order_col))
    else:
        query = query.order_by(order_col)
    total = query.count()
    db_unique_ads = query.offset(offset).limit(limit).all()
    
    result = PaginatedUniqueAdsResponse(
        total=total,
        offset=offset,
        limit=limit,
        items=[transform_unique_ad(ad) for ad in db_unique_ads]
    )
    await set_cache(cache_key, result.json(), ttl=180)
    return result

@api_router.get("/ads/unique/{unique_ad_id}", response_model=DuplicateInfo)
async def get_unique_ad_details(
    unique_ad_id: int,
    db: Session = Depends(get_db)
):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð´ÐµÑ‚Ð°Ð»Ð¸ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð¸ Ð²ÑÐµ ÐµÐ³Ð¾ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹"""
    
    cache_key = f"unique_ad_details:{unique_ad_id}"
    cached = await get_from_cache(cache_key)
    if cached:
        return json.loads(cached)
    
    unique_ad = db.query(db_models.DBUniqueAd).options(
        selectinload(db_models.DBUniqueAd.location),
        selectinload(db_models.DBUniqueAd.photos),
        selectinload(db_models.DBUniqueAd.realtor)
    ).filter(db_models.DBUniqueAd.id == unique_ad_id).first()
    
    if not unique_ad:
        raise HTTPException(status_code=404, detail="Unique ad not found")
    
    processor = DuplicateProcessor(db)
    all_ads_info = processor.get_all_ads_for_unique(unique_ad_id)
    
    base_ad = None
    if all_ads_info['base_ad']:
        base_ad = transform_ad(all_ads_info['base_ad'][0])
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ðµ Ð¸Ð· ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ
        if unique_ad.realtor_id:
            base_ad.realtor_id = unique_ad.realtor_id
            if unique_ad.realtor:
                base_ad.realtor = transform_realtor(unique_ad.realtor)
    duplicates = [transform_ad(ad) for ad in all_ads_info['duplicates']]
    
    sources = []
    if base_ad:
        sources.append(AdSource(
            id=base_ad.id,
            source_name=base_ad.source_name,
            source_url=str(base_ad.source_url) if base_ad.source_url is not None else None,
            published_at=base_ad.published_at,
            is_base=True
        ))
    for ad in duplicates:
        sources.append(AdSource(
            id=ad.id,
            source_name=ad.source_name,
            source_url=str(ad.source_url) if ad.source_url is not None else None,
            published_at=ad.published_at,
            is_base=False
        ))
    
    result = DuplicateInfo(
        unique_ad_id=unique_ad_id,
        total_duplicates=unique_ad.duplicates_count,
        base_ad=base_ad,
        duplicates=duplicates,
        sources=sources
    )
    
    await set_cache(cache_key, result.json(), ttl=600)
    
    return result

@api_router.get("/ads/unique/{unique_ad_id}/sources", response_model=List[AdSource])
async def get_unique_ad_sources(
    unique_ad_id: int,
    db: Session = Depends(get_db)
):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ Ð²ÑÐµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð´Ð»Ñ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ"""
    unique_ad = db.query(db_models.DBUniqueAd).filter(
        db_models.DBUniqueAd.id == unique_ad_id
    ).first()
    
    if not unique_ad:
        raise HTTPException(status_code=404, detail="Unique ad not found")
    processor = DuplicateProcessor(db)
    all_ads_info = processor.get_all_ads_for_unique(unique_ad_id)
    
    sources = []
    if all_ads_info['base_ad']:
        base = all_ads_info['base_ad'][0]
        sources.append(AdSource(
            id=base.id,
            source_url=base.source_url,
            source_name=base.source_name,
            published_at=base.published_at,
            is_base=True
        ))
    for dup in all_ads_info['duplicates']:
        sources.append(AdSource(
            id=dup.id,
            source_url=dup.source_url,
            source_name=dup.source_name,
            published_at=dup.published_at,
            is_base=False
        ))
    
    return sources

@api_router.get("/ads/{ad_id}/duplicates")
async def get_ad_duplicates(ad_id: int, db: Session = Depends(get_db)):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ"""
    try:
        main_ad = db.query(db_models.DBUniqueAd).filter(
            db_models.DBUniqueAd.id == ad_id
        ).first()
        
        if not main_ad:
            raise HTTPException(status_code=404, detail="ÐžÐ±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ðµ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
        
        if main_ad.duplicates_count == 0:
            return {
                "main_ad": {
                    "id": main_ad.id,
                    "title": main_ad.title,
                    "duplicates_count": 0
                },
                "duplicates": []
            }
        
        duplicates = db.query(db_models.DBAd).options(
            selectinload(db_models.DBAd.location),
            selectinload(db_models.DBAd.photos)
        ).filter(
            db_models.DBAd.unique_ad_id == ad_id
        ).all()
        
        duplicates_data = []
        for dup in duplicates:
            dup_data = {
                "id": dup.id,
                "title": dup.title,
                "price": float(dup.price) if dup.price else None,
                "currency": dup.currency,
                "area_sqm": float(dup.area_sqm) if dup.area_sqm else None,
                "rooms": dup.rooms,
                "source_name": dup.source_name,
                "source_url": dup.source_url,
                "created_at": dup.parsed_at.isoformat() if dup.parsed_at else None,
                "location": None,
                "photos": []
            }
            
            if dup.location:
                dup_data["location"] = {
                    "city": dup.location.city,
                    "district": dup.location.district,
                    "address": dup.location.address
                }
            
            if dup.photos:
                dup_data["photos"] = [
                    {
                        "url": photo.url,
                        "is_main": False 
                    } for photo in dup.photos[:5] 
                ]
            
            duplicates_data.append(dup_data)
        
        return {
            "main_ad": {
                "id": main_ad.id,
                "title": main_ad.title,
                "duplicates_count": main_ad.duplicates_count
            },
            "duplicates": duplicates_data
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# === Ð’Ð¡ÐŸÐžÐœÐžÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐÐ¯ Ð¤Ð£ÐÐšÐ¦Ð˜Ð¯ Ð”Ð›Ð¯ ÐÐžÐ ÐœÐÐ›Ð˜Ð—ÐÐ¦Ð˜Ð˜ Ð¢Ð˜ÐŸÐ Ð¡Ð”Ð•Ð›ÐšÐ˜ ===
def normalize_listing_type(listing_type: str) -> str:
    if not listing_type:
        return None
    value = listing_type.strip().lower()
    if value in ["Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð°", "Ð¿Ñ€Ð¾Ð´Ð°Ñ‘Ñ‚ÑÑ", "Ð¿Ñ€Ð¾Ð´Ð°ÐµÑ‚ÑÑ", "sell", "sale"]:
        return "Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð°"
    if value in ["Ð°Ñ€ÐµÐ½Ð´Ð°", "ÑÐ´Ð°Ñ‡Ð°", "ÑÐ´Ð°ÐµÑ‚ÑÑ", "ÑÐ´Ð°Ñ‘Ñ‚ÑÑ", "rent", "lease"]:
        return "Ð°Ñ€ÐµÐ½Ð´Ð°"
    return value  # fallback: ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ, Ð½Ð¾ Ð² Ð½Ð¸Ð¶Ð½ÐµÐ¼ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ðµ

@api_router.post("/ads", response_model=Ad, status_code=status.HTTP_201_CREATED)
async def create_ad(
    ad_data: AdCreateRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð½Ð¾Ð²Ð¾Ðµ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ðµ"""
    # ðŸ” ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð¾Ðµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð²Ñ…Ð¾Ð´ÑÑ‰Ð¸Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
    logger.info(f"ðŸ” API received ad data: title='{ad_data.title[:50] if ad_data.title else 'None'}...'")
    logger.info(f"ðŸ” AI fields received: property_type={ad_data.property_type}, property_origin={ad_data.property_origin}, listing_type={ad_data.listing_type}")
    logger.info(f"ðŸ” Extracted data: rooms={ad_data.rooms}, area_sqm={ad_data.area_sqm}, floor={ad_data.floor}, total_floors={ad_data.total_floors}")
    logger.info(f"ðŸ” Realtor ID: {ad_data.realtor_id}")
    
    existing_ad = db.query(db_models.DBAd).filter(
        db_models.DBAd.source_url == ad_data.source_url
    ).first()
    
    if existing_ad:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="Ad with this source_url already exists"
        )
    
    try:
        db_location = None
        if ad_data.location:
            db_location = db.query(db_models.DBLocation).filter(
                and_(
                    db_models.DBLocation.city == ad_data.location.city,
                    db_models.DBLocation.district == ad_data.location.district,
                    db_models.DBLocation.address == ad_data.location.address
                )
            ).first()
            
            if not db_location:
                db_location = db_models.DBLocation(
                    city=ad_data.location.city,
                    district=ad_data.location.district,
                    address=ad_data.location.address
                )
                db.add(db_location)
                db.flush()

        published_at = None
        if ad_data.published_at:
            try:
                published_at = datetime.fromisoformat(ad_data.published_at)
            except ValueError:
                logger.warning(f"Invalid published_at format: {ad_data.published_at}")

        # --- ÐÐžÐ ÐœÐÐ›Ð˜Ð—ÐÐ¦Ð˜Ð¯ Ð¢Ð˜ÐŸÐ Ð¡Ð”Ð•Ð›ÐšÐ˜ ---
        normalized_listing_type = normalize_listing_type(ad_data.listing_type) if ad_data.listing_type else None

        db_ad = db_models.DBAd(
            source_id=ad_data.source_id,
            source_url=ad_data.source_url,
            source_name=ad_data.source_name,
            title=ad_data.title,
            description=ad_data.description,
            price=ad_data.price,
            price_original=ad_data.price_original,
            currency=ad_data.currency,
            rooms=ad_data.rooms,
            area_sqm=ad_data.area_sqm,
            floor=ad_data.floor,
            total_floors=ad_data.total_floors,
            series=ad_data.series,
            building_type=ad_data.building_type,
                            condition=ad_data.condition,
            furniture=ad_data.furniture,
            heating=ad_data.heating,
            hot_water=ad_data.hot_water,
            gas=ad_data.gas,
            ceiling_height=ad_data.ceiling_height,
            phone_numbers=ad_data.phone_numbers,
            location_id=db_location.id if db_location else None,
            attributes=ad_data.attributes,
            published_at=published_at,
            parsed_at=datetime.utcnow(),
            is_duplicate=False,
            is_processed=False,
            # ÐÐ¾Ð²Ñ‹Ðµ Ð¿Ð¾Ð»Ñ Ð´Ð»Ñ AI ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            property_type=ad_data.property_type,
            property_origin=ad_data.property_origin,
            listing_type=normalized_listing_type,
            realtor_id=ad_data.realtor_id
        )
        
        db.add(db_ad)
        db.flush()
        
        if ad_data.photos:
            for photo in ad_data.photos:
                db_photo = db_models.DBPhoto(
                    url=str(photo.url),
                    ad_id=db_ad.id
                )
                db.add(db_photo)
        
        db.commit()
        db.refresh(db_ad)
        
        # ðŸ” Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        logger.info(f"âœ… Ad created successfully: ID={db_ad.id}")
        logger.info(f"âœ… Saved AI fields: property_type='{db_ad.property_type}', property_origin='{db_ad.property_origin}', listing_type='{db_ad.listing_type}'")
        logger.info(f"âœ… Saved extracted data: rooms={db_ad.rooms}, area_sqm={db_ad.area_sqm}, floor={db_ad.floor}, total_floors={db_ad.total_floors}")
        logger.info(f"âœ… Saved realtor_id: {db_ad.realtor_id}")
        
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð¾ Ð½Ð¾Ð²Ð¾Ð¼ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¸
        try:
            from app.services.event_emitter import event_emitter
            await event_emitter.emit_new_ad(
                ad_id=db_ad.id,
                title=db_ad.title or "Ð‘ÐµÐ· Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ",
                source=db_ad.source_name
            )
            
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
            total_original_ads = db.query(db_models.DBAd).count()
            total_unique_ads = db.query(db_models.DBUniqueAd).count()
            await event_emitter.emit_stats_update({
                "total_original_ads": total_original_ads,
                "total_unique_ads": total_unique_ads,
                "new_ads": 1  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‡ÐµÑ‚Ñ‡Ð¸Ðº Ð½Ð¾Ð²Ñ‹Ñ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹
            })
        except Exception as e:
            logger.warning(f"Failed to emit events for new ad: {e}")
        
        background_tasks.add_task(index_ad_in_elasticsearch, db_ad.id)
        return transform_ad(db_ad)
        
    except Exception as e:
        db.rollback()
        logger.error(f"Error creating ad: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# === ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ ===
@api_router.post("/process/duplicates", status_code=status.HTTP_202_ACCEPTED)
async def process_duplicates(
    background_tasks: BackgroundTasks,
    batch_size: int = Query(100, ge=1, le=1000)
):
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð²ÑÐµÑ… Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð² Ñ„Ð¾Ð½Ðµ"""
    if duplicates_processing_status['status'] == 'running':
        return {"message": "Duplicate processing already running"}
    background_tasks.add_task(process_all_duplicates_async, batch_size)
    return {"message": "Processing all duplicates started", "batch_size": batch_size}

@api_router.post("/process/realtors/detect", status_code=status.HTTP_202_ACCEPTED)
async def detect_realtors(background_tasks: BackgroundTasks):
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð² Ð² Ñ„Ð¾Ð½Ðµ"""
    if realtors_detection_status['status'] == 'running':
        return {"message": "Realtor detection already running"}
    background_tasks.add_task(detect_realtors_async)
    return {"message": "Realtor detection started"}

@api_router.post("/process/photos", status_code=status.HTTP_202_ACCEPTED)
async def process_photos(background_tasks: BackgroundTasks):
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð²ÑÐµÑ… Ð½ÐµÐ¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ñ„Ð¾Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ð¹ Ð² Ñ„Ð¾Ð½Ðµ"""
    if photo_processing_status['status'] == 'running':
        return {"message": "Photo processing already running"}
    background_tasks.add_task(process_photos_async)
    return {"message": "Photo processing started"}

@api_router.get("/process/duplicates/status")
async def get_duplicates_process_status():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²"""
    return duplicates_processing_status

@api_router.get("/process/realtors/status")
async def get_realtors_detection_status():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð²"""
    return realtors_detection_status

@api_router.get("/process/photos/status")
async def get_photos_process_status():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ„Ð¾Ñ‚Ð¾"""
    return await photo_service.get_processing_status()

# === ELASTICSEARCH ===
@api_router.get("/elasticsearch/search", response_model=Dict)
async def search_ads(
    q: Optional[str] = Query(None, description="ÐŸÐ¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ"),
    city: Optional[str] = Query(None, description="Ð“Ð¾Ñ€Ð¾Ð´"),
    district: Optional[str] = Query(None, description="Ð Ð°Ð¹Ð¾Ð½"),
    min_price: Optional[float] = Query(None, ge=0, description="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ†ÐµÐ½Ð°"),
    max_price: Optional[float] = Query(None, ge=0, description="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ñ†ÐµÐ½Ð°"),
    min_area: Optional[float] = Query(None, ge=0, description="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ"),
    max_area: Optional[float] = Query(None, ge=0, description="ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ð»Ð¾Ñ‰Ð°Ð´ÑŒ"),
    rooms: Optional[int] = Query(None, ge=0, description="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐºÐ¾Ð¼Ð½Ð°Ñ‚"),

    source_name: Optional[str] = Query(None, description="Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ"),
    sort_by: Optional[str] = Query("relevance", description="Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°: relevance, price, area_sqm, created_at, published_at, duplicates_count"),
    sort_order: Optional[str] = Query("desc", description="ÐŸÐ¾Ñ€ÑÐ´Ð¾Ðº ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸: asc, desc"),
    page: int = Query(1, ge=1, description="ÐÐ¾Ð¼ÐµÑ€ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹"),
    size: int = Query(20, ge=1, le=100, description="Ð Ð°Ð·Ð¼ÐµÑ€ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹")
):
    """ÐŸÐ¾Ð»Ð½Ð¾Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· Elasticsearch"""
    
    filters = {
        'city': city,
        'district': district,
        'min_price': min_price,
        'max_price': max_price,
        'min_area': min_area,
        'max_area': max_area,
        'rooms': rooms,

        'source_name': source_name
    }
    filters = {k: v for k, v in filters.items() if v is not None}
    try:
        result = es_service.search_ads(q, filters, sort_by, sort_order, page, size)
        return result
    except Exception as e:
        logger.error(f"Error in search: {e}")
        raise HTTPException(status_code=500, detail=f"Search error: {str(e)}")

@api_router.get("/elasticsearch/health")
async def elasticsearch_health():
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÑ Elasticsearch"""
    try:
        health = es_service.health_check()
        return health
    except Exception as e:
        logger.error(f"Error checking Elasticsearch health: {e}")
        raise HTTPException(status_code=500, detail=f"Health check error: {str(e)}")

@api_router.get("/elasticsearch/stats")
async def elasticsearch_stats():
    """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Elasticsearch Ð¸Ð½Ð´ÐµÐºÑÐ°"""
    try:
        stats = es_service.get_stats()
        return stats
    except Exception as e:
        logger.error(f"Error getting Elasticsearch stats: {e}")
        raise HTTPException(status_code=500, detail=f"Stats error: {str(e)}")

@api_router.post("/elasticsearch/reindex", status_code=status.HTTP_202_ACCEPTED)
async def reindex_elasticsearch(background_tasks: BackgroundTasks):
    """ÐŸÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ Ð²ÑÐµÑ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ð² Elasticsearch"""
    background_tasks.add_task(reindex_elasticsearch_async)
    return {"message": "Reindexing started in background"}

# Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
async def index_ad_in_elasticsearch(ad_id: int):
    """Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ Ð² Elasticsearch"""
    try:
        from app.utils.transform import to_elasticsearch_dict
        
        db = SessionLocal()
        db_ad = db.query(db_models.DBAd).filter(db_models.DBAd.id == ad_id).first()
        
        if db_ad:
            ad_data = to_elasticsearch_dict(transform_ad(db_ad))
            es_service.index_ad(ad_data)
            logger.info(f"Ad {ad_id} indexed in Elasticsearch")
        
        db.close()
    except Exception as e:
        logger.error(f"Error indexing ad {ad_id} in Elasticsearch: {e}")

async def process_ad_async(ad_id: int):
    """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ"""
    try:
        from app.database import SessionLocal
        db = SessionLocal()
        
        try:
            db_ad = db.query(db_models.DBAd).filter(db_models.DBAd.id == ad_id).first()
            if not db_ad:
                return
            await photo_service.process_ad_photos(db, db_ad)
            processor = DuplicateProcessor(db)
            processor.process_ad(db_ad)
            
            db.commit()
            logger.info(f"Successfully processed ad {ad_id}")
            
        finally:
            db.close()
            
    except Exception as e:
        logger.error(f"Error processing ad {ad_id}: {e}")

photo_processing_status = {
    'status': 'idle', 
    'last_started': None,
    'last_completed': None,
    'last_error': None
}

duplicates_processing_status = {
    'status': 'idle',
    'last_started': None,
    'last_completed': None,
    'last_error': None,
    'batch_size': None,
    'total_processed': 0,
    'remaining': 0
}

realtors_detection_status = {
    'status': 'idle',
    'last_started': None,
    'last_completed': None,
    'last_error': None
}

async def process_all_duplicates_async(batch_size: int):
    """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²ÑÐµÑ… Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²"""
    try:
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        await event_emitter.emit_notification("info", "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²", "ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²", {
            "batch_size": batch_size
        })
        
        duplicates_processing_status['status'] = 'running'
        duplicates_processing_status['last_started'] = datetime.utcnow().isoformat()
        duplicates_processing_status['batch_size'] = batch_size
        
        from app.database import SessionLocal
        db = SessionLocal()
        
        try:
            processor = DuplicateProcessor(db)
            
            total_unprocessed = db.query(db_models.DBAd).filter(
                and_(
                    db_models.DBAd.is_processed == False,
                    db_models.DBAd.is_duplicate == False
                )
            ).count()
            
            if total_unprocessed == 0:
                logger.info("No unprocessed ads found for duplicate detection")
                duplicates_processing_status['status'] = 'completed'
                duplicates_processing_status['last_completed'] = datetime.utcnow().isoformat()
                await event_emitter.emit_notification("info", "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°", "ÐÐµÑ‚ Ð½ÐµÐ¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹")
                return
            
            logger.info(f"Starting duplicate processing for {total_unprocessed} unprocessed ads...")
            total_processed = 0
            duplicates_processing_status['total_processed'] = 0
            duplicates_processing_status['remaining'] = total_unprocessed
            
            await event_emitter.emit_automation_status({
                "stage": "duplicate_processing",
                "status": "started",
                "total_unprocessed": total_unprocessed,
                "batch_size": batch_size
            })
            
            while True:
                remaining = db.query(db_models.DBAd).filter(
                    and_(
                        db_models.DBAd.is_processed == False,
                        db_models.DBAd.is_duplicate == False
                    )
                ).count()
                
                if remaining == 0:
                    logger.info(f"âœ… All duplicates processed! Total processed: {total_processed}")
                    break
                
                batch_processed = processor.process_new_ads_batch(batch_size)
                total_processed += batch_processed
                
                duplicates_processing_status['total_processed'] = total_processed
                duplicates_processing_status['remaining'] = remaining - batch_processed
                
                logger.info(f"ðŸ“Š Processed batch of {batch_processed} ads. Total: {total_processed}, Remaining: {remaining - batch_processed}")
                
                db.commit()
                
                await asyncio.sleep(0.1)
            
            logger.info("âœ… Duplicate processing completed, ready for next automation stage")
            
            logger.info(f"ðŸŽ‰ Duplicate processing completed! Total processed: {total_processed} ads")
            duplicates_processing_status['status'] = 'completed'
            duplicates_processing_status['last_completed'] = datetime.utcnow().isoformat()
            
            # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ
            await event_emitter.emit_notification("success", "ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°", 
                f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {total_processed} Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹", {
                    "total_processed": total_processed
                })
            
        finally:
            db.close()
            
    except Exception as e:
        duplicates_processing_status['status'] = 'error'
        duplicates_processing_status['last_error'] = str(e)
        logger.error(f"âŒ Error processing duplicates: {e}")
        await event_emitter.emit_notification("error", "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸", str(e))

async def detect_realtors_async():
    """Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð²"""
    try:
        realtors_detection_status['status'] = 'running'
        realtors_detection_status['last_started'] = datetime.utcnow().isoformat()
        
        db = SessionLocal()
        duplicate_service = DuplicateService()
        await duplicate_service.detect_all_realtors(db)
        db.close()
        
        realtors_detection_status['status'] = 'completed'
        realtors_detection_status['last_completed'] = datetime.utcnow().isoformat()
        logger.info("Realtor detection completed")
    except Exception as e:
        realtors_detection_status['status'] = 'error'
        realtors_detection_status['last_error'] = str(e)
        logger.error(f"Error in realtor detection: {e}")

async def reindex_elasticsearch_async():
    """Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸ Elasticsearch"""
    try:
        from app.database import SessionLocal
        from app.utils.transform import to_elasticsearch_dict
        
        db = SessionLocal()
        
        try:
            unique_ads = db.query(db_models.DBUniqueAd).all()
            ads_data = []
            for unique_ad in unique_ads:
                ad_dict = to_elasticsearch_dict(transform_unique_ad(unique_ad))
                ads_data.append(ad_dict)
            
            logger.info(f"Starting reindex of {len(ads_data)} ads")
            success = es_service.reindex_all(ads_data)
            
            if success:
                logger.info("âœ… Elasticsearch reindexing completed successfully!")
            else:
                logger.error("âŒ Some errors occurred during reindexing")
        finally:
            db.close()
    except Exception as e:
        logger.error(f"Error during Elasticsearch reindexing: {e}")

async def process_photos_async():
    """Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ„Ð¾Ñ‚Ð¾Ð³Ñ€Ð°Ñ„Ð¸Ð¹"""
    try:
        photo_processing_status['status'] = 'running'
        photo_processing_status['last_started'] = datetime.utcnow().isoformat()
        db = SessionLocal()
        try:
            await photo_service.process_all_unprocessed_photos(db)
            photo_processing_status['status'] = 'completed'
            photo_processing_status['last_completed'] = datetime.utcnow().isoformat()
        finally:
            db.close()
    except Exception as e:
        photo_processing_status['status'] = 'error'
        photo_processing_status['last_error'] = str(e)
        logger.error(f"Error processing photos: {e}")

# === Ð¡ÐšÐ ÐÐŸÐ˜ÐÐ“ ===
@api_router.post("/scraping/start/{config_name}")
async def start_scraping(config_name: str, background_tasks: BackgroundTasks):
    if config_name not in SCRAPY_CONFIG_TO_SPIDER:
        raise HTTPException(status_code=400, detail="ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ„Ð¸Ð³")
    
    try:
        job_id = await scrapy_manager.start_job(config_name)
        return {"message": "Ð—Ð°Ð´Ð°Ñ‡Ð° Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ð°", "job_id": job_id}
    except ValueError as e:
        if "Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½" in str(e):
            raise HTTPException(status_code=400, detail="ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½ Ð² Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹")
        else:
            raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° {config_name}: {e}")
        raise HTTPException(status_code=500, detail="Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°")

@api_router.get("/scraping/status/{job_id}")
async def get_status(job_id: str):
    job = await scrapy_manager.get_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Ð—Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
    return job

@api_router.get("/scraping/jobs")
async def get_jobs():
    jobs = await scrapy_manager.get_all_jobs()
    return jobs

@api_router.post("/scraping/stop/{job_id}")
async def stop_job(job_id: str):
    job = await scrapy_manager.get_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Ð—Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
    await scrapy_manager.stop_job(job_id)
    return {"message": "Ð—Ð°Ð´Ð°Ñ‡Ð° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°"}

@api_router.post("/scraping/start-all")
async def start_all(background_tasks: BackgroundTasks):
    try:
        job_ids = await scrapy_manager.start_all()
        if not job_ids:
            return {"message": "ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ñ‘Ð½ Ð² Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ñ… ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹", "job_ids": []}
        return {"message": "Ð’ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½Ñ‹", "job_ids": job_ids}
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}")
        raise HTTPException(status_code=500, detail="Ð’Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°")

@api_router.post("/scraping/stop-all")
async def stop_all():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°"""
    try:
        await scrapy_manager.stop_all_jobs()
        return {"message": "Ð’ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹"}
    except Exception as e:
        logger.error(f"Error stopping all jobs: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/scraping/log/{job_id}")
async def get_log(job_id: str, limit: int = 100):
    job = await scrapy_manager.get_status(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Ð—Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
    log = await scrapy_manager.get_log(job_id, limit=limit)
    return {"log": log}

@api_router.get("/scraping/sources")
async def get_scraping_sources():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð¸ Ð¸Ñ… ÑÑ‚Ð°Ñ‚ÑƒÑ"""
    try:
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        jobs = await scrapy_manager.get_all_jobs()
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¸Ð· Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº Ð‘Ð”
        from app.services.settings_service import settings_service
        sources = settings_service.get_setting('scraping_sources', ['house'])
        
        result = {
            "sources": sources,
            "jobs": jobs
        }
        
        return result
    except Exception as e:
        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}")
        raise HTTPException(status_code=500, detail=str(e))



# === ÐÐ’Ð¢ÐžÐœÐÐ¢Ð˜Ð—ÐÐ¦Ð˜Ð¯ ===
@api_router.get("/automation/status")
async def get_automation_status():
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð²ÑÐµÑ… ÑÑ‚Ð°Ð¿Ð¾Ð² Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
    await automation_service.update_stage_status()
    
    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð¸Ð· automation_service
    status = automation_service.get_status()
    
    # Ð—Ð°Ð¼ÐµÐ½ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ Ð¸Ð· Ð‘Ð”
    from app.services.settings_service import settings_service
    
    # ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ€ÐµÐ¶Ð¸Ð¼
    status['is_auto_mode'] = settings_service.get_setting('auto_mode', True)
    
    # Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»
    interval_minutes = settings_service.get_setting('pipeline_interval_minutes', 180)
    status['interval_minutes'] = interval_minutes
    status['interval_hours'] = interval_minutes / 60.0
    
    # Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
    status['scraping_sources'] = settings_service.get_setting('scraping_sources', ['lalafo', 'stroka'])
    
    # Ð’ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ð¿Ñ‹
    status['enabled_stages'] = {
        'scraping': settings_service.get_setting('enable_scraping', True),
        'photo_processing': settings_service.get_setting('enable_photo_processing', True),
        'duplicate_processing': settings_service.get_setting('enable_duplicate_processing', True),
        'realtor_detection': settings_service.get_setting('enable_realtor_detection', True),
        'elasticsearch_reindex': settings_service.get_setting('enable_elasticsearch_reindex', True)
    }
    
    return status

@api_router.get("/users/online")
async def get_online_users():
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð¾Ð½Ð»Ð°Ð¹Ð½ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹"""
    from app.services.websocket_manager import websocket_manager
    return {
        "online": websocket_manager.is_anyone_online(),
        "connection_count": websocket_manager.get_connection_count(),
        "connected_users": websocket_manager.get_connected_users()
    }

@api_router.post("/automation/start")
async def start_automation_pipeline(manual: bool = True):
    """Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    success = await automation_service.start_pipeline(manual=manual)
    return {
        "success": success,
        "message": "ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð·Ð°Ð¿ÑƒÑ‰ÐµÐ½" if success else "ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ ÑƒÐ¶Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ÑÑ"
    }

@api_router.post("/automation/stop")
async def stop_automation_pipeline():
    """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    await automation_service.stop_pipeline()
    return {"message": "ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½"}

@api_router.post("/automation/pause")
async def pause_automation_pipeline():
    """ÐŸÑ€Ð¸Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    automation_service.pause_pipeline()
    return {"message": "ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¿Ñ€Ð¸Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½"}

@api_router.post("/automation/resume")
async def resume_automation_pipeline():
    """Ð’Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"""
    automation_service.resume_pipeline()
    return {"message": "ÐŸÐ°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð²Ð¾Ð·Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½"}



# === ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ Ð¡Ð˜Ð¡Ð¢Ð•ÐœÐ« ===
@api_router.get("/settings")
async def get_all_settings():
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹"""
    from app.services.settings_service import settings_service
    return settings_service.get_all_settings()

@api_router.get("/settings/{category}")
async def get_settings_by_category(category: str):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸"""
    from app.services.settings_service import settings_service
    return settings_service.get_settings_by_category(category)

from pydantic import BaseModel

class SettingUpdateRequest(BaseModel):
    value: Any
    value_type: str = 'string'
    description: str = None
    category: str = None

@api_router.post("/settings/{key}")
async def update_setting(
    key: str,
    request: SettingUpdateRequest
):
    """ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÑƒ"""
    from app.services.settings_service import settings_service
    success = settings_service.set_setting(key, request.value, request.value_type, request.description, request.category)
    
    # Ð¡Ð¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ automation_service Ð´Ð»Ñ Ð²ÑÐµÑ… ÐºÐ»ÑŽÑ‡ÐµÐ¹ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸
    automation_keys = [
        'auto_mode', 'pipeline_interval_minutes', 'scraping_sources',
        'enable_scraping', 'enable_photo_processing', 'enable_duplicate_processing',
        'enable_realtor_detection', 'enable_elasticsearch_reindex', 'run_immediately_on_start'
    ]
    if key in automation_keys and success:
        automation_service.reload_settings()
        logger.info(f"ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ñ Ð‘Ð”: {key}={request.value}")
    
    return {
        "success": success,
        "message": f"ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° '{key}' {'Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð°' if success else 'Ð½Ðµ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð°'}"
    }

@api_router.delete("/settings/{key}")
async def delete_setting(key: str):
    """Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÑƒ"""
    from app.services.settings_service import settings_service
    success = settings_service.delete_setting(key)
    return {
        "success": success,
        "message": f"ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° '{key}' {'ÑƒÐ´Ð°Ð»ÐµÐ½Ð°' if success else 'Ð½Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð°'}"
    }

# === Ð­ÐÐ”ÐŸÐžÐ˜ÐÐ¢Ð« Ð”Ð›Ð¯ Ð ÐÐ‘ÐžÐ¢Ð« Ð¡ Ð Ð˜Ð­Ð›Ð¢ÐžÐ ÐÐœÐ˜ ===
@api_router.get("/realtors", response_model=Dict)
async def get_realtors(
    db: Session = Depends(get_db),
    limit: int = Query(50, ge=1, le=200, description="ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹"),
    offset: int = Query(0, ge=0, description="Ð¡Ð¼ÐµÑ‰ÐµÐ½Ð¸Ðµ"),
    min_ads_count: Optional[int] = Query(None, ge=1, description="ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹"),
    sort_by: Optional[str] = Query("total_ads_count", description="Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°: total_ads_count, confidence_score, last_activity")
):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð² Ñ Ð¸Ñ… ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¾Ð¹"""
    try:
        query = db.query(db_models.DBRealtor)
        
        if min_ads_count is not None:
            query = query.filter(db_models.DBRealtor.total_ads_count >= min_ads_count)
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°
        if sort_by == "confidence_score":
            query = query.order_by(desc(db_models.DBRealtor.confidence_score))
        elif sort_by == "last_activity":
            query = query.order_by(desc(db_models.DBRealtor.last_activity))
        else:  # total_ads_count
            query = query.order_by(desc(db_models.DBRealtor.total_ads_count))
        
        total = query.count()
        realtors = query.offset(offset).limit(limit).all()
        
        result = []
        for realtor in realtors:
            result.append({
                "id": realtor.id,
                "phone_number": realtor.phone_number,
                "name": realtor.name,
                "company_name": realtor.company_name,
                "total_ads_count": realtor.total_ads_count,
                "active_ads_count": realtor.active_ads_count,
                "confidence_score": realtor.confidence_score,
                "average_price": realtor.average_price,
                "favorite_districts": realtor.favorite_districts,
                "property_types": realtor.property_types,
                "first_seen": realtor.first_seen.isoformat() if realtor.first_seen else None,
                "last_activity": realtor.last_activity.isoformat() if realtor.last_activity else None
            })
        
        return {
            "items": result,
            "total": total,
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        logger.error(f"Error getting realtors: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/realtors/{realtor_id}", response_model=Dict)
async def get_realtor_details(
    realtor_id: int,
    db: Session = Depends(get_db),
    include_ads: bool = Query(False, description="Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ")
):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð°"""
    try:
        realtor = db.query(db_models.DBRealtor).filter(
            db_models.DBRealtor.id == realtor_id
        ).first()
        
        if not realtor:
            raise HTTPException(status_code=404, detail="Ð Ð¸ÑÐ»Ñ‚Ð¾Ñ€ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        
        result = {
            "id": realtor.id,
            "phone_number": realtor.phone_number,
            "name": realtor.name,
            "company_name": realtor.company_name,
            "total_ads_count": realtor.total_ads_count,
            "active_ads_count": realtor.active_ads_count,
            "confidence_score": realtor.confidence_score,
            "average_price": realtor.average_price,
            "favorite_districts": realtor.favorite_districts,
            "property_types": realtor.property_types,
            "first_seen": realtor.first_seen.isoformat() if realtor.first_seen else None,
            "last_activity": realtor.last_activity.isoformat() if realtor.last_activity else None
        }
        
        if include_ads:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ñ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð°
            recent_ads = db.query(db_models.DBUniqueAd).options(
                selectinload(db_models.DBUniqueAd.location),
                selectinload(db_models.DBUniqueAd.photos),
                selectinload(db_models.DBUniqueAd.realtor)
            ).filter(
                db_models.DBUniqueAd.realtor_id == realtor_id
            ).order_by(desc(db_models.DBUniqueAd.created_at)).limit(10).all()
            
            result["recent_ads"] = [transform_unique_ad(ad) for ad in recent_ads]
        
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting realtor details: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.get("/realtors/stats", response_model=Dict)
async def get_realtors_stats(db: Session = Depends(get_db)):
    """Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð°Ð¼"""
    try:
        total_realtors = db.query(func.count(db_models.DBRealtor.id)).scalar() or 0
        active_realtors = db.query(func.count(db_models.DBRealtor.id)).filter(
            db_models.DBRealtor.active_ads_count > 0
        ).scalar() or 0
        
        avg_ads = db.query(func.avg(db_models.DBRealtor.total_ads_count)).scalar() or 0
        
        # Ð¢Ð¾Ð¿ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð¾Ð±ÑŠÑÐ²Ð»ÐµÐ½Ð¸Ð¹
        top_realtors = db.query(db_models.DBRealtor).order_by(
            desc(db_models.DBRealtor.total_ads_count)
        ).limit(5).all()
        
        top_realtors_data = []
        for realtor in top_realtors:
            top_realtors_data.append({
                "id": realtor.id,
                "name": realtor.name or f"Ð Ð¸ÑÐ»Ñ‚Ð¾Ñ€ {realtor.phone_number}",
                "phone_number": realtor.phone_number,
                "total_ads_count": realtor.total_ads_count,
                "confidence_score": realtor.confidence_score
            })
        
        return {
            "total_realtors": total_realtors,
            "active_realtors": active_realtors,
            "avg_ads_per_realtor": round(float(avg_ads), 2),
            "top_realtors": top_realtors_data
        }
    except Exception as e:
        logger.error(f"Error getting realtors stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@api_router.post("/realtors/rebuild", status_code=status.HTTP_202_ACCEPTED)
async def rebuild_realtors(background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """ÐŸÐ¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿ÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð²ÑÐµÑ… Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð² Ð¸ Ð¸Ñ… ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ"""
    try:
        # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð²
        db.query(db_models.DBRealtor).delete()
        db.commit()
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€Ð¾Ð² Ð² Ñ„Ð¾Ð½Ðµ
        background_tasks.add_task(detect_realtors_async)
        
        return {"message": "Realtor rebuild started. All existing realtor profiles will be recreated."}
    except Exception as e:
        logger.error(f"Error starting realtor rebuild: {e}")
        raise HTTPException(status_code=500, detail=str(e))

app.include_router(api_router)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=API_HOST,
        port=API_PORT,
        workers=2,
        log_level="info"
    )

