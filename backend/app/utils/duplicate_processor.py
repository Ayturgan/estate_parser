import logging
import re
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func
import numpy as np
from sentence_transformers import SentenceTransformer
import asyncio
from app.database.db_models import DBAd, DBUniqueAd, DBAdDuplicate, DBUniquePhoto
from app.services.ai_data_extractor import get_cached_gliner_model

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è CLIP –º–æ–¥–µ–ª–∏
try:
    from transformers import CLIPProcessor, CLIPModel
    import torch
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    logging.warning("CLIP –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ transformers –∏ torch –¥–ª—è –ø–æ–ª–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.")

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
_clip_model = None
_clip_processor = None
_clip_loaded = False

def get_clip_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é CLIP –º–æ–¥–µ–ª—å"""
    global _clip_model, _clip_processor, _clip_loaded
    
    if not CLIP_AVAILABLE:
        return None, None
    
    if not _clip_loaded:
        try:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º CLIP –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏...")
            _clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            _clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            _clip_loaded = True
            logger.info("CLIP –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CLIP –º–æ–¥–µ–ª—å: {e}")
            _clip_model = None
            _clip_processor = None
    
    return _clip_model, _clip_processor

# –ò–º–ø–æ—Ä—Ç event_emitter –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–±—ã—Ç–∏–π
try:
    from app.services.event_emitter import event_emitter
except ImportError:
    event_emitter = None

_text_model = None

def get_text_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (SentenceTransformer)"""
    global _text_model
    if _text_model is None:
        logger.info("Loading improved SentenceTransformer model...")
        from sentence_transformers import SentenceTransformer
        try:
            _text_model = SentenceTransformer("BAAI/bge-m3")
            logger.info("BGE-M3 model loaded successfully")
        except Exception as e:
            logger.warning(f"Failed to load BGE-M3 model: {e}")
            try:
                _text_model = SentenceTransformer("BAAI/bge-large-zh-v1.5")
                logger.info("BGE-large-zh-v1.5 model loaded successfully")
            except Exception as e2:
                logger.warning(f"Failed to load BGE-large-zh-v1.5 model: {e2}")
                _text_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
                logger.info("Fallback to paraphrase-multilingual-MiniLM-L12-v2 model")
    return _text_model

_gliner_model = None

def get_gliner_model():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å GLiNER"""
    global _gliner_model
    if _gliner_model is None:
        _gliner_model = get_cached_gliner_model()
    return _gliner_model


class DuplicateProcessor:
    def __init__(self, db: Session, realtor_threshold: int = 5):
        self.db = db
        self.text_model = get_text_model()
        self.gliner_model = get_gliner_model()
        self.realtor_threshold = realtor_threshold
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CLIP –º–æ–¥–µ–ª–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é)
        # self.clip_model, self.clip_processor = get_clip_model()  # –û—Ç–∫–ª—é—á–∞–µ–º CLIP
        self.clip_model, self.clip_processor = None, None
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        self.config = {
            'semantic_top_k': 10,         # –ö–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'semantic_threshold': 0.7,    # –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
            'weights': {
                'characteristics': 0.55,  # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ø–ª–æ—â–∞–¥—å, –∫–æ–º–Ω–∞—Ç—ã, —ç—Ç–∞–∂)
                'perceptual_photos': 0.3, # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
                'clip_photos': 0.0,       # CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–æ—Ç–∫–ª—é—á–µ–Ω–æ)
                'text': 0.1,              # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ - —É–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
                'address': 0.05,          # –ê–¥—Ä–µ—Å (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å)
            },
            'similarity_threshold': 0.8,  # –û–±—â–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–∞
            'photo_similarity_threshold': 0.7,  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
            'characteristics_similarity_threshold': 0.8,  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            'area_tolerance_percent': 5,  # –î–æ–ø—É—Å–∫ –ø–æ –ø–ª–æ—â–∞–¥–∏ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (¬±5%)
            'floor_tolerance_abs': 1,     # –î–æ–ø—É—Å–∫ –ø–æ —ç—Ç–∞–∂—É (¬±1 —ç—Ç–∞–∂)
            'photo_early_stop_threshold': 0.8,  # –ü–æ—Ä–æ–≥ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            'photo_required_threshold': 0.6,  # –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        }
    
    def process_new_ads_batch(self, batch_size: int = 1000) -> int:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        unprocessed_ads = self.db.query(DBAd).filter(
            and_(
                DBAd.is_processed == False,
                DBAd.is_duplicate == False
            )
        ).limit(batch_size).all()
        
        total_ads = len(unprocessed_ads)
        processed_count = 0
        
        if total_ads > 0:
            logger.info(f"Starting batch processing of {total_ads} ads")
        
        for i, ad in enumerate(unprocessed_ads):
            try:
                self.process_ad(ad)
                processed_count += 1
                if processed_count % 10 == 0 or processed_count == total_ads:
                    progress = int((processed_count / total_ads) * 100)
                    logger.info(f"Processed {processed_count}/{total_ads} ads ({progress}%)")
            except Exception as e:
                logger.error(f"Error processing ad {ad.id}: {e}")
                ad.is_processed = True
                ad.processed_at = datetime.utcnow()
                processed_count += 1
        
        if total_ads > 0:
            logger.info(f"Completed batch processing: {processed_count}/{total_ads} ads")
        
        return processed_count
    
    def process_ad(self, ad: DBAd):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        logger.info(f"Processing ad {ad.id} ({ad.title})")
        
        # –®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        ad_characteristics = self._get_unified_characteristics(ad)
        
        ad_photo_hashes = [photo.perceptual_hashes for photo in ad.photos 
                          if photo.perceptual_hashes and isinstance(photo.perceptual_hashes, dict)]
        text_embeddings = self._get_text_embeddings(ad, ad_characteristics)
        
        # –®–∞–≥ 2: –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        similar_unique_ads = self._find_similar_unique_ads(ad, ad_characteristics, ad_photo_hashes, text_embeddings)
        
        if similar_unique_ads:
            unique_ad, similarity = similar_unique_ads[0]
            logger.info(f"Found duplicate with similarity {similarity:.2f}")
            self._handle_duplicate(ad, unique_ad, similarity)
        else:
            logger.info("Creating new unique ad")
            self._create_unique_ad(ad, ad_photo_hashes, text_embeddings)
            
        ad.is_processed = True
        ad.processed_at = datetime.utcnow()
    
    def _get_unified_characteristics(self, ad_object: DBAd or DBUniqueAd) -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫, –∏–∑–≤–ª–µ–∫–∞—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ–ª–µ–π –∏ —Ç–µ–∫—Å—Ç–∞.
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–æ–ª–µ–π –ë–î.
        """
        text = f"{ad_object.title or ''} {ad_object.description or ''}".lower()
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–∏—Å–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        def extract_float(pattern, text):
            match = re.search(pattern, text)
            if match:
                try:
                    # –£–¥–∞–ª—è–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–∫–∏/–∑–∞–ø—è—Ç–æ–π, –∑–∞—Ç–µ–º –∑–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
                    val_str = re.sub(r'[^\d.,]', '', match.group(1)).replace(',', '.')
                    return float(val_str)
                except (ValueError, IndexError):
                    return None
            return None

        def extract_int(pattern, text):
            val = extract_float(pattern, text)
            return int(val) if val is not None else None

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ–ª—è–º –ë–î
        characteristics = {
            'area_sqm': ad_object.area_sqm or extract_float(r'(\d+[.,]?\d*)\s*(?:–º2|–∫–≤\. ?–º|–∫–≤–∞–¥—Ä–∞—Ç)', text),
            'rooms': ad_object.rooms or extract_int(r'(\d+)\s*-?\s*(?:–∫–æ–º–Ω|–∫–æ–º–Ω–∞—Ç|–∫\.)', text),
            'floor': ad_object.floor or extract_int(r'—ç—Ç–∞–∂\s*[:\-]?\s*(\d+)', text),
            'total_floors': ad_object.total_floors or extract_int(r'(\d+)\s*—ç—Ç–∞–∂–Ω|–∏–∑\s*(\d+)', text),
            'land_area_sotka': ad_object.land_area_sotka or extract_float(r'(\d+[.,]?\d*)\s*(?:—Å–æ—Ç|—Å–æ—Ç–æ–∫|—Å–æ—Ç–∫–∞)', text),
            'property_type': getattr(ad_object, 'property_type', None),
            'listing_type': getattr(ad_object, 'listing_type', None),
            'attributes': getattr(ad_object, 'attributes', {}),  # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        }
        return characteristics

    def _get_text_embeddings(self, ad: DBAd, characteristics: Dict) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –æ–±–æ–≥–∞—â–∞—è —Ç–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏."""
        if self.text_model is None:
            logger.warning("Text model not available, returning empty embedding")
            return np.array([])
            
        text_parts = [
            ad.title.strip() if ad.title else "",
            ad.description.strip() if ad.description else ""
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        char_text = []
        if characteristics.get('rooms') is not None: char_text.append(f"{characteristics['rooms']} –∫–æ–º–Ω")
        if characteristics.get('area_sqm') is not None: char_text.append(f"{characteristics['area_sqm']} –∫–≤.–º")
        if characteristics.get('floor') is not None: char_text.append(f"—ç—Ç–∞–∂ {characteristics['floor']}")
        
        if char_text:
            text_parts.append(" ".join(char_text))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        filtered_parts = [part for part in text_parts if part is not None and part.strip()]
        full_text = ' '.join(filtered_parts)
        full_text = ' '.join(full_text.split())
        
        if not full_text.strip():
            logger.warning("Empty text for embedding, returning empty array")
            return np.array([])
            
        try:
            return self.text_model.encode(full_text)
        except Exception as e:
            logger.error(f"Error encoding text: {e}")
            return np.array([])
    
    def _find_similar_unique_ads(
        self,
        ad: DBAd,
        ad_characteristics: Dict,
        ad_photo_hashes: List[Dict[str, str]],
        text_embeddings: np.ndarray
    ) -> List[Tuple[DBUniqueAd, float]]:
        
        # –®–∞–≥ 1: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª—è–º –ë–î (–±—ã—Å—Ç—Ä–∞—è)
        base_query = self.db.query(DBUniqueAd)
        if ad.location_id:
            base_query = base_query.filter(DBUniqueAd.location_id == ad.location_id)
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–≥—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–æ–º–Ω–∞—Ç–∞–º - —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ –≤—Å–µ–º–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏
        
        candidate_ads = base_query.all()
        logger.info(f"Found {len(candidate_ads)} candidates after initial DB filtering.")
        if not candidate_ads:
            return []

        # –®–∞–≥ 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        semantic_candidates = self._find_semantic_candidates(
            candidate_ads, text_embeddings, top_k=self.config['semantic_top_k']
        )
        logger.info(f"Found {len(semantic_candidates)} semantic candidates.")
        
        # –®–∞–≥ 3: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
        similar_ads = []
        for unique_ad, semantic_sim in semantic_candidates:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            unique_ad_characteristics = self._get_unified_characteristics(unique_ad)
            
            # –ù–û–í–´–ô –≠–¢–ê–ü: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.
            if not self._check_critical_match(ad_characteristics, unique_ad_characteristics):
                logger.warning(f"Critical characteristics mismatch for ad {ad.id} vs unique {unique_ad.id}. Skipping.")
                continue
            
            # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–≤–µ—Ä–∫—É, —Å—á–∏—Ç–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
            characteristics_sim = self._calculate_property_characteristics_similarity(
                ad_characteristics, unique_ad_characteristics
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏ –¥–ª—è unique_ad
            unique_ad_photo_hashes = [photo.perceptual_hashes for photo in unique_ad.photos if photo.perceptual_hashes]
            
            # –ü–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
            perceptual_photo_sim = self._calculate_photo_similarity(ad_photo_hashes, unique_ad_photo_hashes)
            
            # –ü–æ–ª—É—á–∞–µ–º CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–æ—Ç–∫–ª—é—á–µ–Ω–æ)
            # ad_clip_embeddings = [photo.clip_embedding for photo in ad.photos if photo.clip_embedding]
            # unique_ad_clip_embeddings = [photo.clip_embedding for photo in unique_ad.photos if photo.clip_embedding]
            
            # CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–æ—Ç–∫–ª—é—á–µ–Ω–æ)
            clip_photo_sim = 0.0
            # if self.clip_model and ad_clip_embeddings and unique_ad_clip_embeddings:
            #     clip_photo_sim = self._calculate_clip_embedding_similarity(
            #         ad_clip_embeddings, unique_ad_clip_embeddings
            #     )
            
            text_sim = self._calculate_text_similarity(
                text_embeddings,
                np.array(unique_ad.text_embeddings) if unique_ad.text_embeddings else np.array([])
            )
            address_sim = self._calculate_address_similarity_with_unique(ad, unique_ad)
            
            weights = self.config['weights']
            overall_sim = (
                characteristics_sim * weights['characteristics'] + 
                perceptual_photo_sim * weights['perceptual_photos'] + 
                clip_photo_sim * weights['clip_photos'] + 
                text_sim * weights['text'] + 
                address_sim * weights['address']
            )
            
            logger.info(f"Detailed analysis for ad {ad.id} vs unique {unique_ad.id}: "
                       f"Characteristics: {characteristics_sim:.2f}, "
                       f"Perceptual Photos: {perceptual_photo_sim:.2f}, "
                       f"CLIP Photos: {clip_photo_sim:.2f}, "
                       f"Text: {text_sim:.2f}, Address: {address_sim:.2f}, "
                       f"Overall: {overall_sim:.2f}")
            
            # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ï —É—Å–ª–æ–≤–∏–µ - —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
            photo_sim_combined = perceptual_photo_sim  # –¢–æ–ª—å–∫–æ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏
            
            logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ç–æ –¥–ª—è ad {ad.id} vs unique {unique_ad.id}: "
                       f"photo_sim={photo_sim_combined:.3f}, required_threshold={self.config['photo_required_threshold']}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
            if photo_sim_combined >= self.config['photo_required_threshold']:  # –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
                if (characteristics_sim >= self.config['characteristics_similarity_threshold'] and 
                    photo_sim_combined >= self.config['photo_similarity_threshold'] and 
                    overall_sim > self.config['similarity_threshold']):
                    similar_ads.append((unique_ad, overall_sim))
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º —Ñ–æ—Ç–æ: {photo_sim_combined:.3f}")
                else:
                    logger.info(f"‚ùå –ù–µ –ø—Ä–æ—à–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏: characteristics={characteristics_sim:.3f}, "
                              f"photo_threshold={self.config['photo_similarity_threshold']}, overall={overall_sim:.3f}")
            else:
                # –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π - –ù–ï –¥—É–±–ª–∏–∫–∞—Ç
                logger.info(f"‚ùå –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –¥–ª—è ad {ad.id} vs unique {unique_ad.id} - –ù–ï –¥—É–±–ª–∏–∫–∞—Ç")
        
        return sorted(similar_ads, key=lambda x: x[1], reverse=True)
    
    def _find_semantic_candidates(
        self,
        candidate_ads: List[DBUniqueAd],
        text_embeddings: np.ndarray,
        top_k: int
    ) -> List[Tuple[DBUniqueAd, float]]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ–ø-K —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        semantic_scores = []
        for unique_ad in candidate_ads:
            if unique_ad.text_embeddings is not None and len(unique_ad.text_embeddings) > 0:
                unique_text_embeddings = np.array(unique_ad.text_embeddings)
                semantic_sim = self._calculate_text_similarity(text_embeddings, unique_text_embeddings)
                if semantic_sim >= self.config['semantic_threshold']:
                    semantic_scores.append((unique_ad, semantic_sim))
        
        semantic_scores.sort(key=lambda x: x[1], reverse=True)
        return semantic_scores[:top_k]

    def _check_critical_match(self, char1: Dict, char2: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π."""
        # 1. –ü–ª–æ—â–∞–¥—å - –±–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        area1, area2 = char1.get('area_sqm'), char2.get('area_sqm')
        if area1 is not None and area2 is not None:
            try:
                area1_float = float(area1)
                area2_float = float(area2)
                tolerance = area1_float * (self.config['area_tolerance_percent'] / 100.0)
                if abs(area1_float - area2_float) > tolerance:
                    logger.debug(f"Critical mismatch: area {area1_float} vs {area2_float}")
                    return False # –ü–ª–æ—â–∞–¥—å –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
            except (TypeError, ValueError) as e:
                logger.warning(f"Error comparing areas {area1} vs {area2}: {e}")
                # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Å—Ä–∞–≤–Ω–∏—Ç—å –ø–ª–æ—â–∞–¥–∏, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É - –µ—Å–ª–∏ —É –æ–¥–Ω–æ–≥–æ –µ—Å—Ç—å –ø–ª–æ—â–∞–¥—å, –∞ —É –¥—Ä—É–≥–æ–≥–æ –Ω–µ—Ç, –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º

        # 2. –ö–æ–º–Ω–∞—Ç—ã - –±–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        rooms1, rooms2 = char1.get('rooms'), char2.get('rooms')
        if rooms1 is not None and rooms2 is not None:
            if rooms1 != rooms2:
                logger.debug(f"Critical mismatch: rooms {rooms1} vs {rooms2}")
                return False # –ö–æ–º–Ω–∞—Ç—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É - –µ—Å–ª–∏ —É –æ–¥–Ω–æ–≥–æ –µ—Å—Ç—å –∫–æ–º–Ω–∞—Ç—ã, –∞ —É –¥—Ä—É–≥–æ–≥–æ –Ω–µ—Ç, –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º

        # 3. –≠—Ç–∞–∂ - –±–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        floor1, floor2 = char1.get('floor'), char2.get('floor')
        if floor1 is not None and floor2 is not None:
            if abs(floor1 - floor2) > self.config['floor_tolerance_abs']:
                logger.debug(f"Critical mismatch: floor {floor1} vs {floor2}")
                return False # –≠—Ç–∞–∂–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É - –µ—Å–ª–∏ —É –æ–¥–Ω–æ–≥–æ –µ—Å—Ç—å —ç—Ç–∞–∂, –∞ —É –¥—Ä—É–≥–æ–≥–æ –Ω–µ—Ç, –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
            
        # 4. –¢–∏–ø –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ (–µ—Å–ª–∏ –æ–±–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç)
        type1, type2 = char1.get('property_type'), char2.get('property_type')
        if type1 is not None and type2 is not None and type1 != type2:
            logger.debug(f"Critical mismatch: property_type {type1} vs {type2}")
            return False

        # 5. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–û–í–ï–†–ö–ò –î–õ–Ø –ì–ê–†–ê–ñ–ï–ô
        if type1 == '–ì–∞—Ä–∞–∂' or type2 == '–ì–∞—Ä–∞–∂':
            # –î–ª—è –≥–∞—Ä–∞–∂–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º building_type (–º–∞—Ç–µ—Ä–∏–∞–ª) –∏ condition
            attrs1, attrs2 = char1.get('attributes', {}), char2.get('attributes', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º building_type (–º–∞—Ç–µ—Ä–∏–∞–ª)
            building_type1 = attrs1.get('building_type') or attrs1.get('material')
            building_type2 = attrs2.get('building_type') or attrs2.get('material')
            if building_type1 and building_type2 and building_type1 != building_type2:
                logger.debug(f"Critical mismatch for garage: building_type {building_type1} vs {building_type2}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º condition (—Å–æ—Å—Ç–æ—è–Ω–∏–µ)
            condition1 = attrs1.get('condition')
            condition2 = attrs2.get('condition')
            if condition1 and condition2 and condition1 != condition2:
                logger.debug(f"Critical mismatch for garage: condition {condition1} vs {condition2}")
                return False

        # 6. –û–ë–©–ò–ï –ü–†–û–í–ï–†–ö–ò –î–õ–Ø –í–°–ï–• –¢–ò–ü–û–í –ù–ï–î–í–ò–ñ–ò–ú–û–°–¢–ò
        attrs1, attrs2 = char1.get('attributes', {}), char2.get('attributes', {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º building_type (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö)
        building_type1 = attrs1.get('building_type')
        building_type2 = attrs2.get('building_type')
        if building_type1 and building_type2 and building_type1 != building_type2:
            logger.debug(f"Critical mismatch: building_type {building_type1} vs {building_type2}")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º condition (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö)
        condition1 = attrs1.get('condition')
        condition2 = attrs2.get('condition')
        if condition1 and condition2 and condition1 != condition2:
            logger.debug(f"Critical mismatch: condition {condition1} vs {condition2}")
            return False

        return True # –í—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã
    
    def _calculate_photo_similarity(self, hashes1: List[Dict[str, str]], hashes2: List[Dict[str, str]]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ö–µ—à–µ–π - –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"""
        if not hashes1 or not hashes2: 
            logger.debug("–ü—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ —Ö–µ—à–µ–π –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return 0.0
        
        # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ò—â–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        best_similarity = 0.0
        found_match = False
        
        for hash_dict1 in hashes1:
            for hash_dict2 in hashes2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ö–µ—à–µ–π
                if not isinstance(hash_dict1, dict) or not isinstance(hash_dict2, dict):
                    logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Ö–µ—à–∏: {type(hash_dict1)} vs {type(hash_dict2)}")
                    continue
                    
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ —Ö–µ—à–∏ (pHash –∏ dHash –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã)
                for hash_type in ['pHash', 'dHash']:
                    if hash_type in hash_dict1 and hash_type in hash_dict2:
                        hash1 = hash_dict1[hash_type]
                        hash2 = hash_dict2[hash_type]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ö–µ—à–µ–π
                        if hash1 and hash2 and isinstance(hash1, str) and isinstance(hash2, str):
                            try:
                                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –•—ç–º–º–∏–Ω–≥–∞
                                distance = sum(c1 != c2 for c1, c2 in zip(hash1, hash2))
                                max_distance = len(hash1)
                                similarity = 1.0 - (distance / max_distance)
                                
                                logger.info(f"üîç {hash_type}: hash1={hash1[:8]}..., hash2={hash2[:8]}..., "
                                          f"distance={distance}, similarity={similarity:.3f}")
                                
                                # –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                                if similarity > best_similarity:
                                    best_similarity = similarity
                                    found_match = True
                                    logger.info(f"üéØ –ù–æ–≤–æ–µ –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ {hash_type}: {similarity:.3f}")
                                    
                                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
                                if similarity >= self.config['photo_early_stop_threshold']:
                                    logger.info(f"üèÜ –ù–∞–π–¥–µ–Ω–æ –æ—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ {hash_type}: {similarity:.3f}")
                                    return similarity
                                    
                            except Exception as e:
                                logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –•—ç–º–º–∏–Ω–≥–∞ –¥–ª—è {hash_type}: {e}")
                                continue
                        else:
                            logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Ö–µ—à–∏ {hash_type}: {hash1} vs {hash2}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–µ–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        result = best_similarity if found_match else 0.0
        logger.info(f"üì∏ –°—Ö–æ–∂–µ—Å—Ç—å —Ñ–æ—Ç–æ: {result:.3f} (–Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {found_match})")
        return result
    
    def _calculate_clip_embedding_similarity(self, embeddings1: List[np.ndarray], embeddings2: List[np.ndarray]) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
        Args:
            embeddings1: CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            embeddings2: CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Ç–æ—Ä–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            
        Returns:
            –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        if not embeddings1 or not embeddings2:
            logger.debug("–ü—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return 0.0
        
        if self.clip_model is None:
            logger.warning("CLIP –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏")
            return 0.0
        
        total_similarity = 0.0
        total_comparisons = 0
        
        for emb1 in embeddings1:
            for emb2 in embeddings2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                if emb1 is not None and emb2 is not None:
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                        if len(emb1) == 0 or len(emb2) == 0:
                            logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
                            continue
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                        if not isinstance(emb1, np.ndarray):
                            emb1 = np.array(emb1, dtype=np.float32)
                        if not isinstance(emb2, np.ndarray):
                            emb2 = np.array(emb2, dtype=np.float32)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã
                        if emb1.ndim != 1 or emb2.ndim != 1:
                            logger.warning(f"–ù–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {emb1.shape} vs {emb2.shape}")
                            continue
                        
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                        norm1 = np.linalg.norm(emb1)
                        norm2 = np.linalg.norm(emb2)
                        
                        if norm1 == 0 or norm2 == 0:
                            logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω—É–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
                            continue
                        
                        emb1_norm = emb1 / norm1
                        emb2_norm = emb2 / norm2
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                        similarity = np.dot(emb1_norm, emb2_norm)
                        total_similarity += similarity
                        total_comparisons += 1
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è CLIP —Å—Ö–æ–∂–µ—Å—Ç–∏: {e}")
                        continue
                else:
                    logger.debug("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º None —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
        
        result = total_similarity / total_comparisons if total_comparisons > 0 else 0.0
        logger.debug(f"–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {result:.3f} (—Å—Ä–∞–≤–Ω–µ–Ω–∏–π: {total_comparisons})")
        return result
    
    def _calculate_text_similarity(self, emb1, emb2) -> float:
        if emb1 is None or emb2 is None or len(emb1) == 0 or len(emb2) == 0: return 0.0
        if not isinstance(emb1, np.ndarray): emb1 = np.array(emb1, dtype=np.float32)
        if not isinstance(emb2, np.ndarray): emb2 = np.array(emb2, dtype=np.float32)
        if emb1.shape != emb2.shape:
            min_len = min(len(emb1), len(emb2))
            emb1, emb2 = emb1[:min_len], emb2[:min_len]
        norm1, norm2 = np.linalg.norm(emb1), np.linalg.norm(emb2)
        if norm1 == 0 or norm2 == 0: return 0.0
        return float(np.dot(emb1, emb2) / (norm1 * norm2))
    
    def _calculate_contact_similarity(self, phones1: List[str], phones2: List[str]) -> float:
        if not phones1 or not phones2: return 0.0
        normalize = lambda p: ''.join(filter(str.isdigit, p))
        s1, s2 = set(map(normalize, phones1)), set(map(normalize, phones2))
        return len(s1 & s2) / len(s1 | s2) if s1 | s2 else 0.0

    def _calculate_address_similarity_with_unique(self, ad: DBAd, unique_ad: DBUniqueAd) -> float:
        if not ad.location or not unique_ad.location: return 0.0
        c1 = [ad.location.city, ad.location.district, ad.location.address]
        c2 = [unique_ad.location.city, unique_ad.location.district, unique_ad.location.address]
        matches = sum(1 for v1, v2 in zip(c1, c2) if v1 and v2 and v1 == v2)
        total = sum(1 for v1, v2 in zip(c1, c2) if v1 or v2) # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—è
        return matches / total if total > 0 else 0.0
    
    def _calculate_property_characteristics_similarity(self, char1: Dict, char2: Dict) -> float:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫."""
        scores = []
        weights_sum = 0.0
        
        def compare(key, weight, tolerance=0):
            nonlocal weights_sum
            val1, val2 = char1.get(key), char2.get(key)
            
            if val1 is not None and val2 is not None:
                weights_sum += weight
                try:
                    is_match = abs(float(val1) - float(val2)) <= tolerance
                except (TypeError, ValueError):
                    is_match = str(val1) == str(val2)
                scores.append(1.0 * weight if is_match else 0.0)
            # –ï—Å–ª–∏ –ø–æ–ª—è –Ω–µ—Ç –≤ –æ–±–æ–∏—Ö, –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ–º –≤ –≤–µ—Å–µ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ scores
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º, —Ç–æ —ç—Ç–æ —É–∂–µ –æ—Ç–ª–æ–≤–ª–µ–Ω–æ –≤ _check_critical_match

        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ø–æ–ª—è –ë–î)
        area1_val = char1.get('area_sqm')
        if area1_val is not None:
            try:
                area1_float = float(area1_val)
                tolerance = area1_float * (self.config['area_tolerance_percent'] / 100.0)
                compare('area_sqm', 1.0, tolerance)
            except (TypeError, ValueError):
                compare('area_sqm', 1.0, 0)  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å tolerance, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
        else:
            compare('area_sqm', 1.0, 0)  # –ï—Å–ª–∏ –Ω–µ—Ç –ø–ª–æ—â–∞–¥–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º tolerance 0
        compare('rooms', 1.0)
        compare('floor', 0.8, self.config['floor_tolerance_abs'])
        compare('total_floors', 0.7)
        compare('property_type', 0.9)
        compare('land_area_sotka', 1.0)  # –ü–ª–æ—â–∞–¥—å —É—á–∞—Å—Ç–∫–∞ –≤ —Å–æ—Ç–∫–∞—Ö - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —É—á–∞—Å—Ç–∫–æ–≤
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        attributes_score = self._calculate_attributes_similarity(char1.get('attributes', {}), char2.get('attributes', {}))
        if attributes_score > 0:
            weights_sum += 0.5  # –í–µ—Å –¥–ª—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤
            scores.append(attributes_score * 0.5)
        
        return sum(scores) / weights_sum if weights_sum > 0 else 0.0

    def _calculate_attributes_similarity(self, attrs1: Dict, attrs2: Dict) -> float:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑ JSONB –ø–æ–ª—è –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        if not attrs1 or not attrs2:
            return 0.0
        
        # –í–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—Å –≤–µ—Å–∞–º–∏)
        important_attrs = {
            'utilities': 1.0,           # –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏
            'heating': 0.8,             # –û—Ç–æ–ø–ª–µ–Ω–∏–µ
            'condition': 0.8,           # –†–µ–º–æ–Ω—Ç
            'furniture': 0.7,           # –ú–µ–±–µ–ª—å
            'building_type': 0.9,       # –¢–∏–ø –∑–¥–∞–Ω–∏—è
            'offer_type': 0.6,          # –¢–∏–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            'purpose': 0.8,             # –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ (–¥–ª—è —É—á–∞—Å—Ç–∫–æ–≤)
            'material': 0.7,            # –ú–∞—Ç–µ—Ä–∏–∞–ª (–¥–ª—è –≥–∞—Ä–∞–∂–µ–π)
            'height': 0.6,              # –í—ã—Å–æ—Ç–∞ (–¥–ª—è –≥–∞—Ä–∞–∂–µ–π)
            'capacity': 0.7,            # –í–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (–¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä)
            'amenities': 0.6,           # –£–¥–æ–±—Å—Ç–≤–∞ (–¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä)
            'housing_class': 0.5,       # –ö–ª–∞—Å—Å –∂–∏–ª—å—è (–¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä)
            'additional_features': 0.5,  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            'subletting': 0.4,          # –ü–æ–¥—Å–µ–ª–µ–Ω–∏–µ
            'pets': 0.3,                # –ñ–∏–≤–æ—Ç–Ω—ã–µ
            'parking': 0.5,             # –ü–∞—Ä–∫–∏–Ω–≥
            'documents': 0.6,           # –î–æ–∫—É–º–µ–Ω—Ç—ã
        }
        total_score = 0.0
        total_weight = 0.0
        for attr_name, weight in important_attrs.items():
            val1 = attrs1.get(attr_name)
            val2 = attrs2.get(attr_name)
            
            if val1 is not None and val2 is not None:
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤
                if isinstance(val1, str) and isinstance(val2, str):
                    val1_lower = val1.lower()
                    val2_lower = val2.lower()

                    if val1_lower == val2_lower:
                        score = 1.0
                    elif val1_lower in val2_lower or val2_lower in val1_lower:
                        score = 0.7
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
                        words1 = set(val1_lower.split())
                        words2 = set(val2_lower.split())
                        if words1 and words2:
                            common_words = words1.intersection(words2)
                            score = len(common_words) / max(len(words1), len(words2))
                        else:
                            score = 0.0
                else:
                    # –î–ª—è –Ω–µ—Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    score = 1.0 if val1 == val2 else 0.0
                
                total_score += score * weight
                total_weight += weight
        
        return total_score / total_weight if total_weight > 0 else 0.0
    
    def _handle_duplicate(
        self,
        ad: DBAd,
        unique_ad: DBUniqueAd,
        similarity: float
    ):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç –ë–ï–ó –û–ë–ù–û–í–õ–ï–ù–ò–Ø –£–ù–ò–ö–ê–õ–¨–ù–û–ì–û –û–ë–™–Ø–í–õ–ï–ù–ò–Ø"""
        ad_photo_hashes = [photo.perceptual_hashes for photo in ad.photos 
                          if photo.perceptual_hashes and isinstance(photo.perceptual_hashes, dict)]
        unique_ad_photo_hashes = [photo.perceptual_hashes for photo in unique_ad.photos 
                                 if photo.perceptual_hashes and isinstance(photo.perceptual_hashes, dict)]
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        ad_characteristics = self._get_unified_characteristics(ad)
        unique_ad_characteristics = self._get_unified_characteristics(unique_ad)

        characteristics_sim = self._calculate_property_characteristics_similarity(
            ad_characteristics, unique_ad_characteristics
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î
        perceptual_photo_sim = self._calculate_photo_similarity(ad_photo_hashes, unique_ad_photo_hashes)
        
        # –ü–æ–ª—É—á–∞–µ–º CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–æ—Ç–∫–ª—é—á–µ–Ω–æ)
        # ad_clip_embeddings = [photo.clip_embedding for photo in ad.photos 
        #                      if photo.clip_embedding and isinstance(photo.clip_embedding, list)]
        # unique_ad_clip_embeddings = [photo.clip_embedding for photo in unique_ad.photos 
        #                             if photo.clip_embedding and isinstance(photo.clip_embedding, list)]
        
        # CLIP —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–æ—Ç–∫–ª—é—á–µ–Ω–æ)
        clip_photo_sim = 0.0
        # if self.clip_model and ad_clip_embeddings and unique_ad_clip_embeddings:
        #     clip_photo_sim = self._calculate_clip_embedding_similarity(
        #         ad_clip_embeddings, unique_ad_clip_embeddings
        #     )
        
        # –û–±—â–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (—Ç–æ–ª—å–∫–æ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–µ —Ö–µ—à–∏)
        photo_sim_combined = perceptual_photo_sim
        
        text_sim = self._calculate_text_similarity(
            self._get_text_embeddings(ad, ad_characteristics),
            np.array(unique_ad.text_embeddings) if unique_ad.text_embeddings else np.array([])
        )
        contact_sim = self._calculate_contact_similarity(ad.phone_numbers, unique_ad.phone_numbers)
        address_sim = self._calculate_address_similarity_with_unique(ad, unique_ad)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã –≤ –æ–±—ã—á–Ω—ã–µ float –¥–ª—è PostgreSQL
        duplicate = DBAdDuplicate(
            unique_ad_id=unique_ad.id,
            original_ad_id=ad.id,
            photo_similarity=float(photo_sim_combined),
            text_similarity=float(text_sim),
            contact_similarity=float(contact_sim),
            address_similarity=float(address_sim),
            characteristics_similarity=float(characteristics_sim),
            overall_similarity=float(similarity)
        )
        self.db.add(duplicate)
        
        ad.is_duplicate = True
        ad.duplicate_info = duplicate
        ad.unique_ad_id = unique_ad.id
        unique_ad.duplicates_count = (unique_ad.duplicates_count or 0) + 1
        
        logger.info(f"Ad {ad.id} marked as duplicate of unique ad {unique_ad.id}. "
                   f"Similarities: Perceptual={perceptual_photo_sim:.2f}, CLIP={clip_photo_sim:.2f}, "
                   f"Text={text_sim:.2f}, Characteristics={characteristics_sim:.2f}, "
                   f"Overall={similarity:.2f}. Duplicates count: {unique_ad.duplicates_count}")
        if event_emitter:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
                import asyncio
                from app.services.event_emitter import EventType
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ —Ç–µ–∫—É—â–µ–º event loop
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    loop.create_task(event_emitter.emit(EventType.DUPLICATE_DETECTED, {'ad_id': ad.id, 'unique_ad_id': unique_ad.id}))
                else:
                    # –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
                    logger.info(f"Event loop not running, skipping event emission")
            except Exception as e:
                logger.warning(f"Failed to emit event: {e}")
    
    def _create_unique_ad(
        self,
        ad: DBAd,
        ad_photo_hashes: List[Dict[str, str]],
        text_embeddings: np.ndarray
    ) -> DBUniqueAd:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ"""
        unique_ad = DBUniqueAd(
            title=ad.title,
            description=ad.description,
            price=ad.price,
            price_original=ad.price_original,
            currency=ad.currency,
            phone_numbers=ad.phone_numbers,
            rooms=ad.rooms,
            area_sqm=ad.area_sqm,
            land_area_sotka=ad.land_area_sotka,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–ª–æ—â–∞–¥—å —É—á–∞—Å—Ç–∫–∞
            floor=ad.floor,
            total_floors=ad.total_floors,
            series=ad.series,
            building_type=ad.building_type,
                            condition=ad.condition,
            furniture=ad.furniture,
            heating=ad.heating,
            hot_water=ad.hot_water,
            gas=ad.gas,
            ceiling_height=ad.ceiling_height,
            location_id=ad.location_id,
            attributes=ad.attributes,
            text_embeddings=text_embeddings.tolist(),
            confidence_score=1.0,
            duplicates_count=0,
            base_ad_id=ad.id,
            property_type=ad.property_type,
            listing_type=ad.listing_type
        )
        self.db.add(unique_ad)
        self.db.flush()  
        self.db.refresh(unique_ad)
        
        for photo in ad.photos:
            unique_photo = DBUniquePhoto(url=photo.url, perceptual_hashes=photo.perceptual_hashes, clip_embedding=photo.clip_embedding, unique_ad_id=unique_ad.id)
            self.db.add(unique_photo)
            
        ad.is_duplicate = False
        ad.unique_ad_id = unique_ad.id
        logger.info(f"Created new unique ad {unique_ad.id} from base ad {ad.id}.")
        if event_emitter:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤
                import asyncio
                from app.services.event_emitter import EventType
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ —Ç–µ–∫—É—â–µ–º event loop
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    loop.create_task(event_emitter.emit(EventType.NEW_AD_CREATED, {'unique_ad_id': unique_ad.id, 'base_ad_id': ad.id}))
                else:
                    # –ï—Å–ª–∏ loop –Ω–µ –∑–∞–ø—É—â–µ–Ω, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
                    logger.info(f"Event loop not running, skipping event emission")
            except Exception as e:
                logger.warning(f"Failed to emit event: {e}")
        
        return unique_ad

    def get_base_ad_for_unique(self, unique_ad_id: int) -> Optional[DBAd]:
        unique_ad = self.db.query(DBUniqueAd).filter(DBUniqueAd.id == unique_ad_id).first()
        if not unique_ad: return None
        if hasattr(unique_ad, 'base_ad_id') and unique_ad.base_ad_id:
            return self.db.query(DBAd).filter(DBAd.id == unique_ad.base_ad_id).first()
        return self.db.query(DBAd).filter(DBAd.unique_ad_id == unique_ad.id, DBAd.is_duplicate == False).first()

    def get_all_ads_for_unique(self, unique_ad_id: int) -> Dict[str, List[DBAd]]:
        base_ad = self.get_base_ad_for_unique(unique_ad_id)
        duplicates = self.db.query(DBAdDuplicate).filter(DBAdDuplicate.unique_ad_id == unique_ad_id).all()
        duplicate_ads = [self.db.query(DBAd).get(dup.original_ad_id) for dup in duplicates if self.db.query(DBAd).get(dup.original_ad_id)]
        return {
            'base_ad': [base_ad] if base_ad else [],
            'duplicates': duplicate_ads,
            'total_count': (1 if base_ad else 0) + len(duplicate_ads)
        }

    def detect_realtors(self):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏ —Å–æ–∑–¥–∞–µ—Ç –∏—Ö –ø—Ä–æ—Ñ–∏–ª–∏"""
        from app.database.db_models import DBRealtor
        
        # 1. –ò—â–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        phone_groups = self.db.query(
            func.jsonb_array_elements_text(DBAd.phone_numbers).label("phone_number"),
            func.count(DBAd.id).label("ad_count")
        ) \
            .filter(DBAd.phone_numbers.isnot(None)) \
            .group_by(func.jsonb_array_elements_text(DBAd.phone_numbers)) \
            .having(func.count(DBAd.id) > self.realtor_threshold) \
            .all()

        current_realtor_phones = {pg.phone_number: pg.ad_count for pg in phone_groups}

        # 2. –£–±–∏—Ä–∞–µ–º realtor_id —É –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Ç–µ—Ö, –∫—Ç–æ –±–æ–ª—å—à–µ –Ω–µ —Ä–∏—ç–ª—Ç–æ—Ä
        existing_realtors = self.db.query(DBRealtor).all()
        for realtor in existing_realtors:
            if realtor.phone_number not in current_realtor_phones:
                # –£ —ç—Ç–æ–≥–æ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞–ª–æ –º–µ–Ω—å—à–µ 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
                self.db.query(DBUniqueAd).filter(
                    DBUniqueAd.realtor_id == realtor.id
                ).update({DBUniqueAd.realtor_id: None})
                
                # –£–¥–∞–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å —Ä–∏—ç–ª—Ç–æ—Ä–∞
                self.db.delete(realtor)
                logger.info(f"Removed realtor status from phone: {realtor.phone_number}")

        if not current_realtor_phones:
            logger.info("No realtors detected based on phone number threshold.")
            self.db.commit()
            return

        logger.info(f"Detected {len(current_realtor_phones)} potential realtor phone numbers.")
        
        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –Ω–æ–º–µ—Ä —Ä–∏—ç–ª—Ç–æ—Ä–∞
        for phone_number, total_ads_count in current_realtor_phones.items():
            try:
                self._process_realtor_phone(phone_number, total_ads_count)
            except Exception as e:
                logger.error(f"Error processing realtor phone {phone_number}: {e}")
                continue

        self.db.commit()
        logger.info("Realtor detection complete.")

    def _process_realtor_phone(self, phone_number: str, total_ads_count: int):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ —Ä–∏—ç–ª—Ç–æ—Ä–∞: —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        from app.database.db_models import DBRealtor
        
        # 1. –ù–∞—Ö–æ–¥–∏–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å —Ä–∏—ç–ª—Ç–æ—Ä–∞
        realtor = self.db.query(DBRealtor).filter(
            DBRealtor.phone_number == phone_number
        ).first()
        
        if not realtor:
            realtor = DBRealtor(
                phone_number=phone_number,
                total_ads_count=total_ads_count,
                created_at=datetime.utcnow()
            )
            self.db.add(realtor)
            self.db.flush()  # –ü–æ–ª—É—á–∞–µ–º ID
            logger.info(f"Created new realtor profile for phone: {phone_number}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –æ–± –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —Ä–∏—ç–ª—Ç–æ—Ä–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
            try:
                if event_emitter:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É event loop
                    pass  # –£–±–∏—Ä–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤, —Ç–∞–∫ –∫–∞–∫ –º—ã –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            except Exception as e:
                logger.warning(f"Could not emit realtor detected event: {e}")
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            realtor.total_ads_count = total_ads_count
            realtor.updated_at = datetime.utcnow()
        
        # 2. –ù–∞—Ö–æ–¥–∏–º –∏ —Å–≤—è–∑—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å —Ä–∏—ç–ª—Ç–æ—Ä–æ–º
        unique_ads_to_update = (
            self.db.query(DBUniqueAd)
            .join(DBAd, DBAd.unique_ad_id == DBUniqueAd.id)
            .filter(DBAd.phone_numbers.op("@>")(f'["{phone_number}"]'))
            .distinct()
            .all()
        )

        for unique_ad in unique_ads_to_update:
            unique_ad.realtor_id = realtor.id
        
        logger.info(f"Linked {len(unique_ads_to_update)} unique ads to realtor: {phone_number}")

    def get_duplicate_statistics(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º"""
        total_unique_ads = self.db.query(DBUniqueAd).count()
        total_original_ads = self.db.query(DBAd).count()
        duplicate_ads = self.db.query(DBAd).filter(DBAd.is_duplicate == True).count()
        base_ads = self.db.query(DBAd).filter(DBAd.is_duplicate == False).count()
        unique_ads_with_duplicates = self.db.query(DBUniqueAd).filter(
            DBUniqueAd.duplicates_count > 0
        ).count()

        avg_duplicates = self.db.query(func.avg(DBUniqueAd.duplicates_count)).scalar() or 0
        
        return {
            'total_unique_ads': total_unique_ads,
            'total_original_ads': total_original_ads,
            'base_ads': base_ads,
            'duplicate_ads': duplicate_ads,
            'unique_ads_with_duplicates': unique_ads_with_duplicates,
            'avg_duplicates_per_unique': float(avg_duplicates),
            'deduplication_ratio': (duplicate_ads / total_original_ads * 100) if total_original_ads > 0 else 0
        }

    def get_realtor_statistics(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–∏—ç–ª—Ç–æ—Ä–∞–º –∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è–º"""
        from app.database.db_models import DBRealtor
        
        total_realtors = self.db.query(DBRealtor).count()
        realtor_unique_ads = self.db.query(DBUniqueAd).filter(
            DBUniqueAd.realtor_id.isnot(None)
        ).count()
        realtor_original_ads = self.db.query(DBAd).filter(
            DBAd.realtor_id.isnot(None)
        ).count()
        total_unique_ads = self.db.query(DBUniqueAd).count()
        total_original_ads = self.db.query(DBAd).count()
        # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π –æ—Ç —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤
        realtor_percentage = (realtor_unique_ads / total_unique_ads * 100) if total_unique_ads > 0 else 0
        return {
            'total_realtors': total_realtors,
            'realtor_unique_ads': realtor_unique_ads,
            'realtor_original_ads': realtor_original_ads,
            'total_unique_ads': total_unique_ads,
            'total_original_ads': total_original_ads,
            'realtor_percentage': float(realtor_percentage)
        }



