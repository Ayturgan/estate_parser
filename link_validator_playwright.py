import requests
import asyncio
import aiohttp
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from concurrent.futures import ThreadPoolExecutor
import time
from collections import defaultdict
import re
from datetime import datetime

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = "postgresql://estate_user:admin123@localhost:5432/estate_db"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    db = SessionLocal()
    try:
        return db
    except Exception as e:
        db.close()
        raise e

def check_url_status(url):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ URL"""
    try:
        response = requests.head(url, allow_redirects=True, timeout=10)
        code = response.status_code
        if code == 200:
            return "‚úÖ OK"
        elif code == 404:
            return "‚ùå Not Found"
        elif code == 410:
            return "‚ò†Ô∏è Gone"
        elif code == 302:
            return f"‚Ü™Ô∏è Redirect ‚Üí {response.headers.get('Location', '')}"
        else:
            return f"‚ö†Ô∏è {code}"
    except Exception as e:
        return f"üî• Error: {str(e)}"

async def check_url_status_async(session, url):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ URL"""
    try:
        async with session.head(url, allow_redirects=True, timeout=aiohttp.ClientTimeout(total=10)) as response:
            code = response.status
            if code == 200:
                return url, "‚úÖ OK"
            elif code == 404:
                return url, "‚ùå Not Found"
            elif code == 410:
                return url, "‚ò†Ô∏è Gone"
            elif code == 302:
                location = response.headers.get('Location', '')
                return url, f"‚Ü™Ô∏è Redirect ‚Üí {location}"
            else:
                return url, f"‚ö†Ô∏è {code}"
    except Exception as e:
        return url, f"üî• Error: {str(e)}"

def check_urls_batch(urls, batch_size=50, max_workers=10):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –±–∞—Ç—á–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ThreadPoolExecutor"""
    results = []
    
    def check_batch(url_batch):
        batch_results = []
        for url in url_batch:
            status = check_url_status(url)
            batch_results.append((url, status))
        return batch_results
    
    # –†–∞–∑–±–∏–≤–∞–µ–º URLs –Ω–∞ –±–∞—Ç—á–∏
    batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        batch_results = list(executor.map(check_batch, batches))
        
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for batch in batch_results:
        results.extend(batch)
    
    return results

async def check_urls_async(urls, batch_size=100):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –±–∞—Ç—á–∞–º–∏"""
    results = []
    
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=50)) as session:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            tasks = [check_url_status_async(session, url) for url in batch_urls]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            results.extend(batch_results)
    
    return results

def analyze_results(results):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    stats = defaultdict(int)
    domain_stats = defaultdict(int)
    
    for url, status in results:
        stats[status] += 1
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        try:
            domain = url.split('/')[2]
            domain_stats[domain] += 1
        except:
            pass
    
    analysis = []
    analysis.append("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    analysis.append(f"–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(results)}")
    for status, count in stats.items():
        analysis.append(f"{status}: {count}")
    
    analysis.append("\nüåê –ü–û –î–û–ú–ï–ù–ê–ú:")
    for domain, count in domain_stats.items():
        analysis.append(f"{domain}: {count}")
    
    return "\n".join(analysis)

def save_results_to_file(results, analysis, filename=None):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"link_validation_results_{timestamp}.txt"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("üîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò –°–°–´–õ–û–ö\n")
        f.write("=" * 50 + "\n\n")
        
        # –ê–Ω–∞–ª–∏–∑
        f.write(analysis)
        f.write("\n\n" + "=" * 50 + "\n\n")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        f.write("üìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n\n")
        for url, status in results:
            f.write(f"{url} ‚Üí {status}\n")
    
    return filename

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
try:
    db = get_db()
    
    # –ü–æ–ª—É—á–∞–µ–º source_url –∏–∑ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–≤—è–∑—å —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ–π
    unique_ads_query = text("""
        SELECT a.source_url 
        FROM unique_ads ua 
        JOIN ads a ON ua.base_ad_id = a.id 
        WHERE a.source_url IS NOT NULL
    """)
    unique_ads_result = db.execute(unique_ads_query)
    unique_ads = unique_ads_result.fetchall()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º URLs –≤ —Å–ø–∏—Å–æ–∫
    urls = [source_url for (source_url,) in unique_ads]
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(urls)} —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏
    if len(urls) > 1000:
        print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Å—ã–ª–æ–∫...")
        start_time = time.time()
        results = asyncio.run(check_urls_async(urls))
        end_time = time.time()
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
    else:
        print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É...")
        start_time = time.time()
        results = check_urls_batch(urls)
        end_time = time.time()
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    analysis = analyze_results(results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    filename = save_results_to_file(results, analysis)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filename}")
    
    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å
    print("\n" + analysis)
    
    db.close()
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
