#!/usr/bin/env python3
"""
–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
–£–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ —Å –≥–∏–±–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π
"""

import asyncio
import aiohttp
import logging
import os
from datetime import datetime
from typing import Dict, List, Optional
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AutomationScheduler:
    def __init__(self):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.api_base_url = os.getenv('API_BASE_URL', 'http://app:8000')
        self.pipeline_interval = int(os.getenv('PIPELINE_INTERVAL_HOURS', '3')) * 3600  # –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
        # –ö–∞–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä—Å–∏—Ç—å (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
        scraping_sources = os.getenv('SCRAPING_SOURCES', 'house,lalafo,stroka')
        self.scraping_sources = [s.strip() for s in scraping_sources.split(',') if s.strip()]
        
        # –í–∫–ª—é—á–µ–Ω–∏–µ/–≤—ã–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        self.enable_scraping = os.getenv('ENABLE_SCRAPING', 'true').lower() == 'true'
        self.enable_photo_processing = os.getenv('ENABLE_PHOTO_PROCESSING', 'true').lower() == 'true'
        self.enable_duplicate_processing = os.getenv('ENABLE_DUPLICATE_PROCESSING', 'true').lower() == 'true'
        self.enable_realtor_detection = os.getenv('ENABLE_REALTOR_DETECTION', 'true').lower() == 'true'
        self.enable_elasticsearch_reindex = os.getenv('ENABLE_ELASTICSEARCH_REINDEX', 'true').lower() == 'true'
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å—Ä–∞–∑—É –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        self.run_immediately_on_start = os.getenv('RUN_IMMEDIATELY_ON_START', 'true').lower() == 'true'
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∂–∏–¥–∞–Ω–∏—è –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏
        self.scraping_check_interval = int(os.getenv('SCRAPING_CHECK_INTERVAL_SECONDS', '60'))  # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
        self.processing_check_interval = int(os.getenv('PROCESSING_CHECK_INTERVAL_SECONDS', '30'))  # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.max_wait_time = int(os.getenv('MAX_WAIT_TIME_MINUTES', '120')) * 60  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        
        # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞
        self.last_pipeline_run = 0
        
        self.session: Optional[aiohttp.ClientSession] = None

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏")
        logger.info(f"üìä –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
        logger.info(f"  - API: {self.api_base_url}")
        logger.info(f"  - –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∫–∞–∂–¥—ã–µ {self.pipeline_interval // 3600} —á–∞—Å–æ–≤")
        logger.info(f"  - –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞: {', '.join(self.scraping_sources)}")
        logger.info(f"  - –ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: –ü–∞—Ä—Å–∏–Ω–≥ ‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ ‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ‚Üí –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ ‚Üí –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è")
        
        enabled_steps = []
        if self.enable_scraping: enabled_steps.append("–ü–∞—Ä—Å–∏–Ω–≥")
        if self.enable_photo_processing: enabled_steps.append("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ")
        if self.enable_duplicate_processing: enabled_steps.append("–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        if self.enable_realtor_detection: enabled_steps.append("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤")
        if self.enable_elasticsearch_reindex: enabled_steps.append("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è")
        logger.info(f"  - –í–∫–ª—é—á—ë–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã: {' ‚Üí '.join(enabled_steps)}")
        if self.run_immediately_on_start:
            logger.info("  - –ó–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: –î–ê")
        else:
            logger.info("  - –ó–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: –ù–ï–¢")
        
        self.session = aiohttp.ClientSession()
        
        try:
            # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ API –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –∑–∞–ø—É—Å–∫–æ–º
            if self.run_immediately_on_start:
                await self.wait_for_api_ready()
                logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–µ—Ä–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Å—Ç–∞—Ä—Ç–∞")
                success = await self.run_full_pipeline()
                if success:
                    self.last_pipeline_run = asyncio.get_event_loop().time()
                    logger.info("‚úÖ –ü–µ—Ä–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                    logger.info(f"üìÖ –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {self.pipeline_interval // 3600} —á–∞—Å–æ–≤")
                else:
                    logger.error("‚ùå –û—à–∏–±–∫–∏ –≤ –ø–µ—Ä–≤–æ–º –ø–∞–π–ø–ª–∞–π–Ω–µ")
            else:
                logger.info(f"‚è≥ –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ —á–µ—Ä–µ–∑ {self.pipeline_interval // 3600} —á–∞—Å–æ–≤")
            
            while True:
                current_time = asyncio.get_event_loop().time()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
                if current_time - self.last_pipeline_run >= self.pipeline_interval:
                    logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é")
                    success = await self.run_full_pipeline()
                    if success:
                        self.last_pipeline_run = current_time
                        logger.info("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                        logger.info(f"üìÖ –°–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {self.pipeline_interval // 3600} —á–∞—Å–æ–≤")
                    else:
                        logger.error("‚ùå –û—à–∏–±–∫–∏ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ")
                
                # –ñ–¥—ë–º 60 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                await asyncio.sleep(60)
                
        except KeyboardInterrupt:
            logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏")
        finally:
            if self.session:
                await self.session.close()

    async def run_full_pipeline(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        pipeline_success = True
        
        # –≠—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥
        if self.enable_scraping:
            logger.info("üï∑Ô∏è –≠—Ç–∞–ø 1/5: –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            scraping_success = await self.run_scraping_step()
            if not scraping_success:
                logger.error("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏")
                pipeline_success = False
            else:
                logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.info("‚è≠Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∫–ª—é—á—ë–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        # –≠—Ç–∞–ø 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        if self.enable_photo_processing:
            logger.info("üì∏ –≠—Ç–∞–ø 2/5: –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π")
            photo_success = await self.run_photo_processing_step()
            if not photo_success:
                logger.error("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
                pipeline_success = False
            else:
                logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.info("‚è≠Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –æ—Ç–∫–ª—é—á–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        # –≠—Ç–∞–ø 3: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if self.enable_duplicate_processing:
            logger.info("üîÑ –≠—Ç–∞–ø 3/5: –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            duplicates_success = await self.run_duplicate_processing_step()
            if not duplicates_success:
                logger.error("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
                pipeline_success = False
            else:
                logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.info("‚è≠Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        # –≠—Ç–∞–ø 4: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤
        if self.enable_realtor_detection:
            logger.info("üè¢ –≠—Ç–∞–ø 4/5: –ó–∞–ø—É—Å–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤")
            realtor_success = await self.run_realtor_detection_step()
            if not realtor_success:
                logger.error("‚ùå –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
                pipeline_success = False
            else:
                logger.info("‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.info("‚è≠Ô∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        # –≠—Ç–∞–ø 5: –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è Elasticsearch
        if self.enable_elasticsearch_reindex:
            logger.info("üîç –≠—Ç–∞–ø 5/5: –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ Elasticsearch")
            reindex_success = await self.run_elasticsearch_reindex_step()
            if not reindex_success:
                logger.error("‚ùå –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è Elasticsearch –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
                pipeline_success = False
            else:
                logger.info("‚úÖ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è Elasticsearch –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.info("‚è≠Ô∏è –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è Elasticsearch –æ—Ç–∫–ª—é—á–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        return pipeline_success

    async def run_scraping_step(self) -> bool:
        """–≠—Ç–∞–ø 1: –ó–∞–ø—É—Å–∫ –∏ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        success_count = 0
        total_sources = len(self.scraping_sources)
        job_ids = []
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        for source in self.scraping_sources:
            logger.info(f"üì° –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source}")
            job_id = await self.start_scraping_job(source)
            if job_id:
                job_ids.append((source, job_id))
                success_count += 1
                logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ {source} –∑–∞–ø—É—â–µ–Ω (job_id: {job_id})")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source}")
        
        if success_count == 0:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return False
        
        logger.info(f"üìä –ó–∞–ø—É—â–µ–Ω–æ {success_count} –∏–∑ {total_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞...")
        all_completed = await self.wait_for_scraping_completion(job_ids)
        
        if all_completed:
            logger.info("‚úÖ –í—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
            return True
        else:
            logger.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–∞–º–∏ –∏–ª–∏ –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
            return False

    async def run_photo_processing_step(self) -> bool:
        """–≠—Ç–∞–ø 2: –ó–∞–ø—É—Å–∫ –∏ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π"""
        logger.info("üì∏ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π...")
        success = await self.api_request('POST', '/process/photos')
        if not success:
            return False
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π...")
        completed = await self.wait_for_processing_completion('/process/photos/status')
        return completed

    async def run_duplicate_processing_step(self) -> bool:
        """–≠—Ç–∞–ø 3: –ó–∞–ø—É—Å–∫ –∏ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        success = await self.api_request('POST', '/process/duplicates')
        if not success:
            return False
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        completed = await self.wait_for_processing_completion('/process/duplicates/status')
        return completed

    async def run_realtor_detection_step(self) -> bool:
        """–≠—Ç–∞–ø 4: –ó–∞–ø—É—Å–∫ –∏ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤"""
        logger.info("üè¢ –ó–∞–ø—É—Å–∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤...")
        success = await self.api_request('POST', '/process/realtors/detect')
        if not success:
            return False
        
        # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∏—ç–ª—Ç–æ—Ä–æ–≤...")
        completed = await self.wait_for_processing_completion('/process/realtors/status')
        return completed

    async def run_elasticsearch_reindex_step(self) -> bool:
        """–≠—Ç–∞–ø 5: –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ Elasticsearch"""
        logger.info("üîç –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ Elasticsearch...")
        success = await self.api_request('POST', '/elasticsearch/reindex')
        if success:
            logger.info("‚úÖ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è Elasticsearch –∑–∞–ø—É—â–µ–Ω–∞ (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ–Ω–µ)")
            # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–±—ã—á–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ–Ω–µ, –ø–æ—ç—Ç–æ–º—É –Ω–µ –∂–¥—ë–º –µ—ë –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            return True
        return False

    async def start_scraping_job(self, source: str) -> Optional[str]:
        """–ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ job_id"""
        url = f"{self.api_base_url}/scraping/start/{source}"
        
        try:
            async with self.session.post(url) as response:
                if response.status in [200, 201, 202]:
                    data = await response.json()
                    return data.get('job_id')
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source} - —Å—Ç–∞—Ç—É—Å {response.status}")
                    return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source}: {e}")
            return None

    async def wait_for_scraping_completion(self, job_ids: List[tuple]) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        start_time = asyncio.get_event_loop().time()
        
        while True:
            current_time = asyncio.get_event_loop().time()
            if current_time - start_time > self.max_wait_time:
                logger.warning(f"‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({self.max_wait_time // 60} –º–∏–Ω—É—Ç)")
                return False
            
            all_completed = True
            running_jobs = []
            
            for source, job_id in job_ids:
                status = await self.get_scraping_job_status(job_id)
                if status:
                    job_status = status.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                    if job_status in ['–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è', '–æ–∂–∏–¥–∞–Ω–∏–µ']:
                        all_completed = False
                        running_jobs.append(f"{source}({job_status})")
                    elif job_status == '–æ—à–∏–±–∫–∞':
                        logger.warning(f"‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ {source} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ {source}")
            
            if all_completed:
                return True
            
            if running_jobs:
                logger.info(f"‚è≥ –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {', '.join(running_jobs)}")
            
            await asyncio.sleep(self.scraping_check_interval)

    async def wait_for_processing_completion(self, status_endpoint: str) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        start_time = asyncio.get_event_loop().time()
        
        while True:
            current_time = asyncio.get_event_loop().time()
            if current_time - start_time > self.max_wait_time:
                logger.warning(f"‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({self.max_wait_time // 60} –º–∏–Ω—É—Ç)")
                return False
            
            status = await self.get_processing_status(status_endpoint)
            if status:
                process_status = status.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                if process_status == 'running':
                    logger.info("‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
                elif process_status == 'completed':
                    return True
                elif process_status == 'error':
                    logger.error("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π")
                    return False
                elif process_status == 'idle':
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å idle, –∑–Ω–∞—á–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
                    return True
            
            await asyncio.sleep(self.processing_check_interval)

    async def get_scraping_job_status(self, job_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        url = f"{self.api_base_url}/scraping/status/{job_id}"
        
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return None
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {job_id}: {e}")
            return None

    async def get_processing_status(self, endpoint: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        url = f"{self.api_base_url}{endpoint}"
        
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return None
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return None

    async def api_request(self, method: str, endpoint: str, **kwargs) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP-–∑–∞–ø—Ä–æ—Å–∞ –∫ API"""
        url = f"{self.api_base_url}{endpoint}"
        
        try:
            async with self.session.request(method, url, **kwargs) as response:
                if response.status in [200, 201, 202]:
                    logger.debug(f"‚úÖ {method} {endpoint} - —É—Å–ø–µ—à–Ω–æ")
                    return True
                else:
                    logger.error(f"‚ùå {method} {endpoint} - —Å—Ç–∞—Ç—É—Å {response.status}")
                    text = await response.text()
                    logger.error(f"–û—Ç–≤–µ—Ç: {text[:200]}...")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {method} {endpoint}: {e}")
            return False

    async def wait_for_api_ready(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ FastAPI —Å–µ—Ä–≤–µ—Ä–∞"""
        logger.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ FastAPI —Å–µ—Ä–≤–µ—Ä–∞...")
        max_attempts = 60  # –ú–∞–∫—Å–∏–º—É–º 5 –º–∏–Ω—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è
        attempt = 0
        
        while attempt < max_attempts:
            try:
                async with self.session.get(f"{self.api_base_url}/status") as response:
                    if response.status == 200:
                        logger.info("‚úÖ FastAPI —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤!")
                        return
            except Exception as e:
                logger.debug(f"API –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}): {e}")
            
            attempt += 1
            await asyncio.sleep(5)  # –ñ–¥—ë–º 5 —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        logger.warning("‚ö†Ô∏è FastAPI —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É...")

    async def get_system_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        try:
            async with self.session.get(f"{self.api_base_url}/status") as response:
                if response.status == 200:
                    return await response.json()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
        return {}

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    scheduler = AutomationScheduler()
    await scheduler.start()

if __name__ == "__main__":
    asyncio.run(main()) 